[
["index.html", "STA238: Probability, Statistics, and Data Analysis Chapter 1 Introduction", " STA238: Probability, Statistics, and Data Analysis Alison Gibbs and Alex Stringer 2019-12-04 Chapter 1 Introduction This ‚Äúbook‚Äù serves as supplementary materials for the assigned readings from the course textbooks for STA238: Probability, Statistics and Data Analysis, Winter 2020 at the University of Toronto, taught by Alison Gibbs and Alex Stringer. The course textbooks are as follows: [MIPS] F.M. Dekking, C. Kraaikamp H.P. Lopuha Ãàa and L.E. Meester (2005). A modern Introduction to Probability and Statistics: Understanding How and Why. Springer-Verlag. This is the primary reference for the course. This book is available in the University of Toronto bookstore. A pdf version of this textbook is freely available through the University of Toronto library website. [E&amp;R] M.J. Evans and J.S. Rosenthal (2003). Probability and Statistics: The Science of Uncertainty. W.H. Freeman and Co.Available in pdf here:http://www.utstat.toronto.edu/mikevans/jeffrosenthal. [ISL] G. James, D. Witten, T. Hastie and R. Tibshirani (2013). An Introduction to Statistical Learning with Applications in R. Springer. Available in pdf here:http://faculty.marshall.usc.edu/gareth-james/ISL The primary course text is MIPS. The other two are included because they contain certain important sections that MIPS is missing; we won‚Äôt go over them in nearly as much detail as we will MIPS. These supplementary materials are structured as follows. Each ‚Äúchapter‚Äù corresponds to a chapter in one of the course texts, usually MIPS. You should read the chapter in the course text before reading the supplementary notes, and you should do both of these things before coming to lecture. Each chapter of supplementary notes contains required practice problems from the relevant textbook chapters, which you should attempt before lecture and then solve in detail after lecture. Also do all of the ‚Äúquick exercises‚Äù in MIPS and all of the exercises within these supplementary notes. Answers to selected exercises are available in the back of MIPS. You can find the code used to create this book here. All of the data is stored in the data folder in this repository. You can look at the code for each chapter and copy bits and run them, though of course we recommend typing them out yourself! "],
["section-supplement-to-chapters-15-and-16.html", "Chapter 2 Supplement to Chapters 15 and 16 2.1 Old Faithful 2.2 Drilling 2.3 Exercises 2.4 Extended example: smoking and age and mortality 2.5 Case study: rental housing in Toronto", " Chapter 2 Supplement to Chapters 15 and 16 This chapter implements much of the analysis shown in chapters 15 and 16 of A Modern Introduction to Probability and Statistics (MIPS). R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. The assigned exercises associated with this material are from MIPS, as follows: 15.1, 15.2; 15.5; 15.6; 15.8; 15.11; 15.12 16.1, 16.2; 16.3; 16.4, 16.5; 16.6; 16.8; 16.15. Answers to selected exercises are in the back of the book. You should also do all the ‚Äúquick exercises‚Äù within chapters 15 and 16 (solutions are at the end of the chapter). Use R as much as possible when answering the questions. 2.1 Old Faithful The Old Faithful data is stored in the oldfaithful.txt file in the book data folder. To read it in to R, first you need to look at its contents. 2.1.1 Read in the data You can either open it in a text editor (since it‚Äôs small), or print out the first few lines on the command line. The latter is nicer: the data is printed properly, without added bits from notepad or whatever; and if you‚Äôre working with large data stored in big text files, trying to open the whole thing in an editor will crash your computer. It‚Äôs good to get in the habit of working a bit on the command line now. If you have a Mac or Linux-based system, open a terminal. If you have a Windows based system, get a Mac or Linux. Just kidding (not really though). Download Git Bash: https://git-scm.com/download/win and open a new terminal window. To look at a text file on the command line, use the head command. You have to tell it where to look on your computer. You can navigate to the folder where the data is stored and just type the name of the file, or you can type a whole filepath, or a relative filepath. Right now, I am in the working directory for this book, and relative to this location, the data is stored in data/MIPSdata. If you are not in this directory, you have to navigate there. The pwd command tells you where you are, the ls -l command tells you what‚Äôs there, and the cd command lets you change to a new directory. So if you‚Äôre in /home/alex, you would type pwd and it would print /home/alex. If this location contains a folder called data, you can type ls -l and there should be a list of stuff, including a folder called data. You can type cd data to go there, and repeat the process. Navigate to where you downloaded the data to‚Äì it‚Äôs kind of like a game! Anyways, once you‚Äôre there, type: head data/MIPSdata/oldfaithful.txt ## 216 ## 108 ## 200 ## 137 ## 272 ## 173 ## 282 ## 216 ## 117 ## 261 Don‚Äôt worry if you don‚Äôt see the \\#\\#; those are added by the book. I don‚Äôt see them when I do this right in the command line. A column of numbers is printed. This indicates that the text file oldfaithdful.txt contains a single column of numbers, and no header row. A header row is what it‚Äôs called when the first row of the dataset contains the names of the columns. To read such a dataset into R, we‚Äôll use the read_csv function in the readr package. The readr package is automatically loaded when we load the tidyverse package. If this sentence is confusing (no worries!) I recommend you check out my R tutorial. We‚Äôll just do that: library(tidyverse) oldfaithful &lt;- readr::read_csv( file = &quot;data/MIPSdata/oldfaithful.txt&quot;, # Tell it where the file is col_names = &quot;time&quot;, # Tell it that the first row is NOT column names, and at the same time, tell it what name you want for the column. col_types = &quot;n&quot; # Tell it that there is one column, and it is &quot;numeric&quot; (n) ) # Check what was read in using the dplyr::glimpse() function dplyr::glimpse(oldfaithful) ## Observations: 272 ## Variables: 1 ## $ time [3m[38;5;246m&lt;dbl&gt;[39m[23m 216, 108, 200, 137, 272, 173, 282, 216, 117, 261, 110, 235,‚Ä¶ By glimpseing the data, we see that the format matches what we saw in the raw file, and we are given the number of rows too. Check that the number of rows matches what was in the raw file by printing out the number of rows on the command line using the wc -l command (‚Äúwc‚Äù = ‚Äúword count‚Äù and ‚Äú-l‚Äù means count ‚Äúlines‚Äù): wc -l data/MIPSdata/oldfaithful.txt ## 271 data/MIPSdata/oldfaithful.txt What happened, why don‚Äôt they match? They do. The wc -l command actually counts the number of ‚Äúnewline‚Äù characters in the file. It is customary to end a data file with a newline character. This file doesn‚Äôt, though. How do I know? Type the following: tail data/MIPSdata/oldfaithful.txt ## 111 ## 255 ## 119 ## 135 ## 285 ## 247 ## 129 ## 265 ## 109 ## 268 This prints out the last few lines of the file. In this book, these are printed as normal. But if you do it on the command line, you‚Äôll see that the command prompt gets printed on the same line as the final number, 268. This indicates that the file does not end with a newline character, and hence the total number of newlines in the file is 271, corresponding to 272 actual lines of data. Remark: subtle issues like this come up all the time in data analysis. The only way to get good at it is practice, so don‚Äôt be discouraged! 2.1.2 Graphical Summaries Now that the data has been read in and checked, we can realize the fruits of our labour! Chapter 15 analyzes these data using a tabular display, histogram, kernel density estimate, and empirical CDF. Here is how to do these things in R. # Tabular display print(oldfaithful$time) ## [1] 216 108 200 137 272 173 282 216 117 261 110 235 252 105 282 130 105 ## [18] 288 96 255 108 105 207 184 272 216 118 245 231 266 258 268 202 242 ## [35] 230 121 112 290 110 287 261 113 274 105 272 199 230 126 278 120 288 ## [52] 283 110 290 104 293 223 100 274 259 134 270 105 288 109 264 250 282 ## [69] 124 282 242 118 270 240 119 304 121 274 233 216 248 260 246 158 244 ## [86] 296 237 271 130 240 132 260 112 289 110 258 280 225 112 294 149 262 ## [103] 126 270 243 112 282 107 291 221 284 138 294 265 102 278 139 276 109 ## [120] 265 157 244 255 118 276 226 115 270 136 279 112 250 168 260 110 263 ## [137] 113 296 122 224 254 134 272 289 260 119 278 121 306 108 302 240 144 ## [154] 276 214 240 270 245 108 238 132 249 120 230 210 275 142 300 116 277 ## [171] 115 125 275 200 250 260 270 145 240 250 113 275 255 226 122 266 245 ## [188] 110 265 131 288 110 288 246 238 254 210 262 135 280 126 261 248 112 ## [205] 276 107 262 231 116 270 143 282 112 230 205 254 144 288 120 249 112 ## [222] 256 105 269 240 247 245 256 235 273 245 145 251 133 267 113 111 257 ## [239] 237 140 249 141 296 174 275 230 125 262 128 261 132 267 214 270 249 ## [256] 229 235 267 120 257 286 272 111 255 119 135 285 247 129 265 109 268 # Ugly! You can use the View() function to open the data in a spreadsheet # Not run: # View(oldfaithful) # Histogram using base R hist(oldfaithful$time) # Base R graphics are outdated. Use ggplot2, which is loaded automatically with the tidyverse: oldfaithful %&gt;% ggplot(aes(x = time)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;blue&quot;,alpha = .3) + labs(title = &quot;Eruption times for Old Faithful geyser&quot;, x = &quot;Eruption time&quot;, y = &quot;Density&quot;) # Try different numbers of bins and you might see different patterns. Do you think this is a # good or bad thing, or both? # Their &quot;optimal&quot; bin width: s &lt;- sd(oldfaithful$time) n &lt;- nrow(oldfaithful) b &lt;- (24*sqrt(pi))^(1/3) * s * (n^(-1/3)) b ## [1] 36.89694 oldfaithful %&gt;% ggplot(aes(x = time)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = round(b),colour = &quot;black&quot;,fill = &quot;blue&quot;,alpha = .3) + labs(title = &quot;Eruption times for Old Faithful geyser&quot;, subtitle = stringr::str_c(&quot;&#39;Optimal&#39; bin width of b = &quot;,round(b)), x = &quot;Eruption time&quot;, y = &quot;Density&quot;) To get kernel density estimates, there are a couple different ways. The density function in R does the math for you, using a Gaussian kernel (the functions \\(K_{i}\\) are taken to be Gaussian density functions with mean and standard deviation determined by the data). You can plot the output of density using base R or ggplot. You can also use the ggplot geom_density function to do something similar automatically. # Kernel density estimation in R dens &lt;- density(oldfaithful$time) dens ## ## Call: ## density.default(x = oldfaithful$time) ## ## Data: oldfaithful$time (272 obs.); Bandwidth &#39;bw&#39; = 20.09 ## ## x y ## Min. : 35.74 Min. :3.771e-06 ## 1st Qu.:118.37 1st Qu.:8.571e-04 ## Median :201.00 Median :2.412e-03 ## Mean :201.00 Mean :3.022e-03 ## 3rd Qu.:283.63 3rd Qu.:5.143e-03 ## Max. :366.26 Max. :8.070e-03 plot(dens) # Okay... ggplot? tibble(x = dens$x,y = dens$y) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_line() + labs(title = &quot;Kernel density estimate, Old Faithful data&quot;, subtitle = &quot;Manually-calculated values&quot;, x = &quot;Eruption time&quot;, y = &quot;Density&quot;) # Can also do automatically: oldfaithful %&gt;% ggplot(aes(x = time)) + theme_classic() + geom_density() + labs(title = &quot;Kernel density estimate, Old Faithful data&quot;, subtitle = &quot;Automatically-calculated values&quot;, x = &quot;Eruption time&quot;, y = &quot;Density&quot;) # The reason to manually calculate the values is because you have more control. # I don&#39;t know what&#39;s happening at the endpoints there, and it&#39;s too much work # to go and figure out how to make ggplot not do that. # When you calculate the plotting values yourself and then put them into ggplot, # you have total control! For the empirical distribution function, we use the ecdf function in R. You would think this function should behave in a similar manner to the density function, but it doesn‚Äôt. It returns a function which computes the ecdf. It still has a plot method, but to use it with ggplot we have to use stat_function: faithful_ecdf &lt;- ecdf(oldfaithful$time) plot(faithful_ecdf) # ggplot tibble(x = c(100,300)) %&gt;% # Tell ggplot we want to plot the ecdf from 100 to 300 ggplot(aes(x = x)) + theme_classic() + stat_function(fun = faithful_ecdf) + labs(title = &quot;Empirical CDF for Old Faithful Eruption Times&quot;, x = &quot;Eruption Time&quot;, y = &quot;Empirical probability that an eruption time is less than x&quot;) Discussion point: the CDF is the integrated pdf: \\[ F(x) = \\int_{-\\infty}^{x}f(s)ds \\] So why don‚Äôt we integrate the kernel density estimate to get the empirical CDF? One of the great benefits of taking a computation-forward approach to statistical inference is that we can ‚Äúshoot first and ask questions later‚Äù- just try it, and then (maybe) use math to explain the results. Here is the world‚Äôs most naive numerical integration-based estimate of a CDF: tibble( x = dens$x[-1], y = cumsum(dens$y[-1]) * diff(dens$x) # Quick and dirty numerical integration. Can you put something better? ) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_line() + labs(title = &quot;Numerical integration-based empirical CDF for Old Faithful data&quot;, x = &quot;Eruption time&quot;, y = &quot;Empirical probability that an eruption time is less than x&quot;) What do you think? Is this better, worse, or just different than the ECDF? Chapter 16 discusses boxplots. To get a boxplot, again, you can use base R or ggplot. We have: # Base R boxplot(oldfaithful$time) # ggplot oldfaithful %&gt;% ggplot(aes(y = time)) + theme_classic() + geom_boxplot(width = .1) + labs(title = &quot;Boxplot of eruption times, Old Faithful data&quot;, y = &quot;Eruption time&quot;) + # Have to play around with the x axis to get it to look nice *shrug* coord_cartesian(xlim = c(-.2,.2)) + theme(axis.text.x = element_blank()) 2.1.3 Numerical Summaries (Ch 16) The numerical summaries in chapter 16 include the sample mean and median, sample standard deviation and mean absolute deviation, and quantiles. # Mean mean(oldfaithful$time) ## [1] 209.2684 # Median median(oldfaithful$time) ## [1] 240 # Standard deviation sd(oldfaithful$time) ## [1] 68.48329 # Quantiles: tell it which ones you want. I want 0, 25, 50, 75, 100 quantile(oldfaithful$time,probs = c(0,.25,.50,.75,1.00)) ## 0% 25% 50% 75% 100% ## 96.00 129.75 240.00 267.25 306.00 # The zeroth and hundredth quantiles are the sample minimum and maximum: min(oldfaithful$time) ## [1] 96 max(oldfaithful$time) ## [1] 306 # Actually, you can get all this with the summary() function: summary(oldfaithful$time) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 96.0 129.8 240.0 209.3 267.2 306.0 # Mean absolute deviation # I don&#39;t know an R function for this off the top of my head (maybe you can find one?) # So let&#39;s calculate it manually. # Actually, let&#39;s calculate them ALL manually! # Mean: sum(oldfaithful$time) / length(oldfaithful$time) ## [1] 209.2684 # Median: it&#39;s the 50th percentile quantile(oldfaithful$time,probs = .5) ## 50% ## 240 # Can get it manually too: sort(oldfaithful$time)[length(oldfaithful$time)/2] ## [1] 240 # Better to use the quantile() function though. # Standard deviation. Need to save the mean to a variable first: mn &lt;- mean(oldfaithful$time) sqrt( sum( (oldfaithful$time - mn)^2 ) / ( length(oldfaithful$time) - 1 ) ) ## [1] 68.48329 # MAD. Similar to sd: md &lt;- median(oldfaithful$time) median( abs(oldfaithful$time - md) ) ## [1] 38.5 # IQR: quantile(oldfaithful$time,probs = .75) - quantile(oldfaithful$time,probs = .25) ## 75% ## 137.5 # Note that there are various ways to correct for the fact that not all quantiles # are exact (you may not have a datapoint which has EXACTLY 25% of the data below it, # like if the sample size isn&#39;t divisible by 4). R probably uses a different method # than the book, so the results here are slightly different. 2.2 Drilling 2.2.1 Read in data Read in the drilling.txt file: head data/MIPSdata/drilling.txt ## 5 640.67 830 ## 10 674.67 800 ## 15 708 711.33 ## 20 735.67 867.67 ## 25 754.33 940.67 ## 30 723.33 941.33 ## 35 664.33 924.33 ## 40 727.67 873 ## 45 658.67 874.67 ## 50 658 843.33 # Use the read_tsv (not csv), because this file is &quot;tab-delimited&quot;; the spaces # between columns contain tab characters. drilling &lt;- readr::read_tsv( file = &quot;data/MIPSdata/drilling.txt&quot;, col_names = c(&quot;depth&quot;,&quot;dry&quot;,&quot;wet&quot;), col_types = &quot;nnn&quot; ) glimpse(drilling) ## Observations: 80 ## Variables: 3 ## $ depth [3m[38;5;246m&lt;dbl&gt;[39m[23m 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75,‚Ä¶ ## $ dry [3m[38;5;246m&lt;dbl&gt;[39m[23m 640.67, 674.67, 708.00, 735.67, 754.33, 723.33, 664.33, 72‚Ä¶ ## $ wet [3m[38;5;246m&lt;dbl&gt;[39m[23m 830.00, 800.00, 711.33, 867.67, 940.67, 941.33, 924.33, 87‚Ä¶ 2.2.2 Graphical summaries To make a scatterplot, you can again use base R or ggplot. We want separate plots for dry and wet holes. You can do this by plotting these data separately, or you can re-format the data and have ggplot do it automatically: # Base R par(mfrow = c(1,2)) # Plots on a 1 x 2 grid plot(dry~depth,data = drilling) plot(wet~depth,data = drilling) # ggplot # Two separate plots: dryplt &lt;- drilling %&gt;% ggplot(aes(x = depth,y = dry)) + theme_classic() + geom_point(pch = 21) + # pch=21 is the magic command to give you hollow points labs(title = &quot;Dry Holes&quot;, x = &quot;Depth&quot;, y = &quot;Mean drill time&quot;) + scale_y_continuous(breaks = seq(500,1300,by = 200)) + coord_cartesian(ylim = c(500,1300)) wetplt &lt;- drilling %&gt;% ggplot(aes(x = depth,y = wet)) + theme_classic() + geom_point(pch = 21) + # pch=21 is the magic command to give you hollow points labs(title = &quot;Wet Holes&quot;, x = &quot;Depth&quot;, y = &quot;Mean drill time&quot;) + scale_y_continuous(breaks = seq(500,1300,by = 200)) + coord_cartesian(ylim = c(500,1300)) cowplot::plot_grid(dryplt,wetplt,nrow = 1) # There is a lot of repeated code here. For a better way to make these two # plots, first create a base plot object and then reuse it: drillingplt &lt;- drilling %&gt;% ggplot(aes(x = depth)) + theme_classic() + labs(x = &quot;Depth&quot;, y = &quot;Mean drill time&quot;) + scale_y_continuous(breaks = seq(500,1300,by = 200)) + coord_cartesian(ylim = c(500,1300)) dryplt2 &lt;- drillingplt + labs(title = &quot;Dry holes&quot;) + geom_point(aes(y = dry),pch = 21) wetplt2 &lt;- drillingplt + labs(title = &quot;Wet holes&quot;) + geom_point(aes(y = wet),pch = 21) cowplot::plot_grid(dryplt2,wetplt2,nrow = 1) # Much better # Another option is to reformat the data and create the plot # with a single command. To do this we stack the dry and wet measurements # on top of each other, and create a new variable which indicates whether the # measurements are dry or wet. This is called putting the data into &quot;long&quot; format. # ggplot then knows how to &quot;facet&quot; the plots according to this new variable. # Check it out: drilling_long &lt;- drilling %&gt;% tidyr::gather(type,time,dry:wet) %&gt;% dplyr::mutate(type = case_when( # Rename the type values for plotting type == &quot;dry&quot; ~ &quot;Dry Holes&quot;, type == &quot;wet&quot; ~ &quot;Wet Holes&quot; )) dplyr::glimpse(drilling_long) ## Observations: 160 ## Variables: 3 ## $ depth [3m[38;5;246m&lt;dbl&gt;[39m[23m 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75,‚Ä¶ ## $ type [3m[38;5;246m&lt;chr&gt;[39m[23m &quot;Dry Holes&quot;, &quot;Dry Holes&quot;, &quot;Dry Holes&quot;, &quot;Dry Holes&quot;, &quot;Dry H‚Ä¶ ## $ time [3m[38;5;246m&lt;dbl&gt;[39m[23m 640.67, 674.67, 708.00, 735.67, 754.33, 723.33, 664.33, 72‚Ä¶ drilling_long %&gt;% ggplot(aes(x = depth,y = time)) + theme_classic() + facet_wrap(~type) + geom_point() + labs(title = &quot;Mean drilling times&quot;, x = &quot;Depth&quot;, y = &quot;Mean drilling time&quot;) # Even though there is more overhead with this method, I recommend it because # it scales to more variables. If you wanted to make 20 plots, you&#39;d have to # have 20 plots in the previous method but here, the code is actually identical. To make boxplots, transform the data in the same way as for the side-by-side scatterplots, and give to ggplot: drilling_long %&gt;% ggplot(aes(x = type,y = time)) + theme_classic() + geom_boxplot() + labs(title = &quot;Boxplots for wet and dry drilling times, Drilling data&quot;, x = &quot;Hole Type&quot;, y = &quot;Mean drilling time&quot;) 2.2.3 Numerical summaries We can compute numerical summaries of drilling times in the same way as for the eruption times from the Old Faithful data. However, the drilling data are naturally grouped, so we should compute our summaries by group‚Äî i.e. by hole type, wet or dry. To do this requires a bit more machinery; we will operate on a dataframe and use formal grouping operations. This isn‚Äôt that much harder, but it‚Äôs really powerful. Check it out: drilling_long %&gt;% group_by(type) %&gt;% # Everything that happens now happens separately for Wet Holes and Dry Holes summarize( mean = mean(time), sd = sd(time), min = min(time), median = median(time), quant_25 = quantile(time,probs = .25), quant_75 = quantile(time,probs = .75), max = max(time) ) ## # A tibble: 2 x 8 ## type mean sd min median quant_25 quant_75 max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Dry Holes 806. 154. 584 758. 688. 912. 1238 ## 2 Wet Holes 944. 124. 697. 918. 851. 1030. 1238. Grouped operations are fundamental to modern data analysis. 2.3 Exercises The Janka Hardness data is in the file jankahardness.txt. Reproduce the analysis in the book yourself. 2.4 Extended example: smoking and age and mortality You now have some tools. How do you put them to use in practice? Analyzing data ‚Äúin the wild‚Äù involves a lot of decision making, and this can impact the conclusions you make. Consider a famous dataset containing information on smoking and mortality. The data is available in the R package faraway. We may load the package and data and retrieve information on it as follows: # install.packages(&quot;faraway&quot;) # Run this to install the faraway package, which has useful datasets library(faraway) # Attach the faraway package data(&quot;femsmoke&quot;) # Load the &quot;femsmoke&quot; data # ?femsmoke # Run this to open the help page for the dataset. We see from the help page, and associated reference to the paper in the American Statistician, that the data comes from asking women in Whickham, England, whether they smoke or not, and then following up in 20 years to see if they died. Let‚Äôs perform a descriptive analysis of these data. We need some more quantitative pieces of descriptive information. What might we want to know about our data? Some ideas: How many observations are there in the data, and what does an observation represent in the context of how the data was collected? How many variables are present in the data, and what does each variable represent in the context of how the data was collected? How might we summarize each variable? We might compute a mean and a five-number summary for ‚Äúcontinuous‚Äù variables, and a table of counts for ‚Äúcategorical‚Äù variables (more on this later‚Ä¶). Let‚Äôs see how we can obtain these descriptive measures in R: # Get the number of observations (rows), variables, and an idea # of what the data looks like: glimpse(femsmoke) ## Observations: 28 ## Variables: 4 ## $ y &lt;dbl&gt; 2, 1, 3, 5, 14, 7, 27, 12, 51, 40, 29, 101, 13, 64, 53, 6‚Ä¶ ## $ smoker &lt;fct&gt; yes, no, yes, no, yes, no, yes, no, yes, no, yes, no, yes‚Ä¶ ## $ dead &lt;fct&gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, ye‚Ä¶ ## $ age &lt;fct&gt; 18-24, 18-24, 25-34, 25-34, 35-44, 35-44, 45-54, 45-54, 5‚Ä¶ # One observation represents a count of people in each category. # How many people? femsmoke %&gt;% summarize(num_people = sum(y)) # The summarize() function lets you compute summaries of variables in your dataframe ## num_people ## 1 1314 # How many smokers? femsmoke %&gt;% filter(smoker == &quot;yes&quot;) %&gt;% # filter() lets you choose which rows to keep summarize(num_smokers = sum(y)) ## num_smokers ## 1 582 # How many non-smokers? femsmoke %&gt;% filter(smoker == &quot;no&quot;) %&gt;% summarize(num_non_smokers = sum(y)) ## num_non_smokers ## 1 732 # We can get both those numbers at the same time: femsmoke %&gt;% group_by(smoker) %&gt;% # group_by() makes summarize() compute summaries within levels of a variable summarize(num_people = sum(y)) ## # A tibble: 2 x 2 ## smoker num_people ## &lt;fct&gt; &lt;dbl&gt; ## 1 yes 582 ## 2 no 732 There are lots of other descriptive statistics you could calculate. Summary: descriptive analysis involves communicating properties of a dataset, and the context behind the dataset. 2.4.1 Exercises How many non-smoking 18-24 year olds are there in the femsmoke data? Answer using filter(). How many smokers died? Answer using filter(). How many 45-55 year olds did not die? Compute the following table using group_by() and summarize(): ## # A tibble: 7 x 2 ## age num_people ## &lt;fct&gt; &lt;dbl&gt; ## 1 18-24 117 ## 2 25-34 281 ## 3 35-44 230 ## 4 45-54 208 ## 5 55-64 236 ## 6 65-74 165 ## 7 75+ 77 Let‚Äôs go into a bit more detail. I want to see if there is any apparent association between smoking and mortality. # Compute the mortality rate for smokers and non-smokers. # To do this, create a dataframe containing the numbers of smokers # and non-smokers smoker_numbers &lt;- femsmoke %&gt;% # The %&gt;% operator lets you form sequences of operations group_by(smoker) %&gt;% # group_by() makes all the following operations happen within groups summarize(num_people = sum(y)) # Count the number of people who are smokers and not smokers smoker_numbers ## # A tibble: 2 x 2 ## smoker num_people ## &lt;fct&gt; &lt;dbl&gt; ## 1 yes 582 ## 2 no 732 # Now, compute the number of people who died out of the smokers and non-smokers # This looks the same as above, except we now filter() only the people who died. smoker_numbers_dead &lt;- femsmoke %&gt;% filter(dead == &quot;yes&quot;) %&gt;% # Retains rows where dead == &quot;yes&quot; only group_by(smoker) %&gt;% summarize(num_dead = sum(y)) smoker_numbers_dead ## # A tibble: 2 x 2 ## smoker num_dead ## &lt;fct&gt; &lt;dbl&gt; ## 1 yes 139 ## 2 no 230 # Now, we join these two tables together and compute the mortality rates by group. smoker_numbers %&gt;% inner_join(smoker_numbers_dead,by = &quot;smoker&quot;) %&gt;% # Joins rows with the same value of &quot;smoker&quot; mutate(mort_rate = num_dead/num_people) # mutate() creates a new variable, which can be a function of the other variables in the dataframe. ## # A tibble: 2 x 4 ## smoker num_people num_dead mort_rate ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 yes 582 139 0.239 ## 2 no 732 230 0.314 See anything interesting? What went wrong? Why are we observing that smokers have a lower mortality rate than non-smokers? This contradicts the context surrounding this analysis, which in this case is the large body of formal and anecdotal evidence suggesting that smoking is harmful to health. Did we make a mistake? One thing we definitely did was ignore some present information. Specifically, we also know how old the women were. How can we include this information in our exploratory analysis? We can compute mortality rates by age: smoker_numbers_age &lt;- femsmoke %&gt;% group_by(smoker,age) %&gt;% # Now we&#39;re grouping by smoker AND age. The rest of the code remains unchanged. summarize(num_people = sum(y)) smoker_numbers_age_dead &lt;- femsmoke %&gt;% filter(dead == &quot;yes&quot;) %&gt;% group_by(smoker,age) %&gt;% summarize(num_dead = sum(y)) smoker_numbers_age %&gt;% inner_join(smoker_numbers_age_dead,by = c(&quot;smoker&quot;,&quot;age&quot;)) %&gt;% mutate(mort_rate = num_dead/num_people) ## # A tibble: 14 x 5 ## # Groups: smoker [2] ## smoker age num_people num_dead mort_rate ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 yes 18-24 55 2 0.0364 ## 2 yes 25-34 124 3 0.0242 ## 3 yes 35-44 109 14 0.128 ## 4 yes 45-54 130 27 0.208 ## 5 yes 55-64 115 51 0.443 ## 6 yes 65-74 36 29 0.806 ## 7 yes 75+ 13 13 1 ## 8 no 18-24 62 1 0.0161 ## 9 no 25-34 157 5 0.0318 ## 10 no 35-44 121 7 0.0579 ## 11 no 45-54 78 12 0.154 ## 12 no 55-64 121 40 0.331 ## 13 no 65-74 129 101 0.783 ## 14 no 75+ 64 64 1 Older people are more likely to die within the 20 year followup period. However, examining the raw counts of people in each group, we also see that in these data, older people are less likely to smoke than younger people. So in these data, less smokers died, because less smokers were old, and more old people died. Before moving on, get some practice doing exploratory analysis with the following exercises: 2.4.2 Exercises What is the relative risk of mortality‚Äîthe ratio of the mortality rates‚Äîfor smoking 18-24 year olds vs non-smoking 18-24 year olds? Compute the answer manually by reading the numbers off the above table. Then compute it using R by doing the following: Create two datasets using filter(): one containing smokers and one containing non-smokers. filter() out only the 18-24 year olds. This gives you two datasets each with only one row. For example, smokers &lt;- femsmoke %&gt;% filter(smoker == &quot;yes&quot;,age = &quot;18-24&quot;). inner_join() the two datasets together, using age as the by variable: smokers %&gt;% inner_join(???,by = &quot;age&quot;) Advanced: modify the above steps to create the following table of relative mortality rates. You should start from a cleaned up version of the mortality rate by age table: rates_by_age &lt;- smoker_numbers_age %&gt;% inner_join(smoker_numbers_age_dead,by = c(&quot;smoker&quot;,&quot;age&quot;)) %&gt;% mutate(mort_rate = num_dead/num_people) %&gt;% ungroup() # The data was previously grouped, we don&#39;t want this anymore Use dplyr::select() to remove and rename columns, see ?dplyr::select. You should get the following: ## # A tibble: 7 x 4 ## age smoker_mort_rate nonsmoker_mort_rate relative_risk ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18-24 0.0364 0.0161 2.25 ## 2 25-34 0.0242 0.0318 0.760 ## 3 35-44 0.128 0.0579 2.22 ## 4 45-54 0.208 0.154 1.35 ## 5 55-64 0.443 0.331 1.34 ## 6 65-74 0.806 0.783 1.03 ## 7 75+ 1 1 1 2.5 Case study: rental housing in Toronto The RentSafeTO: Apartment Building Standards program is designed to help renters in the city of Toronto make informed choices about where to live, and to enforce a minimum standard of quality upon rental units within the city. With rents skyrocketing and home ownership not a reasonable option for most, having an informed view of the rental market is imperative for Toronto residents. It also helps keep leaders accountable, specifically if we focus on social and community housing buildings. Comprehensive and fairly clean data from the program, along with specific information, is available at https://open.toronto.ca/dataset/apartment-building-evaluation/. Data for the following were downloaded on 2019/09/16. To start your analysis, go now and download the data and open it in a spreadsheet and have a look. Familiarize yourselves with the variable descriptions and how the data were collected; the documentation. This somewhat tedious task is a first step of any data analysis, in academia, industry, government, or wherever. 2.5.1 Load the data The data are stored in a .csv file, which stands for ‚Äúcomma-separated-values‚Äù. Storing data in a text file with a separator, usually a comma, is very common. These are referred to as ‚Äúflat files‚Äù in an industrial context, to distinguish them from data stored in databases. We may read the data into R using the read_csv function in the readr package. The readr package is part of the tidyverse package that we used before, so if you installed that package, you have it loaded. # https://open.toronto.ca/dataset/apartment-building-evaluation/ # install.packages(&quot;readr&quot;) # Read the data in. This means call the readr::read_csv() function, point it # to where you saved the data on your computer, and then save the result to a # variable. I am naming this variable &#39;apartmentdata&#39;. # Type ?readr::read_csv if you want to read about this function. apartmentdata &lt;- readr::read_csv( file = &quot;data/apartment-data/toronto-apartment-building-evaluations.csv&quot; ) ## Parsed with column specification: ## cols( ## .default = col_double(), ## EVALUATION_COMPLETED_ON = col_character(), ## PROPERTY_TYPE = col_character(), ## RESULTS_OF_SCORE = col_character(), ## SITE_ADDRESS = col_character(), ## WARD = col_character() ## ) ## See spec(...) for full column specifications. The message displayed is telling you that readr::read_csv() guessed at what kind of data were in each column, i.e. numbers, letters, dates, etc. You should make sure, as I have while writing, that these are what you expect. You can get a concise view of this dataset using the glimpse function in the dplyr package, which is automatically loaded when you load the tidyverse: glimpse(apartmentdata) ## Observations: 3,446 ## Variables: 32 ## $ `_id` &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1‚Ä¶ ## $ BALCONY_GUARDS &lt;dbl&gt; NA, NA, NA, NA, 5, NA, 5, 3, 4, 4, 3‚Ä¶ ## $ CONFIRMED_STOREYS &lt;dbl&gt; 28, 4, 3, 3, 29, 3, 7, 18, 17, 32, 4‚Ä¶ ## $ CONFIRMED_UNITS &lt;dbl&gt; 457, 15, 26, 10, 272, 12, 95, 287, 3‚Ä¶ ## $ ELEVATORS &lt;dbl&gt; 4, NA, NA, NA, 5, NA, 5, 4, 5, 4, NA‚Ä¶ ## $ ENTRANCE_DOORS_WINDOWS &lt;dbl&gt; 3, 3, 3, 4, 5, 4, 4, 4, 3, 4, 4, 3, ‚Ä¶ ## $ ENTRANCE_LOBBY &lt;dbl&gt; 4, 3, 3, 4, 5, 4, 4, 4, 4, 4, 4, 4, ‚Ä¶ ## $ EVALUATION_COMPLETED_ON &lt;chr&gt; &quot;04/03/2019&quot;, &quot;05/24/2018&quot;, &quot;07/11/2‚Ä¶ ## $ EXTERIOR_CLADDING &lt;dbl&gt; 3, 4, 4, 4, 5, 4, 5, 4, 4, 3, 3, 4, ‚Ä¶ ## $ EXTERIOR_GROUNDS &lt;dbl&gt; 3, 4, 3, 3, 5, 4, 5, 4, 3, 4, 3, 4, ‚Ä¶ ## $ EXTERIOR_WALKWAYS &lt;dbl&gt; 3, 5, 4, 4, 5, 4, 5, 4, 3, 4, 4, 3, ‚Ä¶ ## $ GARBAGE_BIN_STORAGE_AREA &lt;dbl&gt; 3, 4, 3, 3, 4, 3, 3, 3, 4, 4, 4, 4, ‚Ä¶ ## $ GARBAGE_CHUTE_ROOMS &lt;dbl&gt; 3, NA, NA, NA, 5, NA, 5, 4, 3, 4, 5,‚Ä¶ ## $ GRAFFITI &lt;dbl&gt; 5, 5, 5, 5, 5, 4, 5, 4, 3, 4, 5, 5, ‚Ä¶ ## $ INTERIOR_LIGHTING_LEVELS &lt;dbl&gt; 3, 4, 4, 4, 5, 4, 4, 3, 3, 4, 3, 4, ‚Ä¶ ## $ INTERIOR_WALL_CEILING_FLOOR &lt;dbl&gt; 4, 3, 4, 4, 5, 4, 4, 3, 4, 4, 4, 3, ‚Ä¶ ## $ INTERNAL_GUARDS_HANDRAILS &lt;dbl&gt; 3, 4, 3, 4, 5, 4, 5, 4, 4, 4, 5, 4, ‚Ä¶ ## $ NO_OF_AREAS_EVALUATED &lt;dbl&gt; 18, 14, 14, 13, 19, 16, 17, 18, 19, ‚Ä¶ ## $ OTHER_FACILITIES &lt;dbl&gt; 4, NA, NA, NA, 5, NA, NA, NA, NA, NA‚Ä¶ ## $ PARKING_AREA &lt;dbl&gt; 2, NA, NA, NA, 4, 3, 5, 2, 4, 4, 2, ‚Ä¶ ## $ PROPERTY_TYPE &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SO‚Ä¶ ## $ RESULTS_OF_SCORE &lt;chr&gt; &quot;Evaluation needs to be conducted in‚Ä¶ ## $ RSN &lt;dbl&gt; 4365723, 4364249, 4408585, 4288126, ‚Ä¶ ## $ SCORE &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, ‚Ä¶ ## $ SECURITY &lt;dbl&gt; 4, 3, 3, 4, 5, 4, 5, 3, 4, 4, 4, 4, ‚Ä¶ ## $ SITE_ADDRESS &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL D‚Ä¶ ## $ STAIRWELLS &lt;dbl&gt; 4, 4, 3, 4, 5, 4, 5, 4, 4, 4, 3, 4, ‚Ä¶ ## $ STORAGE_AREAS_LOCKERS &lt;dbl&gt; NA, NA, NA, NA, NA, 4, NA, NA, 3, 4,‚Ä¶ ## $ WARD &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, ‚Ä¶ ## $ WATER_PEN_EXT_BLDG_ELEMENTS &lt;dbl&gt; 4, 4, 4, 4, 5, 4, 5, 4, 5, 3, 3, 4, ‚Ä¶ ## $ YEAR_BUILT &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, ‚Ä¶ ## $ YEAR_REGISTERED &lt;dbl&gt; 2018, 2018, 2018, 2017, 2018, 2017, ‚Ä¶ That‚Äôs bigger than the smoking data! 3,446 rental apartment buildings, each with 32 factors measured. The buliding‚Äôs address and Ward number are in there, which are helpful for characterizing neighbourhoods. 2.5.2 Analysis I: what does the data look like? As a first step, we want to get an idea of what our data ‚Äúlooks like‚Äù. This typically means picking some interesting variables and summarizing their distributions somehow. Which variables to pick will depend on the context. Often it will be clear which variables are important, and sometimes not. Because you read the documentation and familiarized yourselves with the variables in the dataset, you know that there is a variable called SCORE which sums up the individual category scores for each building. In the context of determining building quality, this seems like an important variable to look at. We‚Äôll summarize the distribution of SCORE using a five-number summary and mean, and a histogram with a kernel density estimate. First, prepare the data for analysis: # First, select only the columns you want # This isn&#39;t strictly necessary but trust me, it makes # debugging WAY easier. # I&#39;m also renaming the columns so the dataframe looks prettier. # Again, trust me. This stuff matters. apartmentclean &lt;- apartmentdata %&gt;% filter(!is.na(SCORE)) %&gt;% # Remove apartments with missing scores dplyr::select(ward = WARD, score = SCORE, property_type = PROPERTY_TYPE, year_built = YEAR_BUILT, address = SITE_ADDRESS ) glimpse(apartmentclean) # Much nicer! ## Observations: 3,437 ## Variables: 5 ## $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;0‚Ä¶ ## $ score &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, 78, 73, 76, 57‚Ä¶ ## $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SOCIAL HOUSING&quot;,‚Ä¶ ## $ year_built &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, 2015, 1970, 19‚Ä¶ ## $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACP‚Ä¶ To compute the five-number summary (plus mean), use the summary() function in R. I also want to know the standard deviation of SCORE: summary(apartmentclean$score) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 37.00 68.00 72.00 72.28 77.00 99.00 sd(apartmentclean$score,na.rm = TRUE) ## [1] 7.117172 The worst building in the city has a total score of 37, and the best gets 99. The median score‚Äîhalf the buildings in the city have a lower score, and half a higher score than this‚Äîis 72, and this roughly equals the mean of 72.28. 25% of buildings score higher than 77, and 25% score lower than 68. So most buildings seem to fall within less than one standard deviation of the mean, which indicates that these data are fairly concentrated about their mean. To provide some context, go look up your own building (if you live in a rental building) or that of a friend in the data. Where does your building fall in terms of quality within Toronto? So far we have used tabular displays to summarize our data, for both the smoking and the apartment data. We also learned about graphical displays. Let‚Äôs see a histogram of the scores, with a kernel density estimate: We can make a histogram in R as follows: # The ggplot2 package is loaded as part of the tidyverse score_histogram &lt;- apartmentclean %&gt;% ggplot(aes(x = score)) + # Tell ggplot to use score on the x axis theme_classic() + # Make the plot pretty geom_histogram( # Makes a histogram aes(y = ..density..), bins = 20, colour = &quot;black&quot;, fill = &quot;lightgrey&quot; ) + geom_density() + labs(title = &quot;Distribution of RentSafeTO Apartment Building Standards score&quot;, x = &quot;Score&quot;, y = &quot;Density&quot;) + scale_x_continuous(breaks = seq(30,100,by = 5)) score_histogram It appears that most buildings are in the 65 to 85 range. I actually just moved from a building that has a 66 to a building that has an 86. The difference is substantial! 2.5.3 Analysis II: Do different wards have different quality housing? A Ward is an administrative district within the city that has a single city counsellor. If I‚Äôm thinking about moving to, or within, Toronto, I want to know: Do different wards have different quality housing?. In order to address this question we need to decide on the following: Variable of interest. How do we quantify our research question? We need to pick a measure of quality. Picking different measures can lead to different conclusions. Filters. Do we look at all apartment buildings? Should we look only at those built after, or before, a certain date? Only those that meet a certain minimum, or maximum, standard of quality according to our definition? Are there any other kinds of decisions we might have to consider? Methods. What kind of statistical tools should we use to address our research question? We need to pick descriptive statistics to report, and decide whether we want to include other auxillary variables in the analysis. Conclusions. How do we report our results? Tables, charts, maps? Should we include subjective, editorial commentary, or let the data speak for themselves? This is already overwhelming! Let‚Äôs make an attempt at it. I propose: Our variable of interest should be SCORE, which you know (because you read the documentation‚Ä¶) is the ‚Äúoverall score of the buliding‚Äù. Higher is better. The actual formula is included in the documentation of the data. We will filter the data to only include buildings where PROPERTY_TYPE == 'PRIVATE', which will restrict our analysis to not include social housing. The quality of social housing is an important social justice issue (that you will investigate in the exercises) but it‚Äôs somewhat separate (?) from the question of where to look for rental housing. Our methods will include looking at a table of average scores for each ward. We will also look at whether older or newer buildings receive better scores. We will summarize our conclusions through a subjective assessment of the above table of average scores. With these decisions made, we may proceed with our analysis using the tidyverse as follows: # Apply filter(s). apartmentfiltered &lt;- apartmentclean %&gt;% filter(property_type == &quot;PRIVATE&quot;) # When filtering, always compare the filtered and unfiltered data to ensure # the result is as expected: glimpse(apartmentclean) ## Observations: 3,437 ## Variables: 5 ## $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;0‚Ä¶ ## $ score &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, 78, 73, 76, 57‚Ä¶ ## $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SOCIAL HOUSING&quot;,‚Ä¶ ## $ year_built &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, 2015, 1970, 19‚Ä¶ ## $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACP‚Ä¶ glimpse(apartmentfiltered) ## Observations: 2,873 ## Variables: 5 ## $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;08&quot;, &quot;1‚Ä¶ ## $ score &lt;dbl&gt; 71, 77, 71, 98, 76, 93, 72, 74, 78, 73, 76, 57, 70‚Ä¶ ## $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVA‚Ä¶ ## $ year_built &lt;dbl&gt; 1976, 1953, 1948, 2017, 1967, 2015, 1970, 1976, 19‚Ä¶ ## $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACP‚Ä¶ nrow(apartmentclean) - nrow(apartmentfiltered) # Dropped 567 rows. ## [1] 564 # Now create the table of averages: apartmentfiltered %&gt;% group_by(ward) %&gt;% summarize(avg_score = mean(score)) ## # A tibble: 26 x 2 ## ward avg_score ## &lt;chr&gt; &lt;dbl&gt; ## 1 01 71.5 ## 2 02 73.0 ## 3 03 70.5 ## 4 04 68.2 ## 5 05 71.7 ## 6 06 72.1 ## 7 07 69.8 ## 8 08 73.5 ## 9 09 67.5 ## 10 10 72.2 ## # ‚Ä¶ with 16 more rows Bah! What happened? Why are there these NA values? NA is the value R uses to mean ‚Äúmissing‚Äù. We have to hope that whether a rental apartment building‚Äôs score is missing is not related to what that score is, that is, we hope apartments with higher or lower scores aren‚Äôt missing more often. We will ignore missingness for now. To do this, use the na.rm = TRUE option in mean: apartmentsummary &lt;- apartmentfiltered %&gt;% group_by(ward) %&gt;% summarize(avg_score = mean(score,na.rm = TRUE)) apartmentsummary ## # A tibble: 26 x 2 ## ward avg_score ## &lt;chr&gt; &lt;dbl&gt; ## 1 01 71.5 ## 2 02 73.0 ## 3 03 70.5 ## 4 04 68.2 ## 5 05 71.7 ## 6 06 72.1 ## 7 07 69.8 ## 8 08 73.5 ## 9 09 67.5 ## 10 10 72.2 ## # ‚Ä¶ with 16 more rows This isn‚Äôt a super friendly way of comparing these 26 numbers. I‚Äôd rather use a graphical display, like the boxplots we learned about in chapters 15 and 16: apartmentfiltered %&gt;% ggplot(aes(x = ward,y = score)) + theme_classic() + geom_boxplot() + labs(title = &quot;ABS score shows moderate variability across wards in Toronto&quot;, x = &quot;Ward&quot;, y = &quot;ABS Score&quot;) It looks like some wards are better than others. Or are they? Can we make any definitive conclusions based on this? 2.5.4 Analysis III: trends in quality over time Let‚Äôs go further and analyze some other interesting aspects of these data. I‚Äôm interested in knowing: Are newer buildings higher quality? We have the score and the year_built, and we‚Äôd like to investigate whether newer buildings (higher year_built) have higher scores. We have another decision to make. We could consider year_built to be a categorical variable, and make a bar chart. Or, we could consider it to be a continuous variable. Because values of year_built are inherently comparable, and because our research question involves making such comparisons, we will consider year_built to be a continuous variable. One type of plot used to compare continuous variables is a scatterplot. A scatterplot has continuous variables on the x- and y-axes, and draws a point (or bubble) at each place in the two-dimensional plane where a datapoint occurs. We can make this kind of plot in ggplot2 as well. This time, we use the raw (well, cleaned and filtered) data: apartmentfiltered %&gt;% filter(year_built &gt; 1900) %&gt;% ggplot(aes(x = year_built,y = score)) + theme_classic() + geom_point(pch = 21,colour = &quot;black&quot;,fill = &quot;grey&quot;) + # pch=21 makes the bubbles hollow, looks nice scale_x_continuous(breaks = seq(1900,2020,by=10)) + # Set the x-axis range labs(title = &quot;Less rental buildings are being built recently, but they are of higher quality&quot;, x = &quot;Year Built&quot;, y = &quot;ABS Score&quot;) Very interesting. You can clearly see the baby boom of the 1950‚Äôs to 1970‚Äôs, followed by a massive slowdown in construction during the economic slump in the 1980‚Äôs, and a complete stop when rent control was introduced in 1991 (remember, these are rental buildings only). Then, we see a new wave of rental building construction, and the new buildings seem to be of higher quality. What are the highest and lowest quality rental buildings in Toronto? # Get the 10 highest scoring buildings apartmentfiltered %&gt;% arrange(desc(score)) %&gt;% # Sort the data, descending, by score slice(1:10) # Take the first ten- i.e. the top ten ## # A tibble: 10 x 5 ## ward score property_type year_built address ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 YY 99 PRIVATE 2018 561 SHERBOURNE ST ## 2 17 99 PRIVATE 2017 123 PARKWAY FOREST DR ## 3 16 99 PRIVATE 1963 70 PARKWOODS VILLAGE DR ## 4 07 98 PRIVATE 2017 2 VENA WAY ## 5 17 97 PRIVATE 1968 24 FOREST MANOR RD ## 6 12 96 PRIVATE 1960 42 GLEN ELM AVE ## 7 13 95 PRIVATE 2017 252 VICTORIA ST ## 8 02 95 PRIVATE 1969 500 SCARLETT RD ## 9 16 95 PRIVATE 1962 67 PARKWOODS VILLAGE DR ## 10 07 95 PRIVATE 2016 6 VENA WAY Wow. I know where I want to live. 2.5.5 Summary We have seen how even something simple like trying to figure out whether different areas of the city have different quality housing can require a lot of decision making. And these decisions require expertise. By taking a principled approach to learning data analysis, you are empowering yourself to live a life that is better informed. But notice that we didn‚Äôt really answer any questions in this chapter. We saw some rough patterns, but were they real? If we made different decisions, or if we sampled different data, would we have seen different patterns? In order to understand what the problem is and how to approach it, we need to take a more detailed look at the concept of error. 2.5.6 Exercises Take each of the analyses we have performed on the Toronto rental data and say whether you think it‚Äôs descriptive, exploratory, or prescriptive, or a mix, and say why. What is that ‚ÄúYY‚Äù ward that shows up in the dot plot? Investigate this unusual observation. Read the documentation online and choose three variables that you find the most interesting. Reproduce the analyses I, II and III using your variables. Is there more or less variability across wards than with score? What is the ward with the highest average score? In what ward is/are the building(s) with the highest score(s)? Is this the same ward, or not? Would you expect the ward with the highest average to also have the highest-scoring buildings? Repeat this question with the lowest scoring buildings instead of the highest. If you live in a rental apartment, find it in these data. If not, find a friend‚Äôs place. How does your building compare to other buildings in your ward? Does it score higher or lower? The filter() function is your friend here, or you can use apartmentfiltered %&gt;% arrange(SITE_ADDRESS) %&gt;% print(n = Inf) and then find yours in the list manually. Combine the analyses of sections 2.3.2 and 2.3.3. with that of 2.3.4. Specifically, make a table and a boxplot of the average score by year. This means replace ward by year_built in the analysis of sections 2.3.2. and 2.3.3. Do your conclusions change when comparing with 2.3.4? Why or why not? Would you expect this to always be the case? Advanced: analyze the quality of social housing in Toronto. Perform a similar analysis to what we performed here for PROPERTY_TYPE == 'PRIVATE', but instead for PROPERTY_TYPE %in% c('SOCIAL HOUSING','TCHC') (equivalent to PROPERTY_TYPE != 'PRIVATE'). Does the quality of social housing in Toronto vary greatly across different wards? Is it improving or degrading over time? Do you think we have enough information here to definitively answer these questions? "],
["section-supplement-to-chapters-13-and-14.html", "Chapter 3 Supplement to Chapters 13 and 14 3.1 Law of Large Numbers (Chapter 13) 3.2 Central Limit Theorem (Chapter 14)", " Chapter 3 Supplement to Chapters 13 and 14 This chapter implements much of the analysis shown in chapters 13 and 14 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. The assigned exercises associated with this material are from MIPS, as follows: 13.1; 13.2; 13.3; 13.4; 13.5, 13.6; 13.9; 13.10, 13.12 14.1, 14.2; 14.3; 14.5; 14.6; 14.9. Answers to selected exercises are in the back of the book. You should also do all the ‚Äúquick exercises‚Äù within chapters 13 and 14 (solutions are at the end of the chapter). Use R as much as possible when answering the questions. 3.1 Law of Large Numbers (Chapter 13) Load the packages we need: library(tidyverse) To simulate from a \\(\\text{Gamma}(\\alpha,\\beta)\\) distribution in , use the rgamma function: # Simulate from a gamma # Type ?rgamma to get information on the parametrization # There are three named parameters: shape, scale, and rate. Rate = 1/scale. # The distribution has mean shape * scale, or shape / rate. # The parametrization in the book is the &quot;rate&quot; parametrization. # Always read the docs to understand what the parameters are. # # Simulate with shape = 2 and rate = 1, so mean = 2 and variance = ? (exercise) rgamma(1,shape = 2,rate = 1) ## [1] 2.261417 # The first &quot;1&quot; in the call gives the number of values to simulate: rgamma(10,shape = 2,rate = 1) ## [1] 0.2879474 6.5713582 3.8444569 4.2636825 1.2889744 2.4472714 2.2748499 ## [8] 0.2077977 0.9627291 3.0199498 Plot the density of the gamma sample mean for various \\(n\\), recreating the left side of Figure 13.1: # Define a function to compute the density # Fix the scale and shape arguments at defaults of what&#39;s used in the book # You can play around with these. gamma_samplemean_density &lt;- function(x,n,shape = 2,rate = 1) { # x: point to evaluate the density at # n: sample size # Just use the dgamma function dgamma(x,shape = n * shape,rate = n*rate) } # Plot it for various n # Define a function to make the plot plot_for_n &lt;- function(n) { # Create a function with the n argument fixed densfun &lt;- purrr::partial(gamma_samplemean_density,n = n) # Plot using ggplot tibble(x = c(0,4)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = densfun) + coord_cartesian(xlim = c(0,4),ylim = c(0,1.5)) + labs(title = stringr::str_c(&quot;n = &quot;,n)) } # Create the plots purrr::map(c(1,2,4,9,16,400),plot_for_n) %&gt;% cowplot::plot_grid(plotlist = .,nrow = 3) Simulate the running average of experiments of size \\(n\\) from a \\(\\text{Gamma}(2,1)\\) for \\(n = 1,\\ldots,500\\) and plot them, recreating Figure 13.2: set.seed(54768798) # So I can reproduce these results # Simulate one experiment of size 500 n &lt;- 500 alpha &lt;- 2 beta &lt;- 1 gamma_experiment &lt;- rgamma(n = n,shape = alpha,rate = beta) # Compute the running average- a vector where the nth component is the average # of the first n terms in gamma_experiment runningaverage &lt;- cumsum(gamma_experiment) / 1:length(gamma_experiment) # Plot, remembering that the true mean is 2 / 1 = 2 tibble(x = 1:length(runningaverage), y = runningaverage) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_point(pch = &quot;.&quot;) + geom_hline(yintercept = alpha / beta,colour = &quot;red&quot;,size = .5,linetype = &quot;dotted&quot;) What happens when you increase the number? Try it for \\(n = 1,000, n = 10,000\\), and so on. Now try this for the Cauchy distribution, the left panel of Figure 13.3: set.seed(4235) # So I can reproduce these results # Simulate one experiment of size 500 n &lt;- 500 mu &lt;- 2 sigma &lt;- 1 cauchy_experiment &lt;- rcauchy(n = n,location = mu,scale = sigma) # Compute the running average- a vector where the nth component is the average # of the first n terms in gamma_experiment runningaverage &lt;- cumsum(cauchy_experiment) / 1:length(cauchy_experiment) # Plot, remembering that the true mean is 2 / 1 = 2 tibble(x = 1:length(runningaverage), y = runningaverage) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_point(pch = &quot;.&quot;) + geom_hline(yintercept = alpha / beta,colour = &quot;red&quot;,size = .5,linetype = &quot;dotted&quot;) Yikes! The Cauchy distribution is the worst. Exercise: repeat this for the Pareto distribution, recreating the right panel of Figure 13.3. You can simulate from the Pareto distribution using the rpareto function in the actuar package. Type install.packages(&quot;actuar&quot;) and then actuar::rpareto. Type ?actuar::rpareto to get help on using this function. Figuring out how to use the function is part of the exercise. 3.1.1 Extended example: the probability of heads As an extended example, consider trying to figure out what the probability of heads is for a fair coin, just based on flipping the coin a bunch of times. We can use the material of Section 13.4 to address this challenge. Let \\(X\\) be a random variable which takes values \\(0\\) and \\(1\\) if the coin comes up tails or heads on any given flip. Let \\(C = \\left\\{ 1\\right\\}\\), so the specific event that we are interested in is whether the coin comes up heads on any given flip. \\(p = \\text{P}(X\\in C)\\) is hence the probability of heads. To estimate \\(p\\) using the LLN, we will take a coin which has probability \\(p\\) of coming up heads and flip it a bunch of times and calculate the proportion of flips that come up heads. # Function to flip the coin n times, and return a sequence of 1 if heads and 0 if tails # for each flip. # Use the rbinom function to simulate from a bernoulli/binomial distribution. flip_the_coin &lt;- function(n,p) { # n: number of times to flip the coin. # p: probability of heads # Returns a vector of length n containing the results of each flip. rbinom(n,1,p) } # Function to flip the coin n times and compute the # sample proportion of heads sample_proportion_of_heads &lt;- function(n,p) { # Returns a number representing the sample proportion of heads # in n flips mean(flip_the_coin(n,p)) } # Try it out: sample_proportion_of_heads(10,.5) ## [1] 0.4 Exercise: create a plot of the running average of sample proportions of heads, similar to the above plots for the Gamma and Cauchy (Figure 13.3). How many times do you think you need to flip the coin before the result is an accurate estimate? Does this change for different values of \\(p\\)? 3.2 Central Limit Theorem (Chapter 14) Let‚Äôs investigate the scaling power on \\(n\\) when standardizing averages. The book shows in Figure 14.1 that multiplying \\((\\bar{X} - \\mu)\\) by \\(n^{1/4}\\) isn‚Äôt enough to stabilize the variance in the distribution, \\(n^{1}\\) is too much, and \\(n^{1/2}\\) is just right. Exercise: derive the probability density of \\(Y = n^{p}(\\bar{X} - \\mu)\\) when \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}\\text{Gamma}(2,1)\\). Hint: look at the code below. How do I compute this? What formula am I using? We can recreate Figure 14.1 as follows: scalingdensity &lt;- function(y,n,p) { dgamma( x = y*n^(-p) + 2, shape = 2 * n, rate = n ) * n^(-p) } plotscalingdensity &lt;- function(n,p) { dens &lt;- purrr::partial(scalingdensity,n = n,p = p) tibble(x = c(-3,3)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = dens) + coord_cartesian(xlim = c(-3,3),ylim = c(0,.4)) + labs(title = stringr::str_c(&quot;n = &quot;,n,&quot;; scaling = &quot;,p)) + theme(text = element_text(size = 8)) } # Plot them all! This code is somewhat advanced; you should run it # line by line and figure out what each step does. It&#39;s a pretty concise # way of doing a lot of operations. This type of &quot;list comprehension&quot; is fundamental # to learning to program with data. pltlist &lt;- expand.grid(n = c(1,2,4,16,100), p = c(1/4,1/2,1)) %&gt;% dplyr::arrange(n,p) %&gt;% as.list() %&gt;% purrr::transpose() %&gt;% purrr::map(~plotscalingdensity(n = .x[[&quot;n&quot;]],p = .x[[&quot;p&quot;]])) cowplot::plot_grid(plotlist = pltlist, nrow = 5,ncol = 3) You can play around with different scaling values and sample sizes. Figure 14.2 is like the centre column of Figure 14.1 with a normal density curve overlayed. Exercise: recreate the left column of Figure 14.2 by doing the following: Replace expand.grid(n = c(1,2,4,16,100),p = c(1/4,1/2,1)) by expand.grid(n = c(1,2,4,16,100),p = c(1/2)) in the above code that generates the plots (and set the values of nrow and ncol appropriately as well). Add a normal density line. You have to modify the plotscalingdensity. Add a layer as follows: stat_function(fun = dnorm,linetype = &quot;dotted&quot;). We can compute probabilities involving the standard normal distribution function in R using the pnorm function. Let‚Äôs compute the approximate probability described in a couple of the textbook‚Äôs examples, and compare it to simulated and true values. # &quot;Did we have bad luck?&quot;. # The actual probability is (why?): n &lt;- 500 1 - pgamma(2.06,shape = 2*n,rate = 1*n) ## [1] 0.1710881 # The CLT probability is: 1 - pnorm(.95) ## [1] 0.1710561 # Why are we doing 1 - pnorm()? pnorm() gives P(X &lt; x) for X ~ N(0,1) # To get P(X &gt; x), you can do pnorm(.95,lower.tail = FALSE) ## [1] 0.1710561 # but it&#39;s easier to just do P(X &gt; x) = 1 - P(X &lt; x) # We can also simulate this probability by generating a bunch of gamma random # samples and seeing how often their averages are &gt; 2.06: N &lt;- 10000 exceeded &lt;- numeric(N) for (i in 1:N) { samp &lt;- rgamma(n,shape = 2,rate = 1) mn &lt;- mean(samp) exceeded[i] &lt;- as.numeric(mn &gt; 2.06) } mean(exceeded) ## [1] 0.1714 # Pretty good. Can you experiment with the number of simulations? How # many experiments do you need to simulate in order to get an accurate value? 3.2.1 Extended example: the probability of heads In our coin example from the LLN section, we investigated how many flips were needed to get an average number of heads that was close (in probability) to the true probability of heads. Using the CLT, we can get a probabilistic quantification of the error rate‚Äì how far away from the truth the proportion of heads is likely to be. Similar to the LLN experiment, we‚Äôre goin to flip the coin a bunch of times and calculate the sample proportion of heads; then we‚Äôre going to do that a bunch of times and plot a histogram of the sample proportions. The CLT tells us that as long as each experiment has enough flips, the resulting probability density of the sample proportion of heads should be approximately Normal. N &lt;- 1000 # Number of experiments to do n &lt;- 100 # Number of times to flip the coin in each experiment p &lt;- .5 # True probability of heads experiments &lt;- numeric(N) for (i in 1:N) { experiments[i] &lt;- sample_proportion_of_heads(n,p) } # Plot them tibble(x = experiments) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),bins=30,colour=&quot;black&quot;,fill=&quot;orange&quot;,alpha = .5) + stat_function(fun = dnorm,args = list(mean = p,sd = sqrt(p*(1-p)/n)),colour = &quot;purple&quot;) + labs(title = &quot;Empirical distribution of sample proportions of heads&quot;, subtitle = stringr::str_c(&quot;# of flips: &quot;,n,&quot;, true probability of heads: &quot;,p), x = &quot;Proportion of heads&quot;, y = &quot;Empirical Density&quot;) + scale_x_continuous(breaks = seq(0,1,by=.1)) Exercises: 1. Recreate the above plot with \\(n = 10, 50, 100, 1000\\) and \\(p = .4, .2, .8, .01, .99\\). What do you see? Is the accuracy of the normal approximation to this distribution affected by \\(n\\) or \\(p\\)? 1. What are the mean and variance of the distribution of sample proportions of heads? (Hint: what are the mean and variance of a \\(\\text{Binom}(n,p)\\) random variable?) 1. Recreate the above plot, but scale the sample proportion of heads appropriately such that it has mean \\(0\\) and variance \\(1\\). 1. Recreate the ‚ÄúNormal Approximation of the Binomial Distribution‚Äù calculations using The CLT The exact values, using the rbinom function (look up the help file) A simulation "],
["section-supplement-to-chapters-17-and-19.html", "Chapter 4 Supplement to Chapters 17 and 19 4.1 Statistical models (Chapter 17) 4.2 Unbiased Estimators (Chapter 19)", " Chapter 4 Supplement to Chapters 17 and 19 This chapter implements much of the analysis shown in chapters 17 and 19 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. The assigned exercises associated with this material are from MIPS, as follows: 17.1, 17.2; 17.3; 17.4; 17.7; 17.9 19.1; 19.2, 19.3; 19.5; 19.8; 19.9. Answers to selected exercises are in the back of the book. You should also do all the ‚Äúquick exercises‚Äù within chapters 17 and 19 (solutions are at the end of the chapter). Use R as much as possible when answering the questions. 4.1 Statistical models (Chapter 17) Most of the material from chapter 17 is a review of that from chapters 15 and 16, but in an important new context. You should be doing all exercises from chapter 17 using R, using the tools you have learned from chapters 15 and 16. Here we will focus on the linear regression model. This chapter doesn‚Äôt cover how these models are estimated, so here we will show how to use R to plot regression lines. We‚Äôll use ggplot2 to recreate Figure 17.8, and show how to add ‚Äúsmooth‚Äù non-linear regression lines to plots as well. 4.1.1 Janka Hardness data The Janka Hardness dataset was already discussed in Chapter 15/16. Since you did the exercises from those chapters prescribed in this supplementary book, you have already read these data into R: head data/MIPSdata/jankahardness.txt ## 24.7 484 ## 24.8 427 ## 27.3 413 ## 28.4 517 ## 28.4 549 ## 29 648 ## 30.3 587 ## 32.7 704 ## 35.6 979 ## 38.5 914 By printing it out on the command line, you can tell that the file is tab-delimited. Use readr::read_delim() to read it in: library(tidyverse) janka &lt;- readr::read_delim( file = &quot;data/MIPSdata/jankahardness.txt&quot;, delim = &quot;\\t&quot;, col_names = c(&quot;density&quot;,&quot;hardness&quot;), col_types = &quot;nn&quot; ) glimpse(janka) ## Observations: 36 ## Variables: 2 ## $ density &lt;dbl&gt; 24.7, 24.8, 27.3, 28.4, 28.4, 29.0, 30.3, 32.7, 35.6, 3‚Ä¶ ## $ hardness &lt;dbl&gt; 484, 427, 413, 517, 549, 648, 587, 704, 979, 914, 1070,‚Ä¶ Create a scatterplot with ggplot2: jankascatter &lt;- janka %&gt;% ggplot(aes(x = density,y = hardness)) + theme_classic() + geom_point() + scale_x_continuous(breaks = seq(20,80,by=10)) + scale_y_continuous(breaks = seq(0,3500,by=500)) + coord_cartesian(xlim = c(20,80),ylim = c(0,3500)) + labs(x = &quot;Wood density&quot;, y = &quot;Hardness&quot;) jankascatter To add a line to a plot, use geom_abline(). We can use this to recreate the regression line from Figure 17.8: jankascatter + geom_abline(slope = 57.51,intercept = -1160.5) But how did the book calculate these values? We‚Äôll answer this question in a later chapter. But for now, it would still be nice to get the computer to compute these values for us rather than typing them in manually. We can do this using the geom_smooth() function in ggplot2: jankascatter + geom_smooth(method = &quot;lm&quot;,se = FALSE,colour = &quot;black&quot;,size = .5) The ‚Äúlm‚Äù stands for ‚Äúlinear model‚Äù and the ‚Äúse‚Äù stands for ‚Äústandard error‚Äù; leaving this at its default of ‚ÄúTRUE‚Äù would add error bars to the line, which we‚Äôll learn about later (give it a try though). We can also add a non-linear curve to the plot using this technique. Most of the approaches to doing non-linear regression involve breaking the data up into small chunks based on the x-axis values, and then doing linear regression in each chunk and joining the resulting lines. The ‚Äúloess‚Äù non-linear regression line is an example of this approach. It roughly stands for ‚Äúlocal regression and smoothing splines‚Äù. We can add this using ggplot2 as well: jankascatter + geom_smooth(method = &quot;loess&quot;,se = FALSE,colour = &quot;black&quot;,size = .5) See how it‚Äôs a bit more wiggly, but still pretty straight? The data really does look like it supports a linear relationship between density and hardness. This is not common in modern practice! 4.1.2 Extended example: TTC ridership revenues Toronto‚Äôs population is growing over time. This puts strain on our outdated public transit system. But it should also lead to increased revenues. According to (https://globalnews.ca/news/1670796/how-does-the-ttcs-funding-compare-to-other-transit-agencies/)[a news article from a few years back], the TTC is the least-subsidized major transit agency in North America, which means that its operating budget is the most dependent on fare revenue out of any in all of the US and Canada. Tracking how ridership revenues are changing over time is very important. The city does do this. Go to the City of Toronto Progress Portal and type ‚ÄúTTC‚Äù and click on the box that says ‚ÄúTTC Ridership Revenues‚Äù to see a report. You can download the data from here, but since it‚Äôs a bit tricky to describe exactly how, I have put the file ttc-ridership-revenues.csv in the data folder. We are going to read these data into R and analyze the relationship between year and revenue. If you‚Äôre thinking ‚Äúthat sounds really easy, we just did that!‚Äù‚Ä¶ just keep reading. First, print the data out and count the number of rows on the command line: head data/ttc-ridership-revenues.csv wc -l data/ttc-ridership-revenues.csv ## Year,Jan,Feb,Mar,Apr,May,Jun,Jul,Aug,Sep,Oct,Nov,Dec ## 2007 YTD Actual,$70600000,$131200000,$204600000,$264900000,$322000000,$395100000,$452100000,$507500000,$585600000,$646900000,$712500000,$774700000 ## 2008 YTD Actual,$72700000,$137600000,$217500000,$278200000,$340600000,$419600000,$482400000,$544100000,$629000000,$696400000,$766600000,$837000000 ## 2009 YTD Actual,$69300000,$135400000,$216600000,$280500000,$344000000,$422100000,$483400000,$543500000,$627200000,$693900000,$762400000,$834900000 ## 2010 YTD Actual,$72200000,$143400000,$230700000,$302400000,$372500000,$459100000,$528800000,$595700000,$689100000,$764500000,$842000000,$929300000 ## 2011 YTD Actual,$75300000,$150800000,$244400000,$318300000,$392400000,$484800000,$557300000,$625500000,$722000000,$799500000,$879100000,$969900000 ## 2012 YTD Actual,$75500000,$154800000,$253900000,$331600000,$408300000,$507100000,$581800000,$654300000,$755800000,$835900000,$919100000,$1017600000 ## 2013 YTD Actual,$93200000,$176600000,$278200000,$360300000,$439700000,$539700000,$617400000,$693500000,$799600000,$882500000,$968900000,$1052100000 ## 2014 YTD Actual,$92200000,$178900000,$284400000,$367200000,$449700000,$552300000,$633200000,$712200000,$822000000,$907900000,$998200000,$1086500000 ## 2015 YTD Actual,$90600000,$178200000,$284100000,$370500000,$455300000,$559600000,$641700000,$721400000,$833200000,$920800000,$1011600000,$1107300000 ## 14 data/ttc-ridership-revenues.csv Yikes! Real data is messy. This data isn‚Äôt even that messy and it still seems messy. We see that the file is comma-separated and has a header. The first column is text and the others are‚Ä¶ well, they‚Äôre supposed to be numeric, but they are stored in the file with dollar signs. WHY?! This kind of thing is super annoying and super common. We could remove the dollar signs from the text file directly using sed or a similar UNIX-based tool, but I prefer whenever possible to keep all my analysis on one platform. We‚Äôll read it into R as-is and then parse and change datatypes there: # Read in the data ridership &lt;- readr::read_csv( file = &quot;data/ttc-ridership-revenues.csv&quot;, col_names = TRUE, # Tells readr to read the column names from the first line of the file. col_types = stringr::str_c(rep(&quot;c&quot;,13),collapse = &quot;&quot;) # Read all 13 columns as &quot;c&quot;haracter ) glimpse(ridership) ## Observations: 13 ## Variables: 13 ## $ Year &lt;chr&gt; &quot;2007 YTD Actual&quot;, &quot;2008 YTD Actual&quot;, &quot;2009 YTD Actual&quot;, &quot;2‚Ä¶ ## $ Jan &lt;chr&gt; &quot;$70600000&quot;, &quot;$72700000&quot;, &quot;$69300000&quot;, &quot;$72200000&quot;, &quot;$75300‚Ä¶ ## $ Feb &lt;chr&gt; &quot;$131200000&quot;, &quot;$137600000&quot;, &quot;$135400000&quot;, &quot;$143400000&quot;, &quot;$1‚Ä¶ ## $ Mar &lt;chr&gt; &quot;$204600000&quot;, &quot;$217500000&quot;, &quot;$216600000&quot;, &quot;$230700000&quot;, &quot;$2‚Ä¶ ## $ Apr &lt;chr&gt; &quot;$264900000&quot;, &quot;$278200000&quot;, &quot;$280500000&quot;, &quot;$302400000&quot;, &quot;$3‚Ä¶ ## $ May &lt;chr&gt; &quot;$322000000&quot;, &quot;$340600000&quot;, &quot;$344000000&quot;, &quot;$372500000&quot;, &quot;$3‚Ä¶ ## $ Jun &lt;chr&gt; &quot;$395100000&quot;, &quot;$419600000&quot;, &quot;$422100000&quot;, &quot;$459100000&quot;, &quot;$4‚Ä¶ ## $ Jul &lt;chr&gt; &quot;$452100000&quot;, &quot;$482400000&quot;, &quot;$483400000&quot;, &quot;$528800000&quot;, &quot;$5‚Ä¶ ## $ Aug &lt;chr&gt; &quot;$507500000&quot;, &quot;$544100000&quot;, &quot;$543500000&quot;, &quot;$595700000&quot;, &quot;$6‚Ä¶ ## $ Sep &lt;chr&gt; &quot;$585600000&quot;, &quot;$629000000&quot;, &quot;$627200000&quot;, &quot;$689100000&quot;, &quot;$7‚Ä¶ ## $ Oct &lt;chr&gt; &quot;$646900000&quot;, &quot;$696400000&quot;, &quot;$693900000&quot;, &quot;$764500000&quot;, &quot;$7‚Ä¶ ## $ Nov &lt;chr&gt; &quot;$712500000&quot;, &quot;$766600000&quot;, &quot;$762400000&quot;, &quot;$842000000&quot;, &quot;$8‚Ä¶ ## $ Dec &lt;chr&gt; &quot;$774700000&quot;, &quot;$837000000&quot;, &quot;$834900000&quot;, &quot;$929300000&quot;, &quot;$9‚Ä¶ This does not look like it‚Äôs in a form ready to analyze. Some problems: The Year has unwanted text in it. We just want the number representing what year it is. The revenue is stored across 12 columns, one for each month. We want the annual revenue for our analysis. The actual numeric revenue is stored as text with a dollar sign. We need to parse out the number part and convert to a numeric datatype before we can analyze it. Problems 1 and 3 require a bit of text parsing; Problem 2 requires converting from ‚Äúwide‚Äù to ‚Äúlong‚Äù format. Let‚Äôs do it: # PROBLEM 1: Year # To parse out only the number part, use a regular expression. # Our string starts with a four digit number which starts with 20. We want to capture this number # and nothing else. # The ^ means &quot;the start of the string&quot;. # The [20]{2} means &quot;a 0 or a 2, exactly twice&quot; # The [0-9]{2} means &quot;anything from 0 - 9, exactly twice&quot; year_regex &lt;- &quot;^[20]{2}[0-9]{2}&quot; # Use stringr::str_extract to extract a substring matching the regular expression: stringr::str_extract(&quot;2007 YTD Actual&quot;,year_regex) ## [1] &quot;2007&quot; # PROBLEM 2: wide to long # Use the tidyr::gather() function for &quot;gather&quot;ing columns and putting them # into one column: ridership %&gt;% tidyr::gather(month,revenue,Jan:Dec) ## # A tibble: 156 x 3 ## Year month revenue ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2007 YTD Actual Jan $70600000 ## 2 2008 YTD Actual Jan $72700000 ## 3 2009 YTD Actual Jan $69300000 ## 4 2010 YTD Actual Jan $72200000 ## 5 2011 YTD Actual Jan $75300000 ## 6 2012 YTD Actual Jan $75500000 ## 7 2013 YTD Actual Jan $93200000 ## 8 2014 YTD Actual Jan $92200000 ## 9 2015 YTD Actual Jan $90600000 ## 10 2016 YTD Actual Jan $90800000 ## # ‚Ä¶ with 146 more rows # PROBLEM 3: removing the dollar sign # Again, use text matching. Because $ is itself a special character, # to match it, you have to &quot;escape&quot; it using a backslash dollar_regex &lt;- &quot;\\\\$&quot; # Remove matching strings using stringr::str_remove() stringr::str_remove(&quot;$1234&quot;,dollar_regex) ## [1] &quot;1234&quot; # Now, combine all these into one data cleaning pipeline. # Remember we have monthly revenue, so to get yearly revenue, we sum # over months. ridership_clean &lt;- ridership %&gt;% tidyr::gather(month,revenue,Jan:Dec) %&gt;% # &quot;transmute&quot; is like mutate, but it deletes all original columns transmute(year = stringr::str_extract(Year,year_regex), revenue = stringr::str_remove(revenue,dollar_regex)) %&gt;% mutate_at(c(&quot;year&quot;,&quot;revenue&quot;),as.numeric) %&gt;% # Turn both year and revenue into numeric variables group_by(year) %&gt;% # Sum revenue for each year to get yearly revenue summarize(revenue = sum(revenue)) %&gt;% filter(year &lt; 2019) # 2019 has incomplete data, so remove it glimpse(ridership_clean) ## Observations: 12 ## Variables: 2 ## $ year &lt;dbl&gt; 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 20‚Ä¶ ## $ revenue &lt;dbl&gt; 5067700000, 5421700000, 5413200000, 5929700000, 62193000‚Ä¶ That looks a lot better! As usual, you should run each line of code one by one to understand what is happening. Because we went to the effort of cleaning the data, we can now plot it easily: ridershipscatter &lt;- ridership_clean %&gt;% ggplot(aes(x = year,y = revenue)) + theme_classic() + geom_point() + labs(title = &quot;Annual ridership revenues for the TTC&quot;, x = &quot;Year&quot;, y = &quot;Revenue&quot;) + scale_y_continuous(labels = scales::dollar_format()) # Make the y-axis pretty ridershipscatter Add a linear and non-linear regression line: leftplot &lt;- ridershipscatter + geom_smooth(method = &quot;lm&quot;,size = .5,se = FALSE,colour = &quot;black&quot;) + labs(subtitle = &quot;Linear regression line&quot;) rightplot &lt;- ridershipscatter + geom_smooth(method = &quot;loess&quot;,size = .5,se = FALSE,colour = &quot;black&quot;) + labs(subtitle = &quot;Non-linear regression line&quot;,y = &quot;&quot;) cowplot::plot_grid(leftplot, rightplot + theme(axis.text.y = element_blank()), # Take away the second plot&#39;s y-axis nrow=1) Exercise: re-do this analysis but don‚Äôt sum over month. This will give 12 points per year on the plot. Do one linear regression per month. Recreate the following plot yourself: To do this, you have to create a dataset that looks like this: ## Observations: 144 ## Variables: 3 ## $ month &lt;chr&gt; &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, ‚Ä¶ ## $ revenue &lt;dbl&gt; 70600000, 72700000, 69300000, 72200000, 75300000, 755000‚Ä¶ ## $ year &lt;dbl&gt; 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 20‚Ä¶ You should make the following modifications: Replace transmute with mutate so you don‚Äôt delete the month column. Replace aes(x = year,y = revenue) with aes(x = year,y = revenue,group = month,colour = month) in the call to ggplot. Don‚Äôt sum over months. 4.2 Unbiased Estimators (Chapter 19) Unbiasedness is one property of an estimator that the book claims is attractive. Let‚Äôs investigate this by recreating some of their simulations. 4.2.1 Simulated network data Figure 19.1 shows histograms of simulations of samples \\(X_{1},\\ldots,X_{n}\\) of size \\(n=30\\) from a \\(X\\sim\\text{Poisson}(\\log 10)\\) distribution. We want to estimate the parameter \\(p_{0}\\), which is the probability that \\(X = 0\\): \\[\\begin{equation} p_{0} = P(X = 0) = e^{-\\lambda} \\end{equation}\\] where \\(\\lambda = E(X) = \\log 10\\) in this example. The book suggests two estimators, \\(S = (1/n)\\times\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 0)\\) and \\(T = e^{-\\bar{X}_{n}}\\). \\(S\\) corresponds to calculating the sample proportion of times \\(X_{i}=0\\), and \\(T\\) corresponds to estimating the population mean \\(\\lambda\\) using the sample mean \\(\\bar{X}_{n}\\) and then plugging this in to the actual formula for the value \\(p_{0}\\). These both come from somewhere, and we‚Äôll see this when we talk about Maximum Likelihood. For now, just take them as both being candidates for an estimator of \\(p_{0}\\). Let‚Äôs investigate their sampling distributions by recreating the simulation from the book: set.seed(6574564) # Simulate 500 random samples of size 30 from a poisson(log(10)) # You need the purrr package, part of the tidyverse, for the map() function N &lt;- 500 n &lt;- 30 lambda &lt;- log(10) p0 &lt;- exp(-lambda) # True value of p0 # Simulate the samples samplelist &lt;- 1:N %&gt;% map(~rpois(n,lambda)) # Write functions to compute each estimator compute_S &lt;- function(samp) mean(samp == 0) compute_T &lt;- function(samp) exp(-mean(samp)) # Compute them and then store the results in a tibble() estimators &lt;- samplelist %&gt;% map(~c(S = compute_S(.x),T = compute_T(.x))) %&gt;% reduce(bind_rows) glimpse(estimators) ## Observations: 500 ## Variables: 2 ## $ S &lt;dbl&gt; 0.10000000, 0.16666667, 0.16666667, 0.13333333, 0.03333333, 0.‚Ä¶ ## $ T &lt;dbl&gt; 0.08774387, 0.12245643, 0.13089846, 0.14956862, 0.08774387, 0.‚Ä¶ # Create the plots plt_S &lt;- estimators %&gt;% ggplot(aes(x = S)) + theme_classic() + geom_histogram(colour = &quot;black&quot;,fill = &quot;transparent&quot;,bins = 7) + coord_cartesian(ylim = c(0,250)) + geom_vline(xintercept = p0,colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) plt_T &lt;- estimators %&gt;% ggplot(aes(x = T)) + theme_classic() + geom_histogram(colour = &quot;black&quot;,fill = &quot;transparent&quot;,bins = 7) + coord_cartesian(ylim = c(0,250)) + geom_vline(xintercept = p0,colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) cowplot::plot_grid(plt_S,plt_T,nrow=1) # Compute the mean of each: estimators %&gt;% summarize_all(mean) ## # A tibble: 1 x 2 ## S T ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.101 0.103 Exercise: \\(S\\) is unbiased and \\(T\\) is biased, as shown in the book. Which estimator would you prefer? Compute a five number summary for \\(S\\) and \\(T\\) from our simulations, recreating the following: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00000 0.06667 0.10000 0.10140 0.13333 0.26667 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.03122 0.08209 0.10026 0.10346 0.12246 0.23069 Do you see any meaningful differences? Do the sampling distributions of \\(S\\) and \\(T\\) concentrate around \\(p0\\) in the same way? Now, compute the mode (most frequently-observed value) of \\(S\\) and \\(T\\). You should get the following: ## The mode of S is 0.06666667 ## The mode of T is 0.09697197 (You‚Äôre going to have to figure out how to compute a mode in R. That‚Äôs part of the exercise). What do you think about this? Does this contradict \\(S\\) being unbiased and \\(T\\) being biased? Does it change your opinion about which is a better estimator? Finally, compute the square root of the average squared distance of \\(S\\) and \\(T\\) from the true value \\(p_{0}\\). You should get: ## # A tibble: 1 x 2 ## S T ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0550 0.0299 We will see shortly that this quantity, the mean-squared error of an estimator, is much more representative of the quality of an estimator than the bias on its own. Note, of course, that since it depends on the true value, it can‚Äôt ever be computed in a real data analysis. It‚Äôs a mathematical construct. "],
["section-supplement-to-chapter-20.html", "Chapter 5 Supplement to Chapter 20 5.1 Efficiency and Mean Square Error (Chapter 20)", " Chapter 5 Supplement to Chapter 20 This chapter implements much of the analysis shown in chapter 20 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. The assigned exercises associated with this material are from MIPS, as follows: 20.1; 20.2; 20.3; 20.4; 20.5; 20.8; 20.9; 20.11. Answers to selected exercises are in the back of the book. You should also do all the ‚Äúquick exercises‚Äù within chapter 20 (solutions are at the end of the chapter). Use R as much as possible when answering the questions. 5.1 Efficiency and Mean Square Error (Chapter 20) This chapter compares estimators using the Mean Squared Error (MSE). The motivating example is estimating the number of German tanks using their observed serial numbers, assuming their serial numbers are assigned uniformly at random. Two estimators are used: one based on the sample mean, and one based on the sample maximum. First, let‚Äôs write functions to compute these two estimators, and use simulation to verify that they are unbiased. At this point in the course, you should start feeling comfortable approaching this yourself. I encourage you to try this before looking at my answer as follows: library(tidyverse) # Functions to compute the estimators T1 &lt;- function(x) 2 * mean(x) - 1 T2 &lt;- function(x) ( (length(x) + 1)/length(x) ) * max(x) - 1 # Now, simulate in order to assess their bias. # This goes as follows (try this yourself before looking): # - Choose a true value of N, the parameter to be estimated # - Draw a sample of size n from 1:N without replacement # - Compute T1 and T2 # - Repeat this M times, and compare the average of T1 and T2 to N. N &lt;- 1000 n &lt;- 10 M &lt;- 2000 # Run the simulations. Use the sample.int() function to generate from a DISCRETE # uniform distribution out &lt;- 1:M %&gt;% map(~sample.int(N,n)) %&gt;% map(~c(&quot;T1&quot; = T1(.x),&quot;T2&quot; = T2(.x))) %&gt;% reduce(bind_rows) # What do you expect the mean to be? out %&gt;% summarize_all(mean) ## # A tibble: 1 x 2 ## T1 T2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1004. 998. # Why does this seem to indicate that T1 and T2 have zero bias? # Recreate the plots in Figure 20.1: leftplot &lt;- out %&gt;% ggplot(aes(x = T1)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + scale_x_continuous(breaks = c(300,700,1000,1300,1600)) + coord_cartesian(xlim = c(300,1600)) rightplot &lt;- out %&gt;% ggplot(aes(x = T2)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + scale_x_continuous(breaks = c(300,700,1000,1300,1600)) + coord_cartesian(xlim = c(300,1600)) cowplot::plot_grid(leftplot,rightplot,nrow = 1) Why does \\(T2\\) seem to have a maximum possible value? Can you compute this mathematically? library(tidyverse) "],
["section-supplement-to-evans-rosenthal-section-7-1.html", "Chapter 6 Supplement to Evans &amp; Rosenthal Section 7.1 6.1 Tutorial 6.2 Interactive App", " Chapter 6 Supplement to Evans &amp; Rosenthal Section 7.1 This chapter is a supplement to Chapter 7: Bayesian Inference, section 1: The Prior and Posterior Distributions. This is to support the Bayesian Inference sections of STA238. Materials in this tutorial are taken from Alex‚Äôs comprehensive tutorial on Bayesian Inference, which is very long and outside the scope of this course. The assigned exercises associated with this material are from Evans and Rosenthal (E&amp;R), as follows: 7.1.1, 7.1.2, 7.1.3; 7.1.4; 7.1.8; 7.1.9; 7.1.13, 7.1.14, 7.1.15. 6.1 Tutorial In this example we will discuss at length the Beta-Bernoulli example from section 7.1. First follow along with the tutorial, then check out the interactive app. Given data generated from some family of probability distributions indexed by unknown parameter, statistical inference is concerned with estimating these parameters- finding reasonable values for them, given the observed data. The central notion is that of uncertainty: we simply don‚Äôt know the values of the parameters that generated the data we observed, and we do know that several different values could reasonably have generated these data. Probability is the mathematical construct used to represent uncertainty. 6.1.1 Frequentist/Likelihood Perspective Classically, the approach to this problem is taught from the frequentist perspective. Uncertainty in the values of the parameters that generated the data is represented by probability via the notion of repeated sampling: under the given probability model with the given parameter values, what is the relative frequency with which these same data would be observed, if the experiment that generated the data were repeated again and again? Values of the parameters that have a higher probability of having generated the observed data are thought to be more likely than other values. As an example, consider a coin with unknown probability of heads \\(\\theta\\). We toss the coin once, and observe random outcome (data) \\(X = 1\\) if the toss is heads and \\(X = 0\\) if not. For simplicity, suppose we know we have chosen one of two possible coins, either having \\(\\theta = 0.7\\) or \\(\\theta = 0.3\\). How do we use the observed data to infer which of these two coins we threw? For any \\(0 &lt; \\theta &lt; 1\\), the probability distribution of the single coin toss is given by \\[ P(X = x) = \\theta^{x}(1-\\theta)^{1-x} \\] This just says that \\(P(X = 1) = \\theta\\) and \\(P(X = 0) = 1-\\theta\\). Let‚Äôs say we throw the coin once, and observe \\(X = 1\\). If \\(\\theta = 0.7\\) the probabilty of observing this result is \\(P(X = 1|\\theta = 0.7) = 0.7\\). That is if \\(\\theta = 0.7\\), we would expect roughly \\(70\\%\\) of repetitions of this experiment to yield the same results as we observed in our data. If \\(\\theta = 0.3\\) on the other hand, \\(P(X = 1|\\theta = 0.3) = 0.3\\); only \\(30\\%\\) of the repetitions of this experiment would yield the observed data if \\(\\theta = 0.3\\). Because \\(\\theta = 0.7\\) would yield the observed data more frequently than \\(\\theta = 0.3\\), we say that \\(\\theta = 0.7\\) is more likely to have generated the observed data than \\(\\theta=0.3\\), and our inference favours \\(\\theta =0.7\\). 6.1.2 Bayesian Inference: introduction One criticism of the above approach is that is depends not only on the observed data, but also on infinitely many other possible datasets that are not observed. This is an artifact of the manner in which probability is used to represent uncertainty. In contrast, Bayesian statistics represents uncertainty about the value of a parameter directly using probability distributions. In particular, a prior distribution is placed on the parameter, representing the probable values of that parameter before data is observed. Having observed the data, the prior is updated via Bayes‚Äô Rule, yielding the posterior distribution of the parameter, given the data. The choice of prior distribution is based either on subject-matter knowledge or mathematical convenience, and is a subjective choice on the part of the analyst. Don‚Äôt worry too much about it in this course‚Äì you should get comfortable with the idea and the math. We‚Äôd talk more about it in a more advanced applied statistics course. To see how this works, suppose that we think about \\(4/5\\) coins in our pocket are the \\(\\theta = 0.3\\) coins, and only \\(1/5\\) are the \\(\\theta = 0.7\\) coins. The parameter space here is \\(\\Theta = \\left\\{ 0.3, 0.7\\right\\}\\). Our prior distribution on \\(\\theta\\) is then \\[ P(\\theta = q) = 0.2^{I(q = 0.7)}0.8^{I(q = 0.3)}, \\ q\\in\\Theta \\] where \\(I(q = 0.7) = 1\\) if \\(q = 0.7\\) and \\(0\\) otherwise. This, like the probability distribution of the actual result of the coin toss, just encodes our notion that \\(P(\\theta = 0.3) = 0.8\\) and \\(P(\\theta = 0.7) = 0.2\\). So without knowing the result of the coin toss, we think there is a \\(20\\%\\) chance that \\(\\theta = 0.7\\). We know from above that if we observe heads on the coin toss, we have observed a result that would occur about \\(70\\%\\) of the time if \\(\\theta = 0.7\\). In probability terms, we have a marginal distribution for \\(\\theta\\), and a conditional distribution for \\(X|\\theta\\). These two ideas are combined by computing the conditional distribution of \\(\\theta|X\\), known as the posterior distribution for \\(\\theta\\) having observed \\(X\\). This is obtained (explaining the name) via Bayes‚Äô Rule: \\[ p(\\theta|X) = \\frac{p(X|\\theta)\\times p(\\theta)}{p(X)} \\] where the marginal distribution of \\(X\\), or the normalizing constant or marginal likelihood or model evidence (this thing has a lot of names) is given by \\[ p(X) = \\sum_{\\theta\\in\\Theta}p(X|\\theta)\\times p(\\theta) \\] and ensures \\(p(\\theta|X)\\) is a proper probability distribution. Exercise: verify that \\(p(\\theta|X)\\) is a valid probability density on \\(\\Theta\\) by verifying that \\(\\sum_{\\theta\\in\\Theta}p(\\theta|X) = 1\\). In our example, the prior probability of \\(\\theta = 0.7\\) is only \\(20\\%\\). But we flip the coin and observe \\(X = 1\\). We can see how this observation updates our belief about the likely values of \\(\\theta\\) by computing the posterior distribution of \\(\\theta\\) given the observed data: \\[ \\begin{aligned} &amp;p(\\theta|X) = \\frac{\\theta^{x}(1-\\theta)^{1-x}\\times 0.2^{I(\\theta = 0.7)}0.8^{I(\\theta = 0.3)}}{\\sum_{\\theta = 0.3,0.7}\\theta^{x}(1-\\theta)^{1-x}\\times 0.2^{I(\\theta = 0.7)}0.8^{I(\\theta = 0.3)}} \\\\ \\implies&amp; P(\\theta = 0.7 | X = 1) = \\frac{0.7 \\times 0.2}{0.7\\times0.2 + 0.3\\times0.8} \\\\ &amp;= 0.368 \\end{aligned} \\] Before observing heads, we would have thought the \\(\\theta = 0.7\\) coin to be very unlikely, but because the observed data favours \\(\\theta = 0.7\\) more strongly than \\(\\theta = 0.3\\), after observing these data we feel that \\(\\theta = 0.7\\) is more likely than before. 6.1.3 Flipping More Coins Suppose now that we flip \\(n\\) coins, obtaining a dataset \\(X = (X_{1},\\ldots,X_{n})\\) of heads or tails, represented by 0‚Äôs and 1‚Äôs. If we‚Äôre still considering only two candidate values \\(\\theta = 0.7\\) or \\(\\theta = 0.3\\), we may still ask the question ‚Äúwhich value is more likely to have generated the observed data?‚Äù. We again form the likelihood function for each value of \\(\\theta\\), the relative frequency with which each value of \\(\\theta\\) would have generated the observed sample. Assuming the tosses are statistically independent: \\[ p(X|\\theta) = \\theta^{\\sum_{i=1}^{n}X_{i}} \\times (1 - \\theta)^{n - \\sum_{i=1}^{n}X_{i}} \\] where \\(\\sum_{i=1}^{n}X_{i}\\) is just the number of heads observed in the sample. We see that any two samples that have the same number of heads will lead to the same inferences about \\(\\theta\\) in this manner. Suppose we throw the coin \\(10\\) times and observe \\(6\\) heads. The likelihood function for each candidate value of \\(\\theta\\) is \\[ \\begin{aligned} p(X|\\theta = 0.7) &amp;= 0.7^{6} \\times 0.3^{4} = 0.000953 \\\\ p(X|\\theta = 0.3) &amp;= 0.3^{6} \\times 0.7^{4} = 0.000175 \\\\ \\end{aligned} \\] It is much more likely to observe \\(6\\) heads when \\(\\theta = 0.7\\) than when \\(\\theta = 0.3\\). Exercise: calculate the likelihood function for \\(\\theta\\) when \\(n = 100\\) and \\(\\sum_{i=1}^{n}X_{i} = 60\\). How much more likely is \\(\\theta = 0.7\\) than \\(\\theta = 0.3\\) now? In the Bayesian setting, with our prior distribution on \\(\\theta\\) from above, we would form the posterior distribution as follows: \\[ p(\\theta|X) = \\frac{\\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}}\\times 0.2^{I(\\theta = 0.7)}0.8^{I(\\theta = 0.3)}}{\\sum_{\\theta = 0.3,0.7}\\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}}\\times 0.2^{I(\\theta = 0.7)}0.8^{I(\\theta = 0.3)}} \\] Computing this for our observed data of \\(\\sum_{i=1}^{10}x_{i} = 6\\) yields \\[ \\begin{aligned} p(\\theta = 0.7 |X) = \\frac{0.7^{6}0.3^{4}\\times 0.2}{0.7^{6}0.3^{4}\\times 0.2 + 0.3^{6}0.7^4\\times0.8} = 0.576 \\\\ p(\\theta = 0.3 |X) = \\frac{0.3^{6}0.7^{4}\\times 0.2}{0.3^{6}0.7^{4}\\times 0.8 + 0.7^{6}0.3^4\\times0.2} = 0.424 \\\\ \\end{aligned} \\] We can see that the data ‚Äúupdates‚Äù our prior belief that \\(\\theta = 0.3\\) was more probable than \\(\\theta = 0.7\\), because the observed data was more likely to have occurred if \\(\\theta = 0.7\\) than if \\(\\theta = 0.3\\). 6.1.4 Visualization It is helpful to visualize the prior and posterior, for the observed data. Because both prior and posterior only allow two values, we can do this using a simple bar chart: visualize_binomial_priorposterior &lt;- function(sumx,n) { prior &lt;- function(theta) { if (theta == .3) { return(.8) } else if (theta == .7) { return(.2) } 0 } likelihood &lt;- function(theta) theta^sumx * (1-theta)^(n - sumx) marginal_likelihood &lt;- prior(.7) * likelihood(.7) + prior(.3) * likelihood(.3) posterior &lt;- function(theta) likelihood(theta) * prior(theta) / marginal_likelihood # Plot of the prior and posterior distributions for these observed data data_frame( theta = c(.3,.7,.3,.7), value = c(prior(.3),prior(.7),posterior(.3),posterior(.7)), type = c(&quot;Prior&quot;,&quot;Prior&quot;,&quot;Posterior&quot;,&quot;Posterior&quot;) ) %&gt;% ggplot(aes(x = theta,y = value,fill = type)) + theme_classic() + geom_bar(stat = &quot;identity&quot;,position = &quot;dodge&quot;,colour = &quot;black&quot;) + labs(title = &quot;Prior and Posterior for theta&quot;, subtitle = str_c(&quot;Observed data: &quot;,sumx,&quot; flips in &quot;,n,&quot; throws&quot;), x = &quot;Theta, probability of heads&quot;, y = &quot;Prior/Posterior Probability&quot;, fill = &quot;&quot;) + scale_x_continuous(breaks = c(0.30,0.70),labels = c(&quot;0.30&quot;,&quot;0.70&quot;)) + scale_y_continuous(labels = scales::percent_format()) + scale_fill_brewer(palette = &quot;Reds&quot;) } Plotting is nice as it lets us compare how different observed data, and different experiments (number of throws) affect the prior/posterior balance of belief: cowplot::plot_grid( visualize_binomial_priorposterior(6,6), visualize_binomial_priorposterior(6,10), visualize_binomial_priorposterior(6,20), visualize_binomial_priorposterior(6,50), visualize_binomial_priorposterior(0,10), visualize_binomial_priorposterior(1,10), visualize_binomial_priorposterior(7,10), visualize_binomial_priorposterior(10,10), ncol=2 ) 6.2 Interactive App Go to the app: https://awstringer1.shinyapps.io/bayesian-tutorial/ The app lets you flip coins and estimate the probability of heads using Frequentist and Bayesian methods. We haven‚Äôt covered estimation yet, but we have covered the model for coin flipping in both contexts now, so you should be able to tell what‚Äôs happening. Also shown are interval estimates, which measure the strength of the conclusions about \\(p\\) that are made based on the data and model. Narrower interval estimates mean we‚Äôre more sure about the value of \\(p\\), after seeing the data. The app lets you change the following: The number of times you flip the coin, The true probability of heads, \\(p\\), Your prior belief about the probability of heads, the ‚Äúprior mean‚Äù, and The strength of your prior beliefs, as measured by the prior standard deviation. Lower standard deviation means you‚Äôre more sure about the value of \\(p\\), before seeing any flips. You should answer the following questions: How many flips do you need before the Bayesian and frequentist inferences agree closely? Does this depend on the true value of \\(p\\), your prior belief, and the strength of your prior belief? Intuitively: why are the Bayesian interval estimates narrower than the frequentist ones? Is this always the case? Can you ‚Äúbreak‚Äù the Bayesian answer by expressing really strong and wrong prior beliefs? Can you ‚Äúfix‚Äù it by flipping the coin more times? "],
["section-supplement-to-chapter-18.html", "Chapter 7 Supplement to Chapter 18 7.1 The Bootstrap (Chapter 18)", " Chapter 7 Supplement to Chapter 18 This chapter implements much of the analysis shown in chapter 18 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. The assigned exercises associated with this material are from MIPS, as follows: 18.1 18.2; 18.3, 18.4; 18.6; 18.7; 18.8; 18.9; 18.11. Answers to selected exercises are in the back of the book. You should also do all the ‚Äúquick exercises‚Äù within chapter 18 (solutions are at the end of the chapter). Use R as much as possible when answering the questions. 7.1 The Bootstrap (Chapter 18) The bootstrap is one of the most foundational computational tools in modern statistics. It as important on the computation side as basic calculus is on the math side. Any time you have an estimator and are interested in its repeated sampling properties (which is much of what statistics is), you can simulate data, compute the estimator, and just look at the results to get an idea of what‚Äôs going on. This is called the ‚Äúparametric‚Äù bootstrap, and you have been doing it for the whole course in these supplementary notes. This chapter introduces the ‚Äúempirical‚Äù bootstrap, which allows you to use this idea when you don‚Äôt know the distribution of the data! The idea is to sample with replacement from the data that you have. A lot of the statistical properties of estimators are still recoverable under this type of sampling. It seems like magic. Indeed, the name ‚Äúbootstrap‚Äù comes from the old expression ‚Äúlift yourself up by your bootstraps‚Äù, which roughly means to do something seemingly impossible or contradictory in order to better your situation. One of the best-named concepts in all of stats, in my opinion. 7.1.1 Empirical bootstrap: Old Faithful data Let‚Äôs recreate Figure 18.1: library(tidyverse) # Read in old faithful, copied from chapter 1: oldfaithful &lt;- readr::read_csv( file = &quot;data/MIPSdata/oldfaithful.txt&quot;, # Tell it where the file is col_names = &quot;time&quot;, # Tell it that the first row is NOT column names, and at the same time, tell it what name you want for the column. col_types = &quot;n&quot; # Tell it that there is one column, and it is &quot;numeric&quot; (n) ) # Check what was read in using the dplyr::glimpse() function dplyr::glimpse(oldfaithful) ## Observations: 272 ## Variables: 1 ## $ time &lt;dbl&gt; 216, 108, 200, 137, 272, 173, 282, 216, 117, 261, 110, 235,‚Ä¶ # Bootstrap: sample with replacement set.seed(45356) # So you can reproduce my results B &lt;- 1000 # Number of bootstrap resamples to take n &lt;- nrow(oldfaithful) # Sample size bootstrapmeans &lt;- 1:B %&gt;% map(~sample(oldfaithful$time,n,replace = TRUE)) %&gt;% map(~(mean(.x) - mean(oldfaithful$time))) %&gt;% # Compute the CENTRED mean reduce(c) # Plot the hisrogram and density estimate together tibble(x = bootstrapmeans) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 20,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + geom_density() + scale_x_continuous(breaks = seq(-18,18,by=6)) + coord_cartesian(xlim = c(-18,18)) Exercise: compute the book‚Äôs estimate of \\(P(|\\bar{X}_{n} - \\mu|)\\). Using the above set.seed(45356) I got the following (slightly different than the book): ## [1] 0.234 Exercise: now, repeat the entire bootstrap procedure \\(1000\\) times and plot a histogram and kernel density estimate of \\(P(|\\bar{X}_{n} - \\mu|)\\). I got the following with set.seed(8768432): 7.1.2 Parametric Bootstrap: software data Read in the software data. Try to do it yourself before looking: head data/MIPSdata/software.txt wc -l data/MIPSdata/software.txt ## 30 ## 113 ## 81 ## 115 ## 9 ## 2 ## 91 ## 112 ## 15 ## 138 ## 136 data/MIPSdata/software.txt software &lt;- readr::read_csv( file = &quot;data/MIPSdata/software.txt&quot;, col_names = &quot;time&quot;, col_types = &quot;n&quot; ) glimpse(software) ## Observations: 136 ## Variables: 1 ## $ time &lt;dbl&gt; 30, 113, 81, 115, 9, 2, 91, 112, 15, 138, 50, 77, 24, 108, ‚Ä¶ Plot the CDF of the estimated distribution against the empirical CDF of the data. The \\(t_{ks}\\) described in the book is the maximum vertical distance between these two curves: lambdahat &lt;- 1/mean(software$time) samppoints &lt;- seq(0,3000,by=1) expcdf &lt;- pexp(samppoints,rate = lambdahat) empcdf &lt;- ecdf(software$time) tibble(x = samppoints,y = expcdf) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_line() + stat_function(fun = empcdf,linetype = &quot;dotdash&quot;) Compute a bootstrap sample of \\(T_{ks}\\): set.seed(97876856) compute_tks &lt;- function() { samppoints &lt;- seq(0,3000,by=1) n &lt;- nrow(software) samp &lt;- rexp(n,rate = lambdahat) ecdfboot &lt;- ecdf(samp)(samppoints) expcdf &lt;- pexp(samppoints,rate = lambdahat) max(abs(ecdfboot - expcdf)) } B &lt;- 1000 tksboot &lt;- 1:B %&gt;% map(~compute_tks()) %&gt;% reduce(c) tibble(x = tksboot) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 20,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + geom_density() + scale_x_continuous(breaks = c(seq(0,.2,by=.05),.176)) + coord_cartesian(xlim = c(0,.2)) Exercise: the above compute_tks() is a bit tricky. Write out the exact algorithm I use to compute \\(T_{ks}\\). Basically I am asking you to expand on step 2. of the algorithm presented in the book on page 279. 7.1.3 Extended example: the standard error of a proportion Dozens of news articles are published every day with claims that ‚Äúxx% of Canadians say that yy‚Äù, where xx is some percentage and yy is some claim that the news company wants to make. These articles have a significant impact on our public discourse. How could it be that every day, there are so many earth-shattering claims? It‚Äôs almost like most of them aren‚Äôt actually true. But then why are they published, and why do we believe them? The missing ingredient is variability. These news articles most often only report point estimates‚Äì the ‚Äúxx%‚Äù. They don‚Äôt report the standard error of these estimates, which give you a measure of the variability, and hence uncertainty, in the conclusions that are made based off of them. We can use the bootstrap to get an idea of the variability in estimates based off of surveys. Because most news articles don‚Äôt link to the raw data (another reason not to believe the stuff that these for-profit companies push), we‚Äôll use a general budget survey from the Government of Canada, with responses from Alberta residents. You can get the data in excel format https://open.canada.ca/data/en/dataset/2a5992c4-a050-46b8-b735-641cbc799a8e. You should open the file in excel and save it as a .csv, or if you don‚Äôt have excel, I put the .csv file I used in the data folder. Let‚Äôs read the data into R. We know it has a header row (because we opened it in excel). There is a numeric ID column, two date columns (which we don‚Äôt need), and 58 numeric response columns. I removed the first three (blank) and fourth (information) rows from my .csv, and you should do the same in excel if you have it. col_types &lt;- c( &quot;ncc&quot;, stringr::str_c(rep(&quot;n&quot;,58),collapse = &quot;&quot;) ) %&gt;% stringr::str_c(collapse = &quot;&quot;) col_types # Understand the above code by running it in pieces and looking at the results. ## [1] &quot;nccnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn&quot; budget &lt;- readr::read_csv( file = &quot;data/budget.csv&quot;, col_names = TRUE, col_types = col_types ) ## Warning: Missing column names filled in: &#39;X17&#39; [17], &#39;X18&#39; [18], ## &#39;X19&#39; [19], &#39;X20&#39; [20], &#39;X21&#39; [21], &#39;X22&#39; [22], &#39;X23&#39; [23], &#39;X24&#39; [24], ## &#39;X26&#39; [26], &#39;X27&#39; [27], &#39;X28&#39; [28], &#39;X29&#39; [29], &#39;X30&#39; [30], &#39;X31&#39; [31], ## &#39;X33&#39; [33], &#39;X34&#39; [34], &#39;X35&#39; [35], &#39;X36&#39; [36], &#39;X37&#39; [37], &#39;X38&#39; [38], ## &#39;X39&#39; [39], &#39;X40&#39; [40], &#39;X41&#39; [41], &#39;X42&#39; [42], &#39;X44&#39; [44], &#39;X45&#39; [45], ## &#39;X46&#39; [46], &#39;X47&#39; [47], &#39;X48&#39; [48], &#39;X49&#39; [49], &#39;X50&#39; [50], &#39;X51&#39; [51], ## &#39;X52&#39; [52], &#39;X53&#39; [53], &#39;X55&#39; [55], &#39;X56&#39; [56], &#39;X57&#39; [57], &#39;X58&#39; [58], ## &#39;X59&#39; [59] glimpse(budget) ## Observations: 40,513 ## Variables: 61 ## $ `Respond-ent ID` &lt;dbl&gt; ‚Ä¶ ## $ StartDate &lt;chr&gt; ‚Ä¶ ## $ CompletedDate &lt;chr&gt; ‚Ä¶ ## $ `Q1: To what extent do you think low oil prices impact the Alberta government&#39;s ability to budget?` &lt;dbl&gt; ‚Ä¶ ## $ `Q2: How concerned are you about the $7B shortfall?` &lt;dbl&gt; ‚Ä¶ ## $ `Q3: Should government take action now, wait six months, or hold out for high oil prices?` &lt;dbl&gt; ‚Ä¶ ## $ `Q4: Government needs:` &lt;dbl&gt; ‚Ä¶ ## $ `Question 5 a) Cut spending` &lt;dbl&gt; ‚Ä¶ ## $ `Question 5 b) Raise taxes and user fees` &lt;dbl&gt; ‚Ä¶ ## $ `Question 5 c) Borrow money` &lt;dbl&gt; ‚Ä¶ ## $ `Question 6 a) Reduce expenditures` &lt;dbl&gt; ‚Ä¶ ## $ `Question 6 b) Raise taxes and user fees` &lt;dbl&gt; ‚Ä¶ ## $ `Question 6 c) Run a deficit` &lt;dbl&gt; ‚Ä¶ ## $ `Q7: Are current tax rates in Alberta higher, above average or lower?` &lt;dbl&gt; ‚Ä¶ ## $ `Q8: How important is it that AB taxes are lower?` &lt;dbl&gt; ‚Ä¶ ## $ `Q9: In what ways can the government act to increase revenue without jeopardizing Alberta&#39;s competitive position? (1 = selected; 0 = not selected)` &lt;dbl&gt; ‚Ä¶ ## $ X17 &lt;dbl&gt; ‚Ä¶ ## $ X18 &lt;dbl&gt; ‚Ä¶ ## $ X19 &lt;dbl&gt; ‚Ä¶ ## $ X20 &lt;dbl&gt; ‚Ä¶ ## $ X21 &lt;dbl&gt; ‚Ä¶ ## $ X22 &lt;dbl&gt; ‚Ä¶ ## $ X23 &lt;dbl&gt; ‚Ä¶ ## $ X24 &lt;dbl&gt; ‚Ä¶ ## $ `Q10: If the government needs to increase its revenues through taxation, are there options you feel should NOT be considered? (1 = selected; 0 = not selected)` &lt;dbl&gt; ‚Ä¶ ## $ X26 &lt;dbl&gt; ‚Ä¶ ## $ X27 &lt;dbl&gt; ‚Ä¶ ## $ X28 &lt;dbl&gt; ‚Ä¶ ## $ X29 &lt;dbl&gt; ‚Ä¶ ## $ X30 &lt;dbl&gt; ‚Ä¶ ## $ X31 &lt;dbl&gt; ‚Ä¶ ## $ `Q11: Where would you tolerate cuts? (1 = Selected; 0 = Not Selected)` &lt;dbl&gt; ‚Ä¶ ## $ X33 &lt;dbl&gt; ‚Ä¶ ## $ X34 &lt;dbl&gt; ‚Ä¶ ## $ X35 &lt;dbl&gt; ‚Ä¶ ## $ X36 &lt;dbl&gt; ‚Ä¶ ## $ X37 &lt;dbl&gt; ‚Ä¶ ## $ X38 &lt;dbl&gt; ‚Ä¶ ## $ X39 &lt;dbl&gt; ‚Ä¶ ## $ X40 &lt;dbl&gt; ‚Ä¶ ## $ X41 &lt;dbl&gt; ‚Ä¶ ## $ X42 &lt;dbl&gt; ‚Ä¶ ## $ `Q12: And are there options you feel should NOT be touched? (1 = Selected; 0 = Not selected)` &lt;dbl&gt; ‚Ä¶ ## $ X44 &lt;dbl&gt; ‚Ä¶ ## $ X45 &lt;dbl&gt; ‚Ä¶ ## $ X46 &lt;dbl&gt; ‚Ä¶ ## $ X47 &lt;dbl&gt; ‚Ä¶ ## $ X48 &lt;dbl&gt; ‚Ä¶ ## $ X49 &lt;dbl&gt; ‚Ä¶ ## $ X50 &lt;dbl&gt; ‚Ä¶ ## $ X51 &lt;dbl&gt; ‚Ä¶ ## $ X52 &lt;dbl&gt; ‚Ä¶ ## $ X53 &lt;dbl&gt; ‚Ä¶ ## $ `Q13: Indicate whether you Don&#39;t Know, Disagree or Agree. (1 = Don&#39;t know; 2 = Disagree; 3 = Agree)` &lt;dbl&gt; ‚Ä¶ ## $ X55 &lt;dbl&gt; ‚Ä¶ ## $ X56 &lt;dbl&gt; ‚Ä¶ ## $ X57 &lt;dbl&gt; ‚Ä¶ ## $ X58 &lt;dbl&gt; ‚Ä¶ ## $ X59 &lt;dbl&gt; ‚Ä¶ ## $ `Q15: Years you&#39;ve lived in AB` &lt;dbl&gt; ‚Ä¶ ## $ `Q16: What part of the province do you live in?` &lt;dbl&gt; ‚Ä¶ Ugly! There are some missing column names that were in the fourth row. If we needed all these data, we would have to go in excel and manually label the columns correctly, since the analysts at the government didn‚Äôt. Look at the column names and start to think about the kind of news headlines they could generate. I like the first one: ‚ÄúQ1: To what extent do you think low oil prices impact the Alberta government‚Äôs ability to budget?‚Äù could lead to things like ‚ÄúAlbertans have given up hope: more than half of Albertans believe their province is unable to budget due to the low price of oil.‚Äù Hey, it generates clicks! Here, let‚Äôs estimate the proportion of Albertans who would answer \\(1\\) (‚ÄúA Great Deal‚Äù) to the above question. Exercise: there are 40513 survey responses. The number who respond 1 can be modelled as a Binomial random variable. Derive and calculate the standard deviation of the sample proportion of respondents who answered 1. I got \\(0.0025\\). Is this higher or lower than you would expect with this number of respondents? set.seed(29394032) origsamp &lt;- as.numeric(budget[[&quot;Q1: To what extent do you think low oil prices impact the Alberta government&#39;s ability to budget?&quot;]] == 1) n &lt;- length(origsamp) # Convert to 0/1 indicator of response == 1 # Sample proportion: phat &lt;- mean(origsamp) phat ## [1] 0.5377533 # Standard error (you should DERIVE this formula): sqrt(phat*(1-phat)/n) ## [1] 0.00247703 # So the mean is .5378 and the standard error is about 0.0025 (theoretically). # Bootstrap: repeatedly resample and calculate the mean. The estimated standard error # of the sample mean is then the standard error of these means. doboot &lt;- function(B) { 1:B %&gt;% map(~sample(origsamp,n,replace = TRUE)) %&gt;% map(mean) %&gt;% reduce(c) } bootmeans &lt;- doboot(1000) mean(bootmeans) # Pretty close! ## [1] 0.5377662 sd(bootmeans) # Not bad at all! ## [1] 0.00244279 tibble(x = bootmeans) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + stat_function(fun = rlang::as_function(~n * dbinom(round(n * .x),size = n,prob = phat))) + labs(title = &quot;Bootstrap distribution of the sample mean&quot;, subtitle = &quot;Curve: theoretical distribution&quot;) Exercise: inside stat_function(fun = rlang::as_function(~n * dbinom(round(n * .x),size = n,prob = phat))) I specify the theoretical density of the sample mean. Derive this formula yourself using the change of variables formula. Exercise: derive an appropriate normal approximation to the binomial distribution for use in this example. Add it to the above plot using another stat_function call, with colour = &quot;red&quot; and linetype = &quot;dotdash&quot;. I got the following: The normal approximation looks pretty good! Exercise: suppose that based on this survey, a newspaper claims that ‚ÄúAlbertans have given up hope: more than half of Albertans believe their province is unable to budget due to the low price of oil.‚Äù. Use the bootstrap to calculate the probability that their claim is true, by considering whether each resampled sample mean is greater than .5. Using set.seed(5647244), and \\(B = 1000\\) resamples, I got the following: ## [1] 1 What do you think about this? Is the newspaper‚Äôs claim substantiated by the data? Exercise: surveys usually aren‚Äôt this big. Repeat the above calculation but on a subset of only \\(n = 200\\) people. What do you get now? Use newsample &lt;- sample(origsamp,200,replace = FALSE) and then repeate the bootstrapping on the newsample. Use the same random seed. ## [1] 0.688 Are you going to check the sample size next time you read a news article that reports survey results? "],
["section-supplement-to-chapter-21.html", "Chapter 8 Supplement to Chapter 21 8.1 Maximum Likelihood (Chapter 21)", " Chapter 8 Supplement to Chapter 21 This chapter implements much of the analysis shown in chapter 21 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. The assigned exercises associated with this material are from MIPS, as follows: 21.1; 21.2; 21.3; 21.4; 21.5; 21.6; 21.8; 21.9; 21.11; 21.14. Answers to selected exercises are in the back of the book. You should also do all the ‚Äúquick exercises‚Äù within chapter 21 (solutions are at the end of the chapter). Use R as much as possible when answering the questions. library(tidyverse) 8.1 Maximum Likelihood (Chapter 21) The concept of Maximum Likelihood, and the Likelihood function itself, is one of the single most important concepts in all of statistics. You need to understand this concept. I find a simulation is helpful. 8.1.1 Example: two coins Suppose I have two coins in my pocket. One of them has probability of heads \\(p_{1} = 0.7\\) and the other has probability of heads \\(p_{2} = 0.4\\). I pull one out and hand it to you. Your task is to guess which coin it is. You are only allowed to flip it once. To make this concrete, you flip the coin once and observe one realization of a random variable \\(X\\), which equals \\(1\\) (heads) or \\(0\\) (tails) with probability given either by \\(p = p_{1}\\) or \\(p = p_{2}\\). But you don‚Äôt know which one. Your task is to guess (infer) the value of the probability of heads, \\(p\\), based on the data, \\(X\\). The Maximum Likelihood Principle as quoted in the book says that you should pick the value of \\(p\\) under which your observed data is the most likely‚Äì would occur with the highest relative frequency, if the data-generating experiment were repeated over and over again. The likelihood function is the probability distribution of the observed data \\(X\\), treated as a function of the unknown parameter \\(p\\). That means for every sample you get, you get a different likelihood function. It‚Äôs always a function of \\(p\\), but it‚Äôs a different function of \\(p\\) for different observed data \\(X\\). In our example, our likelihood functions for \\(X=1\\) and \\(X=0\\) are functions from the set \\(\\{p_{1},p_{2}\\}\\mapsto\\mathbb{R}\\), i.e. they are only defined at the two points \\(p = p_{1}\\) and \\(p = p_{2}\\). The distribution of \\(X\\) is \\(\\text{Bernoulli}(p)\\). Suppose \\(X=1\\). The likelihood function \\(L(p)\\) is then defined by \\[\\begin{equation} L(p_{1};x = 1) = 0.7, \\ L(p_{2}; x = 1) = 0.4 \\end{equation}\\] Exercise: derive the likelihood function for tails, \\(L(p;x = 0)\\). The likelihood function \\(L(p;x)\\) is the relative frequency with which the observed value \\(X = x\\) would be observed in repeated sampling at that value of the parameter \\(p\\). This is kind of a mouthful. Because the likelihood function is defined in terms of a relative frequency, we can write a simulation that illustrates it. I will do this for \\(X = 1\\) and then you can do it for \\(X = 0\\). # Simulation to recreate the likelihood function for the two-coin example. # The likelihood function is the relative frequency with which your sample # would occur in repeated sampling, for a particular value of the parameter. # # Suppose you flip the coin and get heads. The likelihood function at p1 = 0.7 # is the relative frequency with which you would get heads if you flipped the coin # over and over, if p really equalled 0.7. # # The likelihood function at p1 = 0.4 is the relative frequency with which you # would get heads if you flipped the coin over and over, if p really equalled 0.4. # # Let&#39;s do it: simulate_likelihood_x1 &lt;- function(p) { # Make sure p is in the range of possible values if (!(p %in% c(.7,.4))) stop(stringr::str_c(&quot;Wrong value for p. p should be .7 or .4. You gave p = &quot;,p)) # Sample the data repeatedly according to this p N &lt;- 1000 repeatedsampling &lt;- sample(c(1,0),N,replace = TRUE,prob = c(p,1-p)) # Sample 1/0 with prob p/(1-p), N times # Return the relative frequency with which x == 1 mean(repeatedsampling == 1) } # Check it out: set.seed(478032749) simulate_likelihood_x1(.7) # Should be around .7 ## [1] 0.695 simulate_likelihood_x1(.4) # Should be around .4 ## [1] 0.389 Exercise: write a simulation for the likelihood function for \\(x = 0\\). Call it simulate_likelihood_x0. You should get the following: set.seed(8907968) simulate_likelihood_x0(.7) ## [1] 0.301 simulate_likelihood_x0(.4) ## [1] 0.613 8.1.2 Example: unknown coins, \\(n = 2\\) The two-coin example illustrates that ML (Maximum Likelihood) follows human intuition: if I tell you to guess which of two coins I flipped based on the result of one flip, you‚Äôre going to try and maximize your chances of being right. You do this by choosing the coin that is most likely to give the result you observed. The book talks about the likelihood function being popularized by R.A. Fisher in his seminal 1922 paper, On the Mathematical Foundations of Theoretical Statistics. What he actually says is as follows: We must return to the actual fact that one value of \\(p\\), of the frequency of which we know nothing, would yield the observed result three times as frequently as would another value of \\(p\\). If we need a word to characterize this relative property of different values of \\(p\\), I suggest without confusion that we may speak of the likelihood of one value of \\(p\\) being thrice the likelihood of another, bearing in mind that likelihood here is not used loosely as a synonym of probability, but simply to express the relative frequencies with which such values of the hypothetical quantity \\(p\\) would in fact yield the observed sample. I like this quote, even though the language at that time was less direct than we‚Äôre used to now, I think Fisher explains the concept better than any modern textbook I‚Äôve ever read. The two-coin example is a bit simple, but it is the most intuitive. Let‚Äôs extend it to the more realistic case where the parameter \\(p\\) can be any value in the open interval \\((0,1)\\). That is, I pull a coin out of my pocket, and you flip it once and have to tell me what you think the probability of heads is. Exercise: show that the likelihood function here for \\(X = 1\\) and \\(X = 0\\) is \\[\\begin{equation} L(p;x = 1) = p, \\ L(p;x = 0) = 1 - p \\end{equation}\\] or more generally, \\[\\begin{equation} L(p;x) = p^x (1-p)^{1-x} \\end{equation}\\] How do we use the observed data to estimate \\(p\\)? We find the value of \\(p\\) which would generate the sample we saw with the highest relative frequency. We maximize the likelihood function, which gives the maximum likelihood estimator \\(\\hat{p}\\). Let‚Äôs do this for this example. The log-likelihood is \\[\\begin{equation} \\ell(p;x) = \\log L(p;x) = x\\log p + (1-x)\\log (1-p) \\end{equation}\\] Exercise: show that the unique global maximum of \\(\\ell(p)\\) on the interval \\([0,1]\\) is \\(p = x\\). Why did I use a closed interval here and an open one above? There‚Äôs a problem: flipping the coin only once doesn‚Äôt really give us enough information to accurately estimate the probability of heads. If you get heads, your best guess is intuitively just going to be \\(p = 1\\)! We can see this by plotting the likelihood function: likelihood &lt;- function(p,x) (p^x) * (1-p)^(1-x) baseplot &lt;- tibble(x = c(0,1)) %&gt;% ggplot(aes(x = x)) + theme_classic() + labs(x = &quot;p&quot;,y = &quot;Likelihood&quot;) leftplot &lt;- baseplot + stat_function(fun = likelihood,args = list(x = 1)) + labs(title = &quot;x = 1&quot;) rightplot &lt;- baseplot + stat_function(fun = likelihood,args = list(x = 0)) + labs(title = &quot;x = 0&quot;) cowplot::plot_grid(leftplot,rightplot,nrow =1) The likelihood is maximized on the closed interval \\([0,1]\\) at \\(p = 1\\) when \\(x = 1\\) and \\(p = 0\\) when \\(x = 0\\). This is the mathematical encoding of your intuition that if the coin is heads, my best guess at the relative frequency of heads is simply that the coin is always heads, because well, I‚Äôve never seen it come up tails! To get better inferences, we need to flip the coin more than once. Suppose I flip the coin \\(n\\) times and observe independent realizations of the random variable \\(X\\) which takes values \\(1\\) with probability \\(p\\) and \\(0\\) with probability \\(1 - p\\). I call these random variables \\(Y = (X_{1},\\ldots,X_{n})\\) and I denote their observed values by \\(y = (x_{1},\\ldots,x_{n})\\). So for example if \\(n = 2\\) then my random variable is \\(X = (X_{1},X_{2})\\) and if I observed a head and a tail (in that order), my realized values would be \\(y = (1,0)\\). Exercise: show that the likelihood function for the observed sample \\(y = (x_{1},\\ldots,x_{n})\\) is \\[\\begin{equation} L(p;y) = p^{\\sum_{i=1}^{n}x_{i}}(1-p)^{n - \\sum_{i=1}^{n}x_{i}} \\end{equation}\\] and the log likelihood is \\[\\begin{equation} \\ell(p;y) = \\sum_{i=1}^{n}x_{i}\\log p + \\left(n - \\sum_{i=1}^{n}x_{i}\\right)\\log (1-p) \\end{equation}\\] Use the log-likelihood to show that the maximum likelihood estimator \\(\\hat{p}\\) is \\[\\begin{equation} \\hat{p} = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i} \\end{equation}\\] Suppose now that \\(n = 2\\). There are four possible samples we could get: \\[\\begin{equation}\\begin{aligned} y_{1} &amp;= (0,0) \\\\ y_{2} &amp;= (1,0) \\\\ y_{3} &amp;= (0,1) \\\\ y_{4} &amp;= (1,1) \\\\ \\end{aligned}\\end{equation}\\] The likelihood functions for each of these are \\[\\begin{equation}\\begin{aligned} L(p;y_{1}) &amp;= (1-p)^{2} \\\\ L(p;y_{2}) &amp;= p(1-p) \\\\ L(p;y_{3}) &amp;= p(1-p) \\\\ L(p;y_{2}) &amp;= p^{2} \\\\ \\end{aligned}\\end{equation}\\] We can plot these four (three?) likelihood functions as follows. I‚Äôm using some more advanced code here; you should run it slowly, line-by-line, and get a feel for what‚Äôs happening. # Store the four samples in a (named) list ylist &lt;- list( &quot;y1&quot; = c(0,0), &quot;y2&quot; = c(1,0), &quot;y3&quot; = c(0,1), &quot;y4&quot; = c(1,1) ) likelihood &lt;- function(p,y) p^(sum(y)) * (1-p)^(sum(1-y)) makeplot &lt;- function(s) { plotname &lt;- as.character(ylist[[s]]) %&gt;% stringr::str_c(collapse = &quot;,&quot;) baseplot + stat_function(fun = likelihood,args = list(y = ylist[[s]])) + labs(title = stringr::str_c(&quot;y = &quot;,plotname)) } names(ylist) %&gt;% map(makeplot) %&gt;% cowplot::plot_grid(plotlist = .,nrow = 2) Exercise: compute the value of \\(\\hat{p}\\) for each sample. Add a verticle line to each plot at this point, using geom_vline(xintercept = ?,colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) where you replace the ? with the appropriate value. It should look like: 8.1.3 Example: unknown coins, \\(n\\) bigger than \\(2\\) For our last example, let‚Äôs flip the coin more times and look just at the likelihood function for our sample. You can flip a coin with probability of heads p n times in R by using sample(c(1,0),n,replace = TRUE,prob = c(p,1-p)). You‚Äôre going to write a function that takes in \\(n\\), generates a sample of size \\(n\\), and plots the likelihood for \\(p\\) for this sample. To do this, you need to generate the sample with a known value of \\(p\\) which we will call \\(p_{0}\\). You can then compare how your likelihood and MLE (Maximum Likelihood Estimator) look to the true value of \\(p\\) that you‚Äôre trying to estimate, \\(p_{0}\\). Start with the following: plotlikelihood &lt;- function(n,p0) { # Code goes here } You can generate the sample using the statement above. Call it samp. You can then make the plot using baseplot + stat_function(fun = likelihood,args = list(y = samp)) + labs(title = stringr::str_c(&quot;n = &quot;,n)) + geom_vline(xintercept = p0,colour = &quot;blue&quot;,linetype = &quot;dotdash&quot;) + geom_vline(xintercept = mean(samp),colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) Notice I changed the title too. I also added two verticle lines: one for the true value of \\(p\\), \\(p_{0}\\), and one for the MLE from the sample, \\(\\hat{p}\\). Put these two code chunks together into the above skeleton of a function. Then call your function as follows, and you should get the following output: set.seed(56576) plotlikelihood(10,.5) Exercise: write the function necessary to produce this output. Then with your new function, run it repeatedly, and with different values of \\(n\\) and \\(p\\). Make sure not to reset the random seed each time you run it‚Äì you want to see what happens for different samples. Think about the following things: How does the shape and location of the likelihood change as you sample the data over and over, for the same \\(n\\)? Specifically, How does the shape and location of the likelihood change when you make \\(n\\) smaller or larger? How does the shape and location of the likelihood change when you change \\(p_{0}\\)? What happens when you push \\(p_{0}\\) really close to \\(0\\) or \\(1\\)? 8.1.4 Extended example: rental housing in Toronto Recall the Toronto Rental Housing dataset from Section 2.5 of these supplementary materials. We were interested in estimating the quality of rental housing for the different wards in Toronto, where ‚Äúquality‚Äù is measured by the RentSafeTO score, which is composed of 20 sub-scores on various qualities of each building. We did this by computing the sample mean score in each ward, as follows: # Read in the data from disk apartmentdata &lt;- readr::read_csv( file = &quot;data/apartment-data/toronto-apartment-building-evaluations.csv&quot; ) ## Parsed with column specification: ## cols( ## .default = col_double(), ## EVALUATION_COMPLETED_ON = col_character(), ## PROPERTY_TYPE = col_character(), ## RESULTS_OF_SCORE = col_character(), ## SITE_ADDRESS = col_character(), ## WARD = col_character() ## ) ## See spec(...) for full column specifications. # Clean it up apartmentclean &lt;- apartmentdata %&gt;% filter(!is.na(SCORE)) %&gt;% # Remove apartments with missing scores dplyr::select(ward = WARD, score = SCORE, property_type = PROPERTY_TYPE, year_built = YEAR_BUILT, address = SITE_ADDRESS ) glimpse(apartmentclean) ## Observations: 3,437 ## Variables: 5 ## $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;0‚Ä¶ ## $ score &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, 78, 73, 76, 57‚Ä¶ ## $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SOCIAL HOUSING&quot;,‚Ä¶ ## $ year_built &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, 2015, 1970, 19‚Ä¶ ## $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACP‚Ä¶ # Make a table of average scores for private residences (i.e. not social housing) apartmentclean %&gt;% filter(property_type == &quot;PRIVATE&quot;) %&gt;% group_by(ward) %&gt;% summarize(avg_score = mean(score)) ## # A tibble: 26 x 2 ## ward avg_score ## &lt;chr&gt; &lt;dbl&gt; ## 1 01 71.5 ## 2 02 73.0 ## 3 03 70.5 ## 4 04 68.2 ## 5 05 71.7 ## 6 06 72.1 ## 7 07 69.8 ## 8 08 73.5 ## 9 09 67.5 ## 10 10 72.2 ## # ‚Ä¶ with 16 more rows Exercise: suppose the data from any single ward follows a Normal model, \\[\\begin{equation} X_{i} \\overset{iid}{\\sim}\\text{Normal}\\left(\\mu,\\sigma^{2}\\right) \\end{equation}\\] Prove that the maximum likelihood estimator of \\(\\mu\\) is \\(\\hat{\\mu} = \\bar{X}\\), the sample mean. This justifies the use of the sample mean as an estimate of the average score of all buildings in the ward. Exercise: now prove that the maximum likelihood estimator for \\(\\sigma^{2}\\) under this model is \\[\\begin{equation} \\hat{\\sigma}^{2} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i} - \\bar{X} \\right)^{2} \\end{equation}\\] Argue that based on the ‚Äúinvariance‚Äù property of the MLE, this implies that the MLE for the \\(\\sigma = \\sqrt{\\sigma^{2}}\\) is \\[\\begin{equation} \\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i} - \\bar{X} \\right)^{2}} \\end{equation}\\] Exercise: use the sd() function in R to modify the above code to compute the MLE for the standard deviation for each ward. Exercise: why are we considering each ward separately? Now let \\(X_{ij}\\) be the score of the \\(i^{th}\\) building in the \\(j^{th}\\) ward. Suppose the data follows the model \\[\\begin{equation}\\begin{aligned} X_{ij} &amp;\\overset{ind}{\\sim}\\text{Normal}\\left(\\mu_{j},\\sigma^{2} \\right) \\\\ i &amp;= 1\\ldots n_{j} \\\\ j &amp;= 1\\ldots 25 \\end{aligned}\\end{equation}\\] Note that ‚Äúiid‚Äù has been replaced by ‚Äúind‚Äù, to indicate that the data are independent but not identically distributed, because they have different means \\(\\mu_{j}\\). Note that I have taken the variance \\(\\sigma^{2}\\) to be the same for each ward, which may or may not be a reasonable thing to do. Prove that the MLE‚Äôs for the means and standard deviations are: \\[\\begin{equation}\\begin{aligned} \\hat{\\mu}_{j} &amp;= \\frac{1}{n_{j}}\\sum_{i=1}^{n_{j}}X_{ij} \\\\ \\hat{\\sigma} &amp;= \\sqrt{\\frac{\\sum_{j=1}^{25}\\sum_{i=1}^{n_{j}}\\left(X_{ij} - \\hat{\\mu}_{j} \\right)^{2}}{\\sum_{j=1}^{25}n_{j}}} \\end{aligned}\\end{equation}\\] "],
["section-supplement-to-chapter-23-and-24.html", "Chapter 9 Supplement to Chapter 23 and 24 9.1 Confidence Intervals for the Mean (Chapter 23) 9.2 More on confidence intervals (Chapter 24)", " Chapter 9 Supplement to Chapter 23 and 24 This chapter implements much of the analysis shown in chapters 23 and 24 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. The assigned exercises associated with this material are from MIPS, as follows: 23.1, 23.2, 23.3; 23.5; 23.6; 23.7; 23.10; 23.11 24.1, 24.2; 24.3; 24.4; 24.6; 24.9; 24.10. Answers to selected exercises are in the back of the book. You should also do all the ‚Äúquick exercises‚Äù within chapters 23 and 24 (solutions are at the end of the chapter). Use R as much as possible when answering the questions. library(tidyverse) 9.1 Confidence Intervals for the Mean (Chapter 23) 9.1.1 Simulation Let‚Äôs first implement the simulation from page 344. We‚Äôll generate a bunch of samples from a \\(\\text{N}(0,1)\\) distribution, compute their confidence intervals, and plot them set.seed(432432432) # How many samples to generate B &lt;- 50 # Sample size of each n &lt;- 20 # Confidence level conf &lt;- .9 # Critical value- the book just gives this as 1.729 # This actually took me a minute to figure out... so make sure # you get what I&#39;m doing here: critval &lt;- qt(1 - (1-conf)/2,df = n-1) # Perform the simulation confints &lt;- 1:B %&gt;% # Generate the samples map(~rnorm(n,0,1)) %&gt;% # Compute the confidence intervals map(~c(&quot;mean&quot; = mean(.x), &quot;lower&quot; = mean(.x) - critval * sd(.x)/sqrt(n), &quot;upper&quot; = mean(.x) + critval * sd(.x)/sqrt(n)) ) %&gt;% # Put them in a dataframe reduce(bind_rows) %&gt;% # Add a row index, for purposes of plotting mutate(id = 1:B) # Compute the proportion that don&#39;t contain zero scales::percent(mean(confints$upper &lt; 0 | confints$lower &gt; 0)) ## [1] &quot;12%&quot; # Plot them. Run this code layer by layer # to understand what each part does. confints %&gt;% ggplot(aes(x = id)) + theme_classic() + geom_point(aes(y = mean),pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;,size = 1) + geom_errorbar(aes(ymin = lower,ymax = upper),size = .1) + geom_hline(yintercept = 0,colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) + scale_y_continuous(breaks = seq(-1,1,by=.2)) + coord_flip() + theme(axis.title.y = element_blank(),axis.text.y = element_blank(),axis.ticks.y = element_blank()) + labs(y = &quot;&quot;) Exercise: re-run this experiment several times with different random seeds. What kind of empirical coverage probabilities‚Äîthe proportion of intervals that don‚Äôt contain zero‚Äîdo you get? What about if you raise the sample size to n = 100? What about if you raise the number of simulations to B = 1000? 9.1.2 Gross calorific value measurements for Osterfeld 262DE27 The Osterfield data is made available with the book. Its file name is misspelled, so be careful: head data/MIPSdata/grosscalOsterfeld.txt wc -l data/MIPSdata/grosscalOsterfeld.txt ## 23.870 ## 23.730 ## 23.712 ## 23.760 ## 23.640 ## 23.850 ## 23.840 ## 23.860 ## 23.940 ## 23.830 ## 23 data/MIPSdata/grosscalOsterfeld.txt Read it in. I‚Äôm leaving this as an exericse (note: not because I‚Äôm lazy, I still had to write the code. It‚Äôs for your learning). You should get the following: glimpse(osterfield) ## Observations: 23 ## Variables: 1 ## $ calorific_value &lt;dbl&gt; 23.870, 23.730, 23.712, 23.760, 23.640, 23.850, ‚Ä¶ Recreate the confidence interval in the book: # Compute the sample mean and size xbar &lt;- mean(osterfield$calorific_value) n &lt;- nrow(osterfield) # The population standard deviation, and the critical value/confidence level # are given as: sigma &lt;- .1 conf &lt;- .95 # Make sure to UNDERSTAND this calculation: critval &lt;- qnorm(1 - (1-conf)/2) # 1.96 # Compute the interval c( &quot;lower&quot; = xbar - critval * sigma/sqrt(n), &quot;upper&quot; = xbar + critval * sigma/sqrt(n) ) ## lower upper ## 23.74691 23.82865 9.1.3 Gross calorific value measurements for Daw Mill 258GB41 As an exercise, now recreate the confidence interval in the book for the Daw Mill sample. Read the data in from file grosscalDawMill.txt, call it dawmill. You can compute the sample standard deviation and appropriate critical value as follows: s &lt;- sd(dawmill$calorific_value) critval &lt;- qt(1 - (1-conf)/2,df = nrow(dawmill) - 1) You should get: ## lower upper ## 30.95422 31.06896 9.1.4 Bootstrap Confidence Intervals First, let‚Äôs simulate a dataset to illustrate this idea and so we can compare the bootstrap and analytical answers. set.seed(43547803) B &lt;- 2000 n &lt;- 5000 # Simulate one dataset ds &lt;- rnorm(n,0,1) # Values conf &lt;- .95 critval &lt;- qnorm(1 - (1 - conf)/2) # Now resample from it and calculate studentized statistics resampledstats &lt;- 1:B %&gt;% map(~sample(ds,n,replace = TRUE)) %&gt;% map(~c(mean(.x) - mean(ds))/(sd(.x)/sqrt(n))) %&gt;% reduce(c) # The confidence limits are obtained from the sample quantiles: conflim &lt;- quantile(resampledstats,probs = c((1-conf)/2,1 - (1 - conf)/2)) # Here&#39;s a plot that illustrates what these look like: tibble(x = resampledstats) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),colour = &quot;black&quot;,fill = &quot;lightgrey&quot;,bins = 100) + geom_vline(xintercept = conflim[1],colour = &quot;orange&quot;,linetype = &quot;dotdash&quot;) + geom_vline(xintercept = conflim[2],colour = &quot;orange&quot;,linetype = &quot;dotdash&quot;) + stat_function(fun = dnorm,args = list(mean = mean(ds),sd = sd(ds)),colour = &quot;blue&quot;) + labs(title = &quot;Resampled student statistics and empirical confidence limits&quot;, subtitle = &quot;A normal distribution (blue curve) fits well&quot;, x = &quot;&quot;,y = &quot;&quot;) I deliberately chose a large sample size and number of bootstrap samples to make the results look good. I encourage you to change these numbers to try and break this simulation. The bootstrap-resampled confidence limits are close to the truth: conflim ## 2.5% 97.5% ## -1.999947 2.005969 qnorm(c((1-conf)/2,1 - (1-conf)/2)) ## [1] -1.959964 1.959964 Let‚Äôs apply this to the software data. Oddly, I get different values for the mean, standard deviation, and sample size than the book reports. If you are the first person to figure out why, I will give you a \\(\\$10\\) Tim card. The differences aren‚Äôt meaningful enough to affect the presentation of these ideas. # Read it in: software &lt;- readr::read_csv( file = &quot;data/MIPSdata/software.txt&quot;, col_names = &quot;time&quot;, col_types = &quot;n&quot; ) B &lt;- 1000 # Same as book n &lt;- nrow(software) mn &lt;- mean(software$time) ss &lt;- sd(software$time) conf &lt;- .9 set.seed(821940379) resampledstats &lt;- 1:B %&gt;% map(~sample(software$time,n,replace = TRUE)) %&gt;% map(~c(mean(.x) - mn)/(sd(.x)/sqrt(n))) %&gt;% reduce(c) # The confidence limits are obtained from the sample quantiles: conflim &lt;- quantile(resampledstats,probs = c((1-conf)/2,1 - (1 - conf)/2)) # The confidence interval: c( &quot;lower&quot; = mn + conflim[1] * ss/sqrt(n), &quot;upper&quot; = mn + conflim[2] * ss/sqrt(n) ) ## lower.5% upper.95% ## 474.9540 781.4853 Exercise: compute a \\(90\\%\\) confidence interval for the mean for the software data assuming the data is normally distributed. This does NOT mean that you should use a normal distribution for calculating the critical values‚Äì if you don‚Äôt understand why, go back and read the ‚ÄúVariance Unknown‚Äù section on page 348. I got the following: ## lower lower ## 502.0045 796.2749 Does this lead to different conclusions in practice than the bootstrap interval? 9.2 More on confidence intervals (Chapter 24) 9.2.1 Binomial distribution The binomial example is a very good one for understanding how confidence intervals work. Consider the form of the confidence interval for \\(p\\) when \\(X\\sim\\text{Bin}(n,p)\\): \\[\\begin{equation} \\left(\\frac{X}{n} - p \\right)^{2} - \\left( z_{\\alpha/2}\\right)^{2}\\frac{p(1-p)}{n} &lt; 0 \\end{equation}\\] The book says the solution is ‚Äúawkward‚Äù. As a self-annointed expert on all things awkward, I am not sure I agree. I do, however, also not want to do the math. It is less tedious and more illustrative to plot this interval and see how it changes with \\(X\\): set.seed(876098978) the_quadratic &lt;- function(p,X,n,alpha = 0.05) { # p: a vector of values at which to plot the quadratic (X/n - p)^2 - qnorm(1 - alpha/2) * (p*(1-p)/n) } # Generate some data from a distribution with a known p p0 &lt;- .3 n &lt;- 10 X &lt;- rbinom(1,n,p0) tibble(p = c(0,1)) %&gt;% ggplot(aes(x = p)) + theme_classic() + stat_function(fun = the_quadratic,args = list(X = X,n = n)) + geom_hline(yintercept = 0,colour = &quot;lightgrey&quot;,linetype = &quot;dotdash&quot;) Exercise: put vertical lines on this plot at the locations where the parabola touch zero. This means use the quadratic formula to find the roots and plot them using geom_vline(xintercept = ???). Consider Remark 24.1, which discusses why we can‚Äôt get confidence intervals with exact coverage probabilities for discrete distributions. For \\(n = 10\\), \\(X = 6\\), \\(\\alpha = .05\\), let‚Äôs investigate the coverage probability of our confidence interval using a parametric bootstrap. We are going to simulate values from a \\(\\text{Bin}(10,.3)\\) distribution, compute the confidence interval for each, and then assess whether it contains the true value \\(p_{0} = .3\\). The proportion of intervals in our \\(B\\) bootstrap samples that contain the true value is the bootstrapped estimate of the interval‚Äôs coverage probability. set.seed(8798) B &lt;- 1000 n &lt;- 10 p0 &lt;- .3 # Simulate B values from a binomial distribution: samps &lt;- rbinom(B,n,p0) # Write a function for computing the interval for each X # I am going to use a numerical root-finding program # You will replace this part with the formula you derived # in the previous exercise compute_interval &lt;- function(X) { optfun &lt;- function(p) the_quadratic(p,X,10) rootSolve::uniroot.all(optfun,c(0,1)) } # Do the bootstrap samps %&gt;% map(compute_interval) %&gt;% # Compute the interval for each sample map(~as.numeric(.x[1] &lt; p0 &amp; .x[2] &gt; p0)) %&gt;% # Create a 0/1 indicator of whether the interval contains p0 reduce(c) %&gt;% # Combine the results in a vector mean() %&gt;% # The proportion of 1&#39;s (the mean) is the proportion of intervals which contained the true value, i.e. the coverage probability scales::percent() # Format as a percentage ## [1] &quot;92%&quot; Exercise: I put the above procedure into a function: bootstrap_coverage &lt;- function(B = 1000,n = 10,p0 = .3) { samps &lt;- rbinom(B,n,p0) compute_interval &lt;- function(X) { optfun &lt;- function(p) the_quadratic(p,X,10) rootSolve::uniroot.all(optfun,c(0,1)) } samps %&gt;% map(compute_interval) %&gt;% map(~as.numeric(.x[1] &lt; p0 &amp; .x[2] &gt; p0)) %&gt;% reduce(c) %&gt;% mean() } Run this function many times and make a histogram of the results. You can use myruns &lt;- 1:1000 %&gt;% map(bootstrap_coverage) %&gt;% reduce(c) to do this, and then refer to previous code for how to make the histogram. What value is in the centre? Is the stated coverage probability accurate? "],
["section-extended-example-reasoning-about-goodness-of-fit.html", "Chapter 10 Extended Example: Reasoning About Goodness of Fit 10.1 Go and read the blog post 10.2 Distribution of last digits", " Chapter 10 Extended Example: Reasoning About Goodness of Fit This chapter is a bit different: we introduce the idea of ‚Äúgoodness of fit‚Äù through implementing the analysis in a blog post discussing a disputed paper in a Psychology journal. You can find the article here: http://datacolada.org/74 library(tidyverse) 10.1 Go and read the blog post To start, go read this blog post: http://datacolada.org/74. This should take you at least an hour or so to do in detail, if not longer. It is a major part of this week‚Äôs materials, effectively replacing textbook readings. It is not optional, and will be tested on assignments and tests. Exercise: summarize this study in your own words. Two or three sentences should suffice. What did they do, how did they do it, and what were they trying to find out? It is important that you understand this before you move on. 10.2 Distribution of last digits The distribution of digits in numeric data is of considerable interest in certain fields. In forensic accounting, where investigators try to identify fraudulent accounting practice by identifying systematic anomalies in financial records, the relative frequency with which different digits occur can indicate potential fraud if it differs from what you would expect. It is well-established that the last digit, in particular, of numeric data should be distributed pretty evenly between the numbers \\(0 - 9\\) in any given set of data. So, let‚Äôs look at the last digit of the measurements from the hand sanitizer study. The data is available from http://datacolada.org/appendix/74/ and their R code can be found at http://datacolada.org/appendix/74/74%20-%20DataColada%20-%20Decoy%20Sanitizer%20-%202018%2008%2023.R. Here we use our own R code but I borrow parts from theirs. 10.2.1 Read in the data First, read in the data as usual: study1 &lt;- readr::read_csv( file = &quot;http://datacolada.org/appendix/74/Study%201%20-%20Decoy%20Effect.csv&quot;, col_names = TRUE, col_types = stringr::str_c(rep(&quot;n&quot;,42),collapse = &quot;&quot;) ) glimpse(study1) ## Observations: 40 ## Variables: 42 ## $ Subject &lt;dbl&gt; 1, 2, ‚Ä¶ ## $ `Group (1=experimental condition, 0 = control condition)` &lt;dbl&gt; 1, 1, ‚Ä¶ ## $ Day1 &lt;dbl&gt; 55, 60‚Ä¶ ## $ Day2 &lt;dbl&gt; 45, 55‚Ä¶ ## $ Day3 &lt;dbl&gt; 45, 55‚Ä¶ ## $ Day4 &lt;dbl&gt; 40, 55‚Ä¶ ## $ Day5 &lt;dbl&gt; 45, 50‚Ä¶ ## $ Day6 &lt;dbl&gt; 45, 45‚Ä¶ ## $ Day7 &lt;dbl&gt; 55, 45‚Ä¶ ## $ Day8 &lt;dbl&gt; 60, 40‚Ä¶ ## $ Day9 &lt;dbl&gt; 45, 40‚Ä¶ ## $ Day10 &lt;dbl&gt; 67, 40‚Ä¶ ## $ Day11 &lt;dbl&gt; 65, 56‚Ä¶ ## $ Day12 &lt;dbl&gt; 70, 40‚Ä¶ ## $ Day13 &lt;dbl&gt; 80, 40‚Ä¶ ## $ Day14 &lt;dbl&gt; 75, 45‚Ä¶ ## $ Day15 &lt;dbl&gt; 70, 50‚Ä¶ ## $ Day16 &lt;dbl&gt; 80, 55‚Ä¶ ## $ Day17 &lt;dbl&gt; 70, 55‚Ä¶ ## $ Day18 &lt;dbl&gt; 75, 50‚Ä¶ ## $ Day19 &lt;dbl&gt; 70, 45‚Ä¶ ## $ Day20 &lt;dbl&gt; 65, 50‚Ä¶ ## $ `Day21 (Beginning of intervention)` &lt;dbl&gt; 85, 55‚Ä¶ ## $ Day22 &lt;dbl&gt; 85, 75‚Ä¶ ## $ Day23 &lt;dbl&gt; 90, 45‚Ä¶ ## $ Day24 &lt;dbl&gt; 85, 60‚Ä¶ ## $ Day25 &lt;dbl&gt; 75, 50‚Ä¶ ## $ Day26 &lt;dbl&gt; 75, 50‚Ä¶ ## $ Day27 &lt;dbl&gt; 65, 35‚Ä¶ ## $ Day28 &lt;dbl&gt; 75, 55‚Ä¶ ## $ Day29 &lt;dbl&gt; 75, 60‚Ä¶ ## $ Day30 &lt;dbl&gt; 80, 55‚Ä¶ ## $ Day31 &lt;dbl&gt; 75, 65‚Ä¶ ## $ Day32 &lt;dbl&gt; 50, 45‚Ä¶ ## $ Day33 &lt;dbl&gt; 70, 40‚Ä¶ ## $ Day34 &lt;dbl&gt; 75, 55‚Ä¶ ## $ Day35 &lt;dbl&gt; 65, 55‚Ä¶ ## $ Day36 &lt;dbl&gt; 75, 60‚Ä¶ ## $ Day37 &lt;dbl&gt; 70, 50‚Ä¶ ## $ Day38 &lt;dbl&gt; 75, 45‚Ä¶ ## $ Day39 &lt;dbl&gt; 90, 65‚Ä¶ ## $ Day40 &lt;dbl&gt; 85, 70‚Ä¶ Exercise: download the data by pasting the link into a web browser. Print out the header on the command line or open the .csv file in Excel or otherwise. Verify that the col_names and col_types arguments I provided are correct. The data is in ‚Äúwide‚Äù format‚Äì each day has its own column. We want one column for subject ID, one for Day, and one for measurement. Let‚Äôs reformat the data: study1_long &lt;- study1 %&gt;% tidyr::gather(day,measurement,Day1:Day40) glimpse(study1_long) ## Observations: 1,600 ## Variables: 4 ## $ Subject &lt;dbl&gt; 1, 2, ‚Ä¶ ## $ `Group (1=experimental condition, 0 = control condition)` &lt;dbl&gt; 1, 1, ‚Ä¶ ## $ day &lt;chr&gt; &quot;Day1&quot;‚Ä¶ ## $ measurement &lt;dbl&gt; 55, 60‚Ä¶ We previously had \\(40\\) subjects each with \\(40\\) columns of measurements; now we have \\(1,600\\) rows, which looks good to me. Let‚Äôs further clean up the data: we need to Rename the colums so they are pleasant (yes, this is important!), Extract the last digit of each measurement and save it in a new column. Let‚Äôs do this. We‚Äôll use the substr function to choose the last digit of each number. Type ?substr to learn about this function. study1_clean &lt;- study1_long %&gt;% rename(subject = Subject, group = `Group (1=experimental condition, 0 = control condition)`) %&gt;% mutate(last_digit = as.numeric(substr(measurement,nchar(measurement),nchar(measurement)))) glimpse(study1_clean) ## Observations: 1,600 ## Variables: 5 ## $ subject &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1‚Ä¶ ## $ group &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶ ## $ day &lt;chr&gt; &quot;Day1&quot;, &quot;Day1&quot;, &quot;Day1&quot;, &quot;Day1&quot;, &quot;Day1&quot;, &quot;Day1&quot;, &quot;Day‚Ä¶ ## $ measurement &lt;dbl&gt; 55, 60, 63, 55, 60, 60, 35, 55, 45, 75, 50, 70, 60, ‚Ä¶ ## $ last_digit &lt;dbl&gt; 5, 0, 3, 5, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0‚Ä¶ Looks clean to me! 10.2.2 Make the histogram Okay, now consider Figure 1 in the blog post. They filter out observations ending in \\(0\\) or \\(5\\), because the authors of the disputed paper claim to have occasionally used a scale with \\(5\\)-gram precision, and occasionally one with \\(1\\)-gram precision (this alone is suspect‚Ä¶). They then make a histogram of all the last digits. We can do this too: study1_clean %&gt;% filter(!(last_digit %in% c(0,5))) %&gt;% ggplot(aes(x = last_digit)) + theme_classic() + geom_bar(stat = &quot;count&quot;,colour = &quot;black&quot;,fill = &quot;lightblue&quot;) + scale_x_continuous(breaks = c(1,2,3,4,6,7,8,9),labels = c(1,2,3,4,6,7,8,9)) + labs(title = &quot;Study 1: Last digit strikingly not uniform&quot;, x = &quot;Last Digit (0&#39;s and 5&#39;s removed)&quot;, y = &quot;Frequency&quot;) I called this plot a ‚Äúhistogram‚Äù (which it is), but I used geom_bar(stat = &quot;count&quot;) to create it. I did this because I wanted the bins to be at specific values. It‚Äôs still a histogram (why?). 10.2.3 Testing goodness of fit: simulation If you expect the last digits of a set of numbers to be evenly-distributed across the values \\(0 - 9\\), then this plot might look surprising. But can we conclude that something is wrong, just by looking at a plot? What if the real underlying distribution of digits is actually evenly-distributed, and we just got a weird sample‚Äì we‚Äôd be making an incorrect harsh judgement. We‚Äôre going to ask the question: if the underlying distribution of last digits really was evenly distributed, what is the probability of seeing the data that we saw, or a dataset that is even more extreme under this hypothesis?. That‚Äôs a mouthful! But it‚Äôs a good question to ask. If it‚Äôs really, really unlikely that we see what we saw (or something even further) if the claim of even distribution is true, then this provides evidence against the notion that the digits are actually evenly distributed. And this provides evidence that there is something funny in the data. We are going to do a simulation to investigate this claim. We are going to Generate a bunch of datasets the same size as ours but where the distribution of the last digit actually is evenly distributed across \\(0 - 9\\), and Record the proportion of digits in each that are \\(3\\) or \\(7\\), and Compute the proportion of our simulated datasets that have a proportion of 3‚Äôs or 7‚Äôs as high, or higher, as what we saw in our sample. There are a few details that we can‚Äôt get exactly right here: the real data was generated by sampling a bunch of values that ended in \\(0&#39;s\\) or \\(5&#39;s\\) and then filtering these out, which is behaviour that I don‚Äôt know how to replicate exactly. We also could consider the distribution of \\(3&#39;s\\) and \\(7&#39;s\\) separately, or jointly (using ‚Äúand‚Äù instead of ‚Äúor‚Äù). I order to keep this idea simple and big-picture, we‚Äôre going to ignore these details here. The first question is, how do we generate data that has last digits evenly distributed? Well, any random numbers should work, but to keep things consistent with the real data, let‚Äôs try generating from a normal distribution with mean and variance equal to the sample mean and variance of our data, rounded to the nearest integer: mn &lt;- mean(study1_clean$measurement) ss &lt;- sd(study1_clean$measurement) testnumbers &lt;- round(rnorm(10000,mn,ss)) # Plot a chart of the last digits tibble(x = testnumbers) %&gt;% mutate(last_digit = as.numeric(substr(x,nchar(x),nchar(x)))) %&gt;% ggplot(aes(x = last_digit)) + theme_classic() + geom_bar(stat = &quot;count&quot;,colour = &quot;black&quot;,fill = &quot;lightblue&quot;) + scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9),labels = c(0,1,2,3,4,5,6,7,8,9)) + labs(title = &quot;Last digits of normal random sample&quot;, x = &quot;Last Digit&quot;, y = &quot;Frequency&quot;) Looks pretty uniform to me! Let‚Äôs proceed with our simulation: set.seed(789685) # Create a function that simulates a dataset # and returns the proportion of last digits # that are either 3 or 7 N &lt;- nrow(study1_clean %&gt;% filter(!(last_digit %in% c(5,0)))) # Size of dataset to simulate B &lt;- 1e04 # Number of simulations to do- 1 million (!) mn &lt;- mean(study1_clean$measurement) ss &lt;- sd(study1_clean$measurement) simulate_proportion &lt;- function() { ds &lt;- round(rnorm(N,mn,ss)) last_digits &lt;- substr(ds,nchar(ds),nchar(ds)) mean(last_digits %in% c(&quot;3&quot;,&quot;7&quot;)) } # What is the proportion of 3&#39;s and 7&#39;s in our data, # after filtering out 5&#39;s and 0&#39;s? study_proportion &lt;- study1_clean %&gt;% filter(!(last_digit %in% c(5,0))) %&gt;% summarize(p = mean(last_digit %in% c(3,7))) %&gt;% pull(p) study_proportion ## [1] 0.8814815 # 88.1%. Wow. # Perform the simulation: sim_results &lt;- 1:B %&gt;% map(~simulate_proportion()) %&gt;% map(~as.numeric(.x &gt;= study_proportion)) %&gt;% reduce(c) # This is a vector of 0/1 which says whether each simulation&#39;s proportion of # 3&#39;s and 7&#39;s exceeded the study proportion. Its mean is the simulated probability # of seeing what we saw in the study, if the digits are actually evenly distributed: mean(sim_results) ## [1] 0 # Okay... how many? sum(sim_results) ## [1] 0 In \\(B = 10,000\\) simulations, I didn‚Äôt even get a single dataset that was as extreme as ours. This provides strong evidence against the notion that the digits are, in fact, evenly distributed. Exercise: how many simulations do you need before you get even one that is as extreme as our dataset? 10.2.4 Testing goodness of fit: math We may also use mathematics and statistical modelling to answer the question: if the underlying distribution of last digits really was evenly distributed, what is the probability of seeing the data that we saw, or a dataset that is even more extreme under this hypothesis?. We do this in a clever way: we construct a statistical model that represents the truth, if the truth is what we say it is. Namely, we will define a probability distribution that should represent the distribution of the last digits of our measurements, if the last digits are evenly distributed. We then see how probable our data is under this model. If, under this model, it is very unlikely to see a dataset like ours, then this provides evidence that the model isn‚Äôt representative of the truth. And since the model was built under the premise that the digits are evenly distributed, a lack of fit of the model to the observed data provides evidence against the notion that the digits are evenly distributed. Let‚Äôs develop a model. Define \\(y_{ij} = 1\\) if the last digit of the \\(i^{th}\\) measurement equals \\(j\\), for \\(j \\in J = \\left\\{1,2,3,4,6,7,8,9\\right\\}\\), and equals \\(0\\) otherwise. So for example, if the \\(i^{th}\\) measurement is \\(42\\) then \\(y_{i1} = 0\\) and \\(y_{i2} = 1\\) and \\(y_{i3} = 0\\) and so on. Define \\(y_{i} = (y_{i1},\\ldots,y_{i9})\\), a vector containing all zeroes except for exactly one \\(1\\). Then the \\(y_{i}\\) are independent draws from a Multinomial distribution, \\(y_{i}\\overset{iid}{\\sim}\\text{Multinomial}(1,p_{1},\\ldots,p_{9})\\), with \\(\\mathbb{E}(y_{ij}) = \\mathbb{P}(y_{ij} = 1) = p_{j}\\) and \\(\\sum_{j\\in J}p_{j} = 1\\). The vectors \\(y_{i}\\) have the following (joint) density function: \\[\\begin{equation} \\mathbb{P}(y_{i} = (y_{i1},\\ldots,y_{i9})) = p_{1}^{y_{i1}}\\times\\cdots\\times p_{9}^{y_{i9}} \\end{equation}\\] The multinomial is the generalization of the binomial/bernoulli to multiple possible outcomes on each trial. If the bernoulli is thought of as flipping a coin (two possible outcomes), then the multinomial should be thought of as rolling a die (six possible outcomes). How does this help us answer the question? If the digits are actually evenly distributed, then this means \\(p_{1} = \\cdots = p_{9} = 1/8\\) (why?). However, the data might tell us something different. We estimate the \\(p_{j}\\) from out data \\(y_{1},\\ldots,y_{n}\\) by computing the maximum likelihood estimator: \\[\\begin{equation} \\hat{p}_{j} \\ \\frac{1}{n}\\sum_{i=1}^{n}y_{ij} \\end{equation}\\] which are simply the sample proportions of digits that equal each value of \\(j\\). We assess how close the MLEs are to what the true values ought to be using the likelihood ratio. For \\(p_{0} = (1/8,\\ldots,1/8)\\), \\(\\hat{p} = (\\hat{p}_{1},\\ldots,\\hat{p}_{9})\\), and \\(L(\\cdot)\\) the likelihood based off of the multinomial density, \\[\\begin{equation} \\Lambda = \\frac{L(p_{0})}{L(\\hat{p})} \\end{equation}\\] Remember the definition of the likelihood, for discrete data: for any \\(p\\), \\(L(p)\\) is the relative frequency with which the observed data would be seen in repeated sampling, if the true parameter value were \\(p\\). The likelihood ratio \\(\\Lambda\\) is the ratio of how often our data would be seen if \\(p = p_{0}\\), against how often it would be seen if \\(p = \\hat{p}\\). If \\(\\Lambda = 0.5\\), for example, that means that our data would occur half as often if \\(p = p_{0}\\), compared to if \\(p = \\hat{p}\\). Note that \\(0 &lt; \\Lambda \\leq 1\\) (why?). Lower values of \\(\\Lambda\\) mean that there is stronger evidence against the notion that \\(p_{0}\\) is a plausible value for \\(p\\)‚Äì that is, that the digits are evenly distributed. But how do we quantify how much less likely is less likely enough? Here is where our previous question comes back. We ask: if the digits were evenly distributed, what is the probability of seeing the data we saw, or something with an even more extreme distribution of digits? To compute this, we need to be able to compute probabilities involving \\(\\Lambda\\). It turns out that using \\(\\Lambda\\) is very clever, because well, we can do this. There is a Big Theorem which states that if \\(p\\in\\mathbb{R}^{d}\\) and \\(p = p_{0}\\), \\[\\begin{equation} -2\\log\\Lambda \\overset{\\cdot}{\\sim} \\chi_{d-1}^{2} \\end{equation}\\] So, if the digits are evenly distributed, then our value of \\(-2\\log\\Lambda\\) should be a realization of a \\(\\chi^{2}_{7}\\) random variable. We therefore compute \\[\\begin{equation} \\nu = \\mathbb{P}\\left(\\chi^{2}_{7} \\geq -2\\log\\Lambda\\right) \\end{equation}\\] The quantity \\(\\nu\\) is the probability of observing a distribution of digits as or more extreme than the one we observed in our data, if the distribution of digits truly is even. It‚Äôs called a p-value, and it‚Äôs one helpful summary statistic in problems where the research question concerns comparing observations to some sort of reference, like we‚Äôre doing here. Let‚Äôs compute the likelihood ratio for our data: # Compute the MLEs study1_filtered &lt;- study1_clean %&gt;% filter(!(last_digit %in% c(5,0))) # Should have done this before... pmledat &lt;- study1_filtered %&gt;% group_by(last_digit) %&gt;% summarize(cnt = n(), pp = n() / nrow(study1_filtered)) obsvec &lt;- pmledat$cnt names(obsvec) &lt;- pmledat$last_digit pmle &lt;- pmledat$pp names(pmle) &lt;- pmledat$last_digit pmle ## 2 3 4 6 7 8 ## 0.007407407 0.525925926 0.014814815 0.022222222 0.355555556 0.051851852 ## 9 ## 0.022222222 # Truth, if digits evenly distributed p0 &lt;- rep(1,length(pmle)) / length(pmle) names(p0) &lt;- names(pmle) p0 ## 2 3 4 6 7 8 9 ## 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 # Compute minus twice the likelihood ratio multinom_log_likelihood &lt;- function(x,p) { # x: named vector where the name is j and the value is the count of # times y_ij = 1 in the sample # p: named vector with names equal to the unique values of x, # containing probabilities of each sum(x * log(p[names(x)])) } lr &lt;- -2 * (multinom_log_likelihood(obsvec,p0) - multinom_log_likelihood(obsvec,pmle)) lr ## [1] 221.1061 # Compute the probability that a chisquare random variable is greater than this # value: 1 - pchisq(lr,df = length(pmle) - 1) # zero. ## [1] 0 Exercise: write your own function to implement the multinomial loglikelihood, using the dmultinom function and a for loop. Compare your results to mine. We would interpret this result as: the observed data provides strong evidence against the notion that the digits are evenly distributed. The fact that there is essentially zero chance of seeing what we saw if our claim that the digits are evenly distributed agrees with our simulation. We just did some statistical forensics! Pretty cool. This type of reasoning is abundant in the scientific literature, as are these p-value things. They are useful, but we want you, as stats majors and specialists, to leave this course understanding than statistics is so very much more than just a ritual or toolbox! library(tidyverse) "],
["section-supplement-to-evans-rosenthal-section-7-2.html", "Chapter 11 Supplement to Evans &amp; Rosenthal Section 7.2 11.1 Estimation in Bayesian Inference: general ideas 11.2 Estimation in Bayesian Inference: point and interval estimation 11.3 Choosing a Prior", " Chapter 11 Supplement to Evans &amp; Rosenthal Section 7.2 This chapter is a supplement to Chapter 7: Bayesian Inference, section 2: Estimation. This is to support the Bayesian Inference sections of STA238. Materials in this tutorial are taken from Alex‚Äôs comprehensive tutorial on Bayesian Inference, which is very long and outside the scope of this course. The assigned exercises associated with this material are from Evans and Rosenthal (E&amp;R), as follows: E&amp;R 7.2.1, 7.2.2, 7.2.3, 7.2.4, 7.2.5, 7.2.6; 7.2.12a; 7.2.14 11.1 Estimation in Bayesian Inference: general ideas The case where there are only two possible parameter values is useful for examples, but not common in practice. More realistic is the case where \\(0 &lt; \\theta &lt; 1\\), and we need to use the observed data to estimate \\(\\theta\\). Intuition and frequentist results dictate that a ‚Äúgood‚Äù estimator of \\(\\theta\\) is the sample proportion number of heads, \\[ \\hat{\\theta}_{freq} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i} \\] This makes sense; of course the best guess of the value of the probability of heads based on one sample is just the relative frequency with which \\(X\\) is heads in that sample. It‚Äôs also the maximum likelihood estimator, and the unbiased estimator with minimum variance. Let‚Äôs see how this estimator, which is optimal from a frequentist perspective, behaves compared to what we come up with using Bayesian estimation. 11.1.1 The Prior The new parameter space is \\(\\Theta = (0,1)\\). Bayesian inference proceeds as above, with the modification that our prior must be continuous and defined on the unit interval \\((0,1)\\). This reflects the fact that our parameter can take any value on the interval \\((0,1)\\). Choosing the prior is a subjective decision, and is slightly more difficult in the continuous case because interpreting densities is harder than interpreting discrete probability mass functions. In a nutshell, we should choose our prior distribution such that values of \\(\\theta\\) that we think are reasonable have high probability under the prior, and values of \\(\\theta\\) that we think are not reasonable have low probability under the prior. There is a lot more that goes into the choice of prior in more complicated applied problems, but this is always the basic idea. A popular choice for a prior for a binomial likelihood like we have here is the beta distribution, \\[ \\begin{aligned} \\theta &amp;\\sim Beta(a,b) \\\\ f(\\theta;a,b) &amp;= \\theta^{a-1}(1-\\theta)^{b-1}, 0 &lt; \\theta &lt; 1, a &gt; 0, b &gt; 0 \\\\ E(\\theta) &amp;= \\frac{a}{a+b} \\\\ Var(\\theta) &amp;= \\frac{ab}{(a+b)^2 (a+b+1)} \\end{aligned} \\] The Beta distribution is defined on \\(0,1\\) and itself has two parameters \\(a,b\\) which control the shape of the distribution, and its moments. If a parameter having a mean and variance makes you uncomfortable at first, try interpreting these quantities less literally; the mean is just the ‚Äúcentre‚Äù of the possible values of \\(\\theta\\) as weighted by their probabilities, and the variance (or more accurately, the standard deviation) roughly describes the size of the region around the mean in which \\(\\theta\\) is likely to fall. This just gives a measure of how ‚Äúsure‚Äù we are, before seeing the results of any coin flips, that \\(\\theta\\) is near the centre of its possible values. Let‚Äôs visualize the prior distribution in order to help us specify the parameters \\(a,b\\) that will give us a reasonable prior: # Generate plot data- dbeta evaluated at x and a,b for x on a grid between 0 and 1 and various values of a,b expand.grid(a = c(.5,1,2),b = c(.5,1,2)) %&gt;% pmap_dfr(~data_frame(x = seq(0.01,0.99,by=0.01),y = dbeta(x = x,shape1=.x,shape2=.y),a = .x,b = .y)) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + facet_wrap(a~b) + geom_line(colour = &quot;purple&quot;) + labs(title = &quot;Beta Distribution, various a and b&quot;, subtitle = &quot;Top value is a, bottom is b&quot;, x = &quot;Datapoint x&quot;, y = &quot;Density f(x;a,b)&quot;) + theme(text = element_text(size = 22)) The Beta distribution is very flexible; different values of a and b give very different shapes. If we thought extreme values (close to 0 or 1) of \\(\\theta\\) were likely, we could choose a prior with a and b both less than 1; if we think middle values are more likely, we can choose a and b to be greater than 1. For our example, we will choose a Beta(12,12) distribution, for reasons we will discuss below in the section on choosing prior distributions. This looks like this: data_frame(x = c(0.01,0.99)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = dbeta, args = list(shape1 = 12,shape2 = 12), colour = &quot;blue&quot;) + labs(title = &quot;Beta Prior for Theta&quot;, subtitle = &quot;Bayesian Coin Flipping Example&quot;, x = &quot;Theta&quot;, y = &quot;Prior Density, p(Theta)&quot;) + scale_x_continuous(breaks = seq(0,1,by=0.1)) This prior puts strong weight on the coin being close to fair; values below \\(\\theta = 0.3\\) and \\(\\theta = 0.7\\) have very little prior probability. This can be verified: # Prior probability of theta being between 0.3 and 0.7 pbeta(0.7,shape1=12,shape2=12) - pbeta(0.3,shape1=12,shape2=12) ## [1] 0.957104 Most of the mass of the distribution is between \\(0.3\\) and \\(0.7\\). 11.1.2 The Posterior Our prior has density \\[ p(\\theta;a,b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}(1-\\theta)^{b-1} \\] Our likelihood remains the same as before: \\[ p(X|\\theta) = \\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}} \\] Bayes‚Äô Rule is still used to compute the posterior from these quantities; however, it looks slightly different now: \\[ \\begin{aligned} p(\\theta|X) &amp;= \\frac{p(X|\\theta)p(\\theta)}{p(X)} \\\\ &amp;= \\frac{p(X|\\theta)p(\\theta)}{\\int_{0}^{1}p(X|\\theta)p(\\theta)d\\theta} \\end{aligned} \\] Now, because \\(\\theta\\) is defined on a continuous interval, the marginal likelihood/model evidence/normalizing constant is computed via integrating the joint distributionn of \\(X\\) and \\(\\theta\\) over the range of \\(\\theta\\). In this example, the marginal likelihood and the posterior can be computed explicitly as follows: \\[ \\begin{aligned} p(X) &amp;= \\int_{0}^{1}p(X|\\theta)p(\\theta)d\\theta \\\\ &amp;= \\int_{0}^{1} \\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}} \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}(1-\\theta)^{b-1} d\\theta \\\\ &amp;= \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\times \\int_{0}^{1} \\theta^{\\sum_{i=1}^{n}x_{i} + a - 1}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i} + b - 1} d\\theta \\\\ &amp;= \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\times \\frac{\\Gamma(\\sum_{i=1}^{n}x_{i} + a)\\Gamma(n - \\sum_{i=1}^{n}x_{i} + b)}{\\Gamma(n + a + b)} \\end{aligned} \\] How did we evaluate that integral and get to the last line? We recognized the integrand as being the \\(\\theta\\)-dependent part of a \\(Beta(\\sum_{i=1}^{n}x_{i} + a,n - \\sum_{i=1}^{n}x_{i} + b)\\) density; hence it integrates to the reciprocal of the appropriate normalizing constant. This trick is commonly used in examples illustrating Bayesian inference; it shouldn‚Äôt be taken from this, though, that this integral is always easy to evaluate like this. It is almost never easy, or even possible, to evaluate this integral in anything beyond these simple examples- more on this later. Exercise: use this trick to ‚Äúshow‚Äù that \\[\\begin{equation} \\int_{-\\infty}^{\\infty}e^{-x^{2}}dx = \\sqrt{\\pi} \\end{equation}\\] and \\[\\begin{equation} \\int_{0}^{\\infty}x^{\\alpha}e^{-\\beta x} = \\frac{\\Gamma(\\alpha + 1)}{\\beta^{(\\alpha + 1)}} \\end{equation}\\] where \\(\\Gamma(\\cdot)\\) is the Gamma function. Hint: the wikipedia articles on the Normal and Gamma distributions will be helpful. With \\(p(X)\\) available, we can explicitly compute the posterior: \\[ \\begin{aligned} p(\\theta|X) &amp;= \\frac{p(X|\\theta)p(\\theta)}{p(X)} \\\\ &amp;= \\frac{\\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}} \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}(1-\\theta)^{b-1}}{\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\times \\frac{\\Gamma(\\sum_{i=1}^{n}x_{i} + a)\\Gamma(n - \\sum_{i=1}^{n}x_{i} + b)}{\\Gamma(n + a + b)}} \\\\ &amp;= \\frac{\\Gamma(n + a + b)}{\\Gamma(\\sum_{i=1}^{n}x_{i} + a)\\Gamma(n - \\sum_{i=1}^{n}x_{i} + b)} \\times \\theta^{\\sum_{i=1}^{n}x_{i} + a - 1}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i} + b - 1} \\end{aligned} \\] which we recognize as a \\(Beta(a + \\sum_{i=1}^{n}x_{i},b + n - \\sum_{i=1}^{n}x_{i})\\) distribution. Priors where the posterior belongs to the same family of distributions as the prior are called conjugate priors, and while they represent the minority of practical applications, they are very useful for examples. In this scenario, we can interpret the likelihood as directly updating the prior parameters, from \\((a,b)\\) to \\((a + \\sum_{i=1}^{n}x_{i},b + n - \\sum_{i=1}^{n}x_{i})\\). Let‚Äôs visualize this for a few different datasets and sample sizes, for our chosen prior: prior &lt;- function(theta) dbeta(theta,shape1 = 12,shape2 = 12) posterior &lt;- function(theta,sumx,n) dbeta(theta,shape1 = 12 + sumx,shape2 = 12 + n - sumx) data_frame(x = c(0.01,0.99)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = prior, colour = &quot;blue&quot;) + stat_function(fun = posterior, args = list(sumx = 5,n = 10), colour = &quot;purple&quot;) + stat_function(fun = posterior, args = list(sumx = 0,n = 10), colour = &quot;red&quot;) + stat_function(fun = posterior, args = list(sumx = 10,n = 10), colour = &quot;orange&quot;) + labs(title = &quot;Beta Prior vs Posterior for Theta, 10 coin flips&quot;, subtitle = &quot;Blue: Prior. Purple: 5 heads. Red: 0 heads. Orange: 10 heads&quot;, x = &quot;Theta&quot;, y = &quot;Density&quot;) + scale_x_continuous(breaks = seq(0,1,by=0.1)) Some interesting points can be seen from this plot: When the observed data ‚Äúmatches‚Äù the prior, in the sense that we observe a dataset for which the original frequentist estimate of \\(\\theta\\) is very probable under the prior, the posterior becomes more peaked around that value. When the observed data are extreme, as in the case of 0 or 10 heads in 10 flips, the frequestist inference would also be extreme. In these cases, we would have estimated \\(\\hat{\\theta}_{freq} = 0\\) or \\(1\\). Because our prior distribution is not extreme, though, the posterior is more moderate, and ends up being peaked at the low/high end of the range of values that were reasonable under the prior. Exercise: Now is a good time to go back to the Shiny App you saw a few weeks ago, and see if you can elucidate the behaviour I describe above using your own simulations. 11.2 Estimation in Bayesian Inference: point and interval estimation With the posterior in hand, what do we actually do? We‚Äôre used to immediately having a point estimate from frequentist inference, and there we typically proceed to derive a confidence interval for the parameter using the sampling distribution of the estimator. The situation isn‚Äôt so different here. The posterior is analagous to the sampling distribution in the frequentist case, although the interpretation is different. In frequentist inference, we make statements about probable values for estimates in repeated sampling at fixed parameter values, and use this to infer the value of the parameter under which our observed data was most likely. In Bayesian inference, the posterior distribution is intepreted literally as the conditional distribution of the parameter given the data. We can just directly say things like ‚Äúthere is a \\(95\\%\\) chance that \\(\\theta\\) is between \\(0.4\\) and \\(0.5\\)‚Äù. Specifically, to obtain point estimates of parameters, we may use either the posterior mean: \\[ \\hat{\\theta}_{post. mean} = E(\\theta|X) = \\int_{\\theta}\\theta p(\\theta|X)d\\theta \\] interpreted as a weighted average of possible values of \\(\\theta\\), with weights corresponding to their posterior probabilities (densities); or, we may use the posterior mode: \\[ \\hat{\\theta}_{post. mode} = \\mbox{argmax}_{\\theta} p(\\theta|X) \\] which is the most probable value of \\(\\theta\\), given the observed data. In simple examples the posterior may be symmetric or nearly symmetric, so the two are nearly the same; in more complicated applications, either one is directly preferred, or both are computed and compared. For interval estimation, the frequentist notion of a confidence interval is replaced by a credible interval: an interval which has a specified probability of containing the parameter, given the observed data. Contrast this interpretation to that of the frequentist confidence interval, which states that a certain proportion of the intervals computed in the same manner from repeatedly sampled datasets would contain the parameter. The Bayesian credible interval interpretation is closer to how many people would interpret such an interval intuitively. Remark: technically, any interval \\(I\\) with \\(P(\\theta\\in I|X) = 1 - \\alpha\\) is a \\((1-\\alpha)\\times 100\\%\\) credible interval for \\(\\theta\\), having observed \\(X\\). In practical applications and certainly in this course, we choose the ‚ÄúHighest Posterior Density‚Äù credible interval‚Äì the shortest possible \\((1-\\alpha)\\times 100\\%\\) credible interval. This is the one that falls in the ‚Äúcentre‚Äù of the distribution, and is computed using quantiles, much in the same was as a confidence interval. It‚Äôs important that you know this, but going forward you can just assume we‚Äôre using credible intervals derived in this manner and don‚Äôt have to think about it each time. Computing such a credible interval is a matter of finding the corresponding quantiles of the posterior, which is either simple or complicated depending on what the posterior is. In our example the posterior is known completely, and we can get a \\(95\\%\\) credible interval using the qbeta function: # E.g. for n = 10 and sumx = 5 c(qbeta(0.025,shape1=12 + 5,shape2 = 12 + 5),qbeta(0.975,shape1=12 + 5,shape2 = 12 + 5)) ## [1] 0.3354445 0.6645555 The point estimate based off the posterior mode is the same as the frequentist estimate for these data, \\(\\hat{\\theta}_{freq} = \\hat{\\theta}_{post. mode} = 0.5\\), as can be seen from the plot above. The corresponding frequentist confidence interval is given by c(.5 - sqrt(.5*.5/10)*1.96,.5 + sqrt(.5*.5/10)*1.96) ## [1] 0.1900968 0.8099032 which is much wider. The Bayesian approach gives a more accurate estimate here, because we assumed strong prior information that ended up agreeing with the data. If the data had been more extreme, say \\(X = 1\\) heads in \\(n = 10\\) flips, then the situation is different. The frequentist point estimate would be \\(\\hat{\\theta}_{freq} = 0.1\\) with confidence interval: c(.1 - sqrt(.1*.9/10)*1.96,.1 + sqrt(.1*.9/10)*1.96) ## [1] -0.08594193 0.28594193 Observing a single head in \\(10\\) tosses leads us to believe strongly that \\(\\theta\\) must be small, and the corresponding confidence interval actually goes beyond the parameter space. It is true that if the coin were fair (\\(\\theta = 0.5\\)), the observed data had about a \\(1\\%\\) chance of occurring. But, if this did occur, we‚Äôd still want to make sensible inferences! If we had a prior belief that the coin had a probability of heads that is anywhere between \\(0.3\\) and \\(0.7\\), as above, we can compute the point and interval estimates obtained in a Bayesian setting: # Point estimate- find the posterior mode, which is a critical point # Use R&#39;s built in optimization tools, function nlminb() performs constrained minimization # Pass it a function that returns minus the posterior; minimizing -f(x) is the same as maximizing f(x) minus_posterior &lt;- function(theta,sumx,n) -1 * posterior(theta,sumx,n) opt &lt;- nlminb(objective = minus_posterior,start = 0.3,sumx = 1,n = 10,lower = 0.01,upper = 0.99) opt$par # Return the value at which the maximum occurs, i.e. the posterior mode ## [1] 0.375 # Credible interval c(qbeta(0.025,shape1=12 + 1,shape2 = 12 + 9),qbeta(0.975,shape1=12 + 1,shape2 = 12 + 9)) ## [1] 0.2290662 0.5487546 This interval is much more reasonable, stays within the parameter space, and still even includes the possibility that the coin is fair‚Äì good, since we only flipped the coin \\(n = 10\\) times! Had we increased the sample size and observed a similarly extreme result, the posterior would become more centered in that region of the parameter space- that is, observing an equally extreme result with more data would diminish the effect of the prior on the resulting inferences. You can play around once more in the Shiny App to get a feel for the comparison between frequentist confidence intervals and bayesian credible intervals works in this example. 11.3 Choosing a Prior The following section on choosing a prior distribution is more subjective, and doesn‚Äôt include any calculations. It is still part of the course material and important to understand. The choice of prior distribution is up to the analyst. There is no formula for doing this that will work in every problem; we can, though, discuss a few guidelines for doing so. When choosing a prior, you should consider at a minimum: Reasonability: does the chosen prior give reasonable prior estimates for parameters, having observed no data? Put another way, does it put mass in regions of the parameter space where the parameter is likely to be, and does it put mass in regions where it is not likely to be? Sensitivity: how much does the prior we choose actually affect the posterior, and in what ways? Does a given prior get ‚Äúswamped‚Äù by the data, and how much data does it take for the prior to have negligible effect on the posterior? Does the prior affect the posterior differently for more ‚Äúextreme‚Äù data than for less ‚Äúextreme‚Äù data? Tractability: do the prior and likelihood combine to give a posterior for which we can compute point estimates and credible intervals (quantiles)? Can we evaluate the integral required to compute the normalization constant? Can the posterior density/distribution (with or without the constant) be evaluated with a reasonable computational complexity? These are just some of the questions to ask when choosing a prior. It may sound like more work that in the frequentist paradigm, but an advantage of the Bayesian approach is that it makes it relatively simple for us to ask these questions of our modelling procedure. How did we choose our prior for the coin-flipping example? To begin, we knew that the parameter of interest, \\(\\theta\\), was bounded on \\((0,1)\\) and could take any real value in that region, so we considered only distributions that were continuous and defined on \\((0,1)\\). That alone narrowed it down- then we thought about what shape we wanted the distribution to have. We didn‚Äôt really have any idea about this, so we picked a distribution with a very flexible shape. We then chose hyperparameters (the parameters of the prior distribution, that we specify in advance) that gave us a reasonable location and spread of this distribution (more on this below). We then did a sensitivity analysis, showing the prior/posterior for various potential observed datasets, and even used a simple Shiny App to get a feel for how different priors and datasets would combine in the posterior. All this was to ensure that our choice gives reasonable inferences for datasets that we could possibly/are likely to see. If this is sounding like it should be easy, it isn‚Äôt. I used a concept we haven‚Äôt learned yet that renders a Beta distribution an ‚Äúobvious‚Äù choice for a prior on \\(\\theta\\) for a \\(Bern(\\theta)\\) distribution: the Beta is the conjugate prior for the Bernoulli. 11.3.1 Conjugate Priors A conjugate prior, in relation to a specific likelihood, is a prior that when combined with that likelihood gives a posterior with the same functional form as that prior. The Beta/Bernoulli we saw above is an example of this, because we found: \\[ \\begin{aligned} \\mbox{Prior: } &amp; p(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\\\ \\mbox{Likelihood: } &amp; \\ell(X|\\theta) \\propto \\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}} \\\\ \\implies \\mbox{ Posterior: } &amp; p(\\theta | X) \\propto \\theta^{a + \\sum_{i=1}^{n}x_{i} - 1}(1-\\theta)^{b + n - \\sum_{i=1}^{n}x_{i} - 1} \\end{aligned} \\] The prior has the form \\(\\theta^{a-1}(1-\\theta)^{b - 1}\\), and the posterior has the form \\(\\theta^{c-1}(1-\\theta)^{d - 1}\\), with \\(c\\) and \\(d\\) depending on \\(a\\) and \\(b\\) as well as the data. The posterior has the same functional form as the prior, with parameters that are updated after the data is observed. Conjugate priors are great because they are mathematically tractible, and allow us to easily evaluate the impact of the prior distribution on the posterior under different datasets. It is often not possible, though, to find a conjugate prior for a given likelihood in anything but the most simple examples. Exercise: Here are some common likelihoods and their conjugate priors; as an exercise, verify that each posterior is in the same family as the prior, and find expressions for the updated parameters: Likelihood Prior Posterior Bernoulli or Binomial, \\(P(X = x) = \\theta^{x}(1-\\theta)^{1-x}\\) \\(\\theta \\sim Beta(a,b)\\) ??? Poisson, \\(P(X = x) = \\frac{\\lambda^{x} e^{-\\lambda}}{x!}\\) \\(\\lambda \\sim Gamma(a,b)\\) ??? Normal, \\(f(x|\\mu,\\tau) = \\sqrt{\\frac{\\tau}{2\\pi}}\\exp\\left( -\\frac{\\tau}{2} (x - \\mu)^{2} \\right)\\) (note \\(\\tau = 1/\\sigma^{2}\\) is called the precision, and is the inverse of the variance) \\(\\mu \\sim Normal(m,v)\\), \\(\\tau \\sim Gamma(a,b)\\) ??? Wikipedia has a great list containing many more examples. 11.3.2 Setting Hyperparameters by moment-matching When using a conjugate prior (or any prior), once a family of distributions like Beta, Normal, Gamma, etc is chosen, the analyst still needs to set hyperparameter values. We did this above- first we chose a \\(Beta(a,b)\\) family of distributions, then we went a step further and actually specified \\(a = 12\\) and \\(b = 12\\). How did we come up with such wonky looking values of \\(a\\) and \\(b\\)? We will discuss two ways here. A very direct way to encode your prior beliefs about the range of reasonable values of a parameter into a prior distribution is by setting hyperparameters via moment-matching. Analagous to the Method of Moments in frequentist estimation, we pick prior moments (mean, variance, etc) that give us a sensible range of values for the parameter, then find the prior hyperparameters that give us those moments. This is where we got the \\((12,12)\\) in the above example. Suppose we think that, prior to seeing any data, \\(\\theta\\) is most likely to be around \\(0.5\\), with values on in either direction away from this being equally likely, and that \\(\\theta\\) is most likely between \\(0.3\\) and \\(0.7\\). Translate this statement into mathematical terms: we think the prior should be peaked at \\(0.5\\) and be symmetric about that value, which implies that its mean is also \\(0.5\\). We think that ‚Äúmost‚Äù of the mass should be between \\(0.3\\) and \\(0.7\\), so let‚Äôs say that \\(0.3\\) and \\(0.7\\) should be two standard deviations away from \\(E(\\theta) = 0.5\\). This gives \\(SD(\\theta) = 0.1\\). A \\(Beta(a,b)\\) distribution has mean \\(E(\\theta) = \\frac{a}{a+b}\\) and \\(Var(\\theta) = \\frac{ab}{(a+b)^{2}(a+b+1)}\\). Moment-matching proceeds by setting these equal to the values we decided on above, and solving for \\(a\\) and \\(b\\): \\[ \\begin{aligned} E(\\theta) = \\frac{a}{a+b} &amp;= 0.5 \\\\ Var(\\theta) = \\frac{ab}{(a+b)^{2}(a+b+1)} &amp;= 0.1^{2} \\\\ \\implies (a,b) &amp;= (12,12) \\end{aligned} \\] As an exercise, verify the solutions to the above equations. We can verify that our answer is correct computationally by taking a sample from a Beta distribution with these parameters, and checking that the mean and standard deviation are close to what we want: x &lt;- rbeta(1000,12,12) c(mean(x),sd(x)) ## [1] 0.5008392 0.1017135 library(tidyverse) "],
["section-predictive-modelling.html", "Chapter 12 Predictive Modelling 12.1 Overview 12.2 Plug-in prediction: coin flipping 12.3 Bayesian Prediction: coin flipping 12.4 Extended example: Income and Advertsing datasets 12.5 Extended example: predicting call centre wait times", " Chapter 12 Predictive Modelling This chapter is a supplementary tutorial on predictive modelling for STA238. The readings on this topic are An Introduction to Statistical Learning with Applications in R (ISLR), sections 2.1, 2.1.1, 2.2, 2.2.1, and 2.2.2; and Probability and Statistics: the Science of Uncertainty, section 7.2.4. The former has an overview of predictive modelling; the latter treats the Bayesian perspective. The assigned exercises associated with this material are from ISLR Section 2.4. Do questions 1, 2, 3, 4, 5, 8, 9, 10. 12.1 Overview Up to this point in the course, we have been focussing on data analysis and inference. Analyzing the specific data that you have available helps answer the question ‚Äúwhat happened?‚Äù. Combining the available data with computational and mathematical modelling helps answer the question ‚Äúwhat can be learned about the world, based on what happened?‚Äù. Predictive modelling focusses on answering ‚Äúwhat is going to happen next?‚Äù. While the former two questions are of primary scientific interest, modern data science in business and industrial practice is often concerned with making predictions about what kind of data will be seen in the future. Predictive modelling, which is itself a science, is applied to address this question. Often, the point estimates we get from our inferential procedures are themselves predictions. For example, we built a regression model of TTC ridership revenue over time back in chapter 4 of these supplementary notes. We inferred the slope of the regression line of revenue across years‚Äì but we could also use this line to make predictions for future years by simply extending the line to the year we want, and reading off its value. However, when measuring the uncertainty in our predictions, the situation is different. When doing inference, we have one source of uncertainty that we choose to address: the variability in the data. If we sampled the dataset again, we would get different inferences, and so on. In prediction, we have two sources of uncertainty: that from the model that we use for prediction, which we inferred from the data and hence is subject to variability; and that from the act of sampling a new datapoint from this model. ISLR referrs to these two sources of error as reducible and irreducible. We can always make our inferences less uncertain in theory by getting more data (reducibile uncertainty), but even if we knew the underlying data-generating mechanism perfectly, its output is still random (irreducible uncertainty)! In fact: recall we measured the quality of an estimator \\(\\hat{\\theta}\\) of a parameter \\(\\theta\\) using the Mean Squared Error (MSE): \\(\\mathbb{E}(\\hat{\\theta} - \\theta)^{2}\\). The analagous concept in prediction is the Mean Square Prediction Error (MSPE). Suppose we use some function \\(\\hat{Y} \\equiv \\hat{Y}(Y_{1},\\ldots,Y_{n})\\) to predict a new \\(Y_{n+1}\\) using a random sample \\(Y_{1},\\ldots,Y_{n}\\), where \\(Y_{i}, i = 1\\ldots n+1\\) are drawn IID from the same (unknown) distribution. Then define: \\[\\begin{equation} MSPE(\\hat{Y}) = \\mathbb{E}(\\hat{Y}_{n+1} - Y_{n+1})^{2} \\end{equation}\\] ISLR (page 19) makes an argument that in the simple model \\(Y = \\theta + \\epsilon\\) where \\(\\epsilon\\) is a zero-mean ‚Äúnoise‚Äù random variable which is independent of both \\(Y_{n+1}\\) and \\(\\hat{Y}_{n+1}\\) and \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\), that \\[\\begin{equation}\\begin{aligned} MSPE(\\hat{Y}) &amp;= \\mathbb{E}(\\hat{\\theta} - \\theta)^{2} + \\mathbb{E}(\\epsilon^{2}) \\\\ &amp;= MSE(\\hat{\\theta}) + \\text{Var}(\\epsilon) \\end{aligned}\\end{equation}\\] Exercise: prove this formula. Hint: look at what ISLR does, figure out the differences in notation between us and them, and fill in the details. The key takeaway is that the prediction error comes from two sources: error in inferring the underlying data-generating mechanism, and error in drawing new data from this mechanism. 12.2 Plug-in prediction: coin flipping To illustrate the idea of multiple sources of error, let‚Äôs briefly revist our now-classic coin flipping example. We are going to This is an example of a classification problem: we are predicting a binary event, something that either happens or it doesn‚Äôt. We‚Äôre either completely right, or completely wrong. Note that the MSPE here has a special form, because \\(|\\hat{Y} - Y| \\in \\left\\{ 0,1 \\right\\}\\). Exercise: show that the MSPE here is equal to the probability of making an incorrect prediction. First let‚Äôs draw a sample. We‚Äôll leave the parameters to be specified at the top of the program so you can play around with them: set.seed(87886) n &lt;- 10 # Number of flips p0 &lt;- 0.5 # True probability of heads # Draw the sample samp &lt;- rbinom(n,1,p0) samp ## [1] 1 0 0 0 1 0 1 0 0 1 We have drawn \\(n = 10\\) samples from a \\(\\text{Bernoulli}(p)\\) distribution with parameter \\(p = 0.5\\). If \\(Y_{i}\\) is a \\(0/1\\) indicator of the \\(i^{th}\\) toss being heads, the likelihood function for \\(p\\) is given by: \\[\\begin{equation} L(p) = p^{\\sum_{i=1}^{n}Y_{i}}(1-p)^{n - \\sum_{i=1}^{n}Y_{i}} \\end{equation}\\] Exercise (review): show that the maximum likelihood estimator is \\(\\hat{p} = \\frac{1}{n}\\sum_{i=1}^{n}Y_{i}\\), the sample proportion of heads. We can compute the MLE for our sample: pmle &lt;- mean(samp) pmle ## [1] 0.4 How to use this to make predictions? We need a rule‚Äîa function \\(\\hat{Y}\\)‚Äîthat takes in our sample and predicts the next value. This construction is very similar in spirit to constructing an estimate of a parameter; we want something that makes intuitive sense and that has ‚Äúdesirable‚Äù statistical properties. It can be shown that the prediction rule that minimizes the MSPE for this simple case of binary prediction is to simply predict the outcome (heads or tails, 0 or 1) that is the most probable under our estimated model. Our inferred model ascribes a \\(40\\%\\) chance of heads and \\(60\\%\\) chance of tails to each flip, so our prediction rule (for this sample) is simply \\(\\hat{Y} = 0\\). Exercise: suppose that \\(p_{0} = \\hat{p} = 0.4\\), that is, suppose we were able to perfectly infer the value of \\(p\\) based on our sample and hence incur no model error. Show that the MSPE for our prediction rule is \\(\\mathbb{E}(\\hat{Y} - Y)^{2} = p_{0} = 0.4\\). Exercise (challenge): now suppose that the true value of \\(p\\) is \\(p_{0} = 0.5\\) (which it actually is, in our simulation). Show that the MSPE of our prediction rule equals \\(0.5\\) when you incorporate the model error into the calculation. Hint: recall the ‚Äútower property‚Äù of expectation: for any two random variables \\(X,Y\\), \\[\\begin{equation} \\mathbb{E}(X) = \\mathbb{E}_{Y}\\mathbb{E}_{X|Y}(X) \\end{equation}\\] That is, you can first take the expectation with respect to one random variable conditional on another, and then take the expectation of this expression with respect to the second random variable. Use this to separate the model variability from the sampling variability. Let‚Äôs assess the MSPE using a prediction. We‚Äôre going to repeat the following procedure a bunch of times: set.seed(874327086) n &lt;- 10 # Number of flips p0 &lt;- 0.5 # True probability of heads B &lt;- 1000 # Number of simulations to do do_simulation &lt;- function(n,p) { pmle &lt;- mean(rbinom(n,1,p)) # Compute the MLE newy &lt;- rbinom(1,1,p) # Draw a new value from the true distribution if (pmle &gt;= 0.5) { predy &lt;- 1 # Predict 1 if pmle &gt;= 0.5 } else { predy &lt;- 0 } (predy - newy)^2 } # Do the simulation sim_mspe &lt;- 1:B %&gt;% map(~do_simulation(n = n,p = p0)) %&gt;% reduce(c) %&gt;% mean() sim_mspe ## [1] 0.498 Notice that it is close to \\(0.5\\), not \\(0.4\\)‚Äì the model error matters, as you saw in the above theoretical exercise. Actually, to drive this point home‚Ä¶ Exercise: what did I do differently in the below code, compared to the above? set.seed(874327086) n &lt;- 10 # Number of flips p0 &lt;- 0.5 # True probability of heads B &lt;- 1000 # Number of simulations to do do_simulation &lt;- function(n,p) { pmle &lt;- mean(rbinom(n,1,p)) # Compute the MLE newy &lt;- rbinom(1,1,pmle) # Draw a new value from the true distribution if (pmle &gt;= 0.5) { predy &lt;- 1 # Predict 1 if pmle &gt;= 0.5 } else { predy &lt;- 0 } (predy - newy)^2 } # Do the simulation sim_mspe2 &lt;- 1:B %&gt;% map(~do_simulation(n = n,p = p0)) %&gt;% reduce(c) %&gt;% mean() sim_mspe2 ## [1] 0.389 Figure out why it‚Äôs lower, computationally (what did I do differently in the code), mathematically (relate it to the above exercises), and intuitively (explain in words what the difference is). 12.3 Bayesian Prediction: coin flipping We saw above that making predictions using our frequentist-inferred models for the observed data wasn‚Äôt that complicated (just ‚Äúplug in‚Äù the inferred model parameters), but assessing the uncertainty in the predictions was challenging because the uncertainty comes from multiple different sources, and it is not always clear how to effectively combine these. One more systematic way to quantify the uncertainty in a prediction, conditional on the observed sample is, well, to literally do this, via the conditional distribution \\(\\pi(Y_{n+1}|Y_{1},\\ldots,Y_{n})\\). The mean of this distribution says ‚Äúwhat will happen on average, given what we have observed?‚Äù, and the standard deviation of this distribution quantifies how close future values are to be concentrated around this mean, which seems like a perfectly reasonable measure of the uncertainty in a prediction. So what‚Äôs the problem? If we‚Äôre in the frequentist paradigm, there is a big one: the data are IID! So \\(Y_{n+1}\\) is statistically independent of \\(Y_{1},\\ldots,Y_{n}\\), and \\[\\begin{equation} \\pi(Y_{n+1}|Y_{1},\\ldots,Y_{n}) = \\pi(Y_{n+1}) \\end{equation}\\] Under this paradigm, the sample provides no information about the distribution of a new datapoint. In the Bayesian paradigm, things are a bit different. Following Evans and Rosenthal Section 7.2.4, we consider the posterior predictive distribution: \\[\\begin{equation} \\pi(Y_{n+1}|Y_{1},\\ldots,Y_{n}) = \\int_{0}^{1}\\pi(Y_{n+1}|p)\\pi(p|Y_{1},\\ldots,Y_{n})dp \\end{equation}\\] where \\(\\pi(p|Y_{1},\\ldots,Y_{n})\\) is the posterior distribution of \\(p\\) given the sample, and \\(\\pi(Y_{n+1}|p)\\) is simply the likelihood for each value of \\(p\\), and for a single datapoint. This has the following attractive properties: Let‚Äôs see what this looks like for the coin flipping example. Recall the setup: we use a \\(\\text{Beta}(\\alpha,\\beta)\\) prior for the parameter \\(p\\), which leads to the posterior: \\[\\begin{equation} \\pi(p|Y_{1},\\ldots,Y_{n}) = \\text{Beta}\\left(n\\bar{Y} + \\alpha,n(1 - \\bar{Y}) + \\beta\\right) \\end{equation}\\] Evans and Rosenthal derive the posterior predictive distribution using messier, but similar algebra to how we derived the posterior: \\[\\begin{equation} Y_{n+1} | Y_{1},\\ldots,Y_{n} \\sim \\text{Bernoulli}\\left(\\frac{n\\bar{Y} + \\alpha}{n + \\alpha + \\beta}\\right) \\end{equation}\\] Exercise: derive the formula for the posterior predictive as reported by Evans and Rosenthal. We use the posterior predictive mode for prediction. Since the posterior predictive distribution is only defined at two values, \\(0\\) and \\(1\\), the mode is simply whichever value has the higher posterior predictive density. Exercise: prove Evans and Rosenthal‚Äôs formula for the posterior predictive mode: \\[\\begin{equation} \\hat{Y}_{n+1} | Y_{1},\\ldots,Y_{n} = \\begin{cases} 1 \\ \\text{if} \\ \\frac{n\\bar{Y} + \\alpha}{n + \\alpha + \\beta} \\geq \\frac{n(1-\\bar{Y}) + \\beta}{n + \\alpha + \\beta} \\\\ 0 \\ \\text{else} \\end{cases} \\end{equation}\\] Hint: start by writing out the Bernoulli density. How does the use of the posterior predictive mode do in terms of MSPE? Consider the setup from chapter 11 of these supplementary notes, where we put a \\(\\text{Beta}(12,12)\\) prior on \\(p\\). Let‚Äôs simulate the MSPE again, but using this for prediction: set.seed(874327086) n &lt;- 10 # Number of flips p0 &lt;- 0.5 # True probability of heads alpha &lt;- 12 # Prior beta &lt;- 12 # Prior B &lt;- 1000 # Number of simulations to do do_simulation_bayes &lt;- function(n,p) { pmle &lt;- mean(rbinom(n,1,p)) # Compute the MLE newy &lt;- rbinom(1,1,p) # Draw a new value from the true distribution postparam &lt;- (n * pmle + alpha) / (n + alpha + beta) if (postparam &gt;= (1 - postparam)) { predy &lt;- 1 # Predict 1 if postparam &gt;= (1 - postparam) (i.e. postparam &gt;= 0.5) } else { predy &lt;- 0 } (predy - newy)^2 } # Do the simulation sim_mspe_bayes &lt;- 1:B %&gt;% map(~do_simulation_bayes(n = n,p = p0)) %&gt;% reduce(c) %&gt;% mean() sim_mspe_bayes ## [1] 0.498 Notice anything odd about these results compared to what we got using the frequentist approach? It turns out that if the prior is symmetric (\\(\\alpha = \\beta\\)), then the posterior mode exactly equals the prediction rule we obtained by minimizing the MSPE. If the prior is not symmetric, they won‚Äôt be the same‚Äì so which do you expect to be better? 12.4 Extended example: Income and Advertsing datasets Let‚Äôs recreate the examples from chapter 2 of ISLR. 12.5 Extended example: predicting call centre wait times The coin-flipping example is illustrative, but too simple to be interesting. For a given dataset, we either predict heads, or tails, the same for all future coins! For a more interesting example, we‚Äôll fit a curve to Toronto 311 contact centre wait times. We‚Äôll use a minor extension of the linear regression we saw in chapter 4 of these supplementary notes. It‚Äôs more important to focus on what we‚Äôre doing (prediction) than how we‚Äôre doing it (linear regression with polynomials). Materials in this tutorial are taken from the (much) more advanced tutorial by Alex on this stuff, here. 311 is a service operated by the City of Toronto in which residents can call (dial *311) and submit service requests for things like potholes, excess garbage pickup, downed trees, and so on. People call in to the service and speak to an actual human being in a contact centre. When you call, sometimes you have to wait. Data on the daily average wait time for such calls for the period from December 28th, 2009 to January 2nd, 2011 is available from Open Data Toronto here, and stored on github by us for your use. Let‚Äôs read in and visualize the data: # Read data contactdat &lt;- readr::read_csv( file = &quot;https://media.githubusercontent.com/media/awstringer1/leaf2018/gh-pages/datasets/contact-centre.csv&quot;, col_names = TRUE, col_types = &quot;cd&quot;) %&gt;% mutate_at(&quot;date&quot;,lubridate::mdy) glimpse(contactdat) ## Observations: 371 ## Variables: 2 ## $ date &lt;date&gt; 2009-12-28, 2009-12-29, 2009-12-30, 2009-12-31, 20‚Ä¶ ## $ answer_speed &lt;dbl&gt; 55, 67, 56, 15, 9, 62, 51, 20, 16, 14, 23, 33, 14, ‚Ä¶ contactdat %&gt;% ggplot(aes(x = date,y = answer_speed)) + theme_classic() + geom_point(pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;) + labs(title = &quot;Average Wait Time by Day, Toronto 311 Contact Centre&quot;, x = &quot;Date&quot;, y = &quot;Wait Time&quot;) Woah! That spike in wait times in July is terrible. If we were analyzing these data with the intent of informing any actual policy, we would contact the 311 office and ask what the heck happened there before proceeding. Let‚Äôs proceed. To mitigate the drastic spike, consider a log transformation of the answer_speed: contactdat %&gt;% ggplot(aes(x = date,y = log(answer_speed))) + theme_classic() + geom_point(pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;) + labs(title = &quot;Average Wait Time by Day, Toronto 311 Contact Centre&quot;, x = &quot;Date&quot;, y = &quot;log(Wait Time)&quot;) There‚Äôs a lot of variance, but it looks like there might be a kind of quadratic trend. Recall the simple linear model from Chapter 4 of these supplementary notes: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\] where \\(y_{i}\\) is the average wait time on the \\(i^{th}\\) date, \\(x_{i}\\) is a suitable numerical value for the \\(i^{th}\\) date (e.g. number of days since some reference point), and \\(\\epsilon_{i} \\sim N(0,\\sigma^{2})\\) is the error of the \\(i^{th}\\) observation about its mean. Because it looks like the trend in the data might be quadratic, not linear, we can extend this slightly: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{i}^{2} + \\epsilon_{i} \\] In general, it‚Äôs not too much of an extension to consider models of the form \\[ y_{i} = \\beta_{0} + \\sum_{k=1}^{p}\\beta_{k}x_{i}^{k} + \\epsilon_{i} \\] The higher we choose \\(p\\), the degree of polynomial, the closer we will fit to the observed data, and the better we will be able to predict the datapoints we have already seen. However, paradoxically, models that are too complicated will actually predict new data worse! This is due in part to the variance-bias tradeoff discussed in this week‚Äôs readings in ISLR. Let‚Äôs fit the model(s) and assess the in-sample mean-square prediction error. # THIS CODE IS ADVANCED-- the idea is important, but you won&#39;t be # tested on the following CODE. contact_std &lt;- contactdat %&gt;% mutate_at(&quot;date&quot;,as.numeric) %&gt;% mutate_at(&quot;answer_speed&quot;,log) %&gt;% mutate_all(list(~(. - mean(.)) / sd(.))) # Fit a polynomial regression of degree p fit_poly_regression &lt;- function(p) { lm(answer_speed ~ poly(date,degree = p,raw = TRUE), data = contact_std) } contactmod_1 &lt;- fit_poly_regression(1) contactmod_2 &lt;- fit_poly_regression(2) contactmod_10 &lt;- fit_poly_regression(10) contactmod_100 &lt;- fit_poly_regression(100) # Plot of the predictions prediction_plot &lt;- function(mod) { contact_std %&gt;% mutate(pred = predict(mod)) %&gt;% ggplot(aes(x = date)) + theme_classic() + geom_point(aes(y = answer_speed),pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;) + geom_line(aes(y = pred),colour = &quot;purple&quot;) } cowplot::plot_grid( prediction_plot(contactmod_1), prediction_plot(contactmod_2), prediction_plot(contactmod_10), prediction_plot(contactmod_100), nrow = 2 ) Exercise: I deliberately didn‚Äôt label the plots. Can you guess which plot is from which degree of polynomial? Remember that higher degree polynomials should be more wiggly than lower degrees, and should give better in-sample predictions than lower degrees. Indeed, if we compute the MSPE for each of these models, we find the higher the degree, the better the fit to the data we have observed: compute_mspe_polymod &lt;- function(mod) { mean(( predict(mod) - contact_std$answer_speed )^2) } compute_mspe_polymod(contactmod_1) ## [1] 0.9847131 compute_mspe_polymod(contactmod_2) ## [1] 0.7439483 compute_mspe_polymod(contactmod_10) ## [1] 0.5907413 compute_mspe_polymod(contactmod_100) ## [1] 0.466288 The problem with this is that the MSPE is supposed to be for predicting a new datapoint, not one that you‚Äôve already seen! To get a better estimate of the real MSPE, and hence tell how the algorithm might do on new datapoints, we can bootstrap it. We‚Äôll use a slightly different kind of bootstrap, called cross-validation, where we There are many types of cross-validation; this one is called ‚ÄúK-fold‚Äù cross validation. We can implement this as follows: set.seed(809796857) do_crossval &lt;- function(p,K = 10) { n &lt;- nrow(contact_std) mspevec &lt;- numeric(K) # Split the data into K &quot;folds&quot; # First randomly shuffle the rows: contact_std_shuffle &lt;- contact_std[sample.int(n), ] # Then split the data in order: datachunks &lt;- split(contact_std_shuffle,rep(1:K,each = round(n/K))[1:n]) # Now fit the model to each combination of K-1 chunks, and predict # on the Kth chunk: for (k in 1:K) { ds &lt;- reduce(datachunks[(1:K)[-k]],bind_rows) mod &lt;- lm(answer_speed ~ poly(date,degree = p,raw = TRUE), data = ds) mspevec[k] &lt;- mean( (predict(mod,newdata = datachunks[[k]]) - datachunks[[k]]$answer_speed)^2 ) } mean(mspevec) } do_crossval(1) ## [1] 0.9951656 do_crossval(2) ## [1] 0.7489409 do_crossval(10) ## [1] 0.6090095 do_crossval(100) ## [1] 4.185759 Woah! The fit on new data‚Äìthat is, the quality of the predictions‚Äì seems to improve as the degree of polynomial increases, but only to a certain point. For the \\(p = 100\\) degree polynomial, the out-of-sample MSPE is terrible. The reason is because of the bias-variance tradeoff. The lower-degree models have some bias‚Äìthey don‚Äôt predict perfectly in-sample‚Äìbut they have low variance, in that the inferred model parameters tend to be similar across datasets. Consider again the \\(p = 1\\) and \\(p = 100\\) models: cowplot::plot_grid( prediction_plot(contactmod_1), prediction_plot(contactmod_100), nrow = 1 ) The \\(p = 1\\) plot (exercise: make sure you can tell which is which) doesn‚Äôt predict very many datapoints correctly. The \\(p = 100\\) hugs the observed data much closer. But because of this, if we sample a new dataset‚Ä¶ # Take a bootstrap sample contact_new &lt;- contact_std[sample.int(nrow(contact_std),replace = TRUE), ] contactmod_new_1 &lt;- lm(answer_speed ~ poly(date,degree = 1,raw = TRUE),data = contact_new) contactmod_new_100 &lt;- lm(answer_speed ~ poly(date,degree = 100,raw = TRUE),data = contact_new) prediction_plot_new &lt;- function(mod) { contact_new %&gt;% mutate(pred = predict(mod)) %&gt;% ggplot(aes(x = date)) + theme_classic() + geom_point(aes(y = answer_speed),pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;) + geom_line(aes(y = pred),colour = &quot;purple&quot;) } cowplot::plot_grid( prediction_plot_new(contactmod_new_1), prediction_plot_new(contactmod_new_100), nrow = 1 ) The \\(p = 1\\) model remains nearly identical, while the \\(p = 100\\) model changes! High variance refers to the fact that slight changes in the sample cause big changes in the model‚Äì and this leads to bad predictions on new data. "],
["section-installing-r-and-rstudio.html", "Chapter 13 Installing R and RStudio 13.1 Installing R 13.2 Installing RStudio 13.3 Using RMarkdown", " Chapter 13 Installing R and RStudio This supplementary document contains a guided tutorial on how to install R and RStudio, and create a simple document with RMarkdown. It is your responsibility to have access to a computer on which you can use these tools. We want to help! However, us providing this help should in no way be interpreted as us taking on the responsibility of getting these tools working. In particular, you have an assignment due in the second week of classes that requires these tools, and it is 100% your responsibility to complete it on time. Most of these screenshots are taken from https://rstudio.com/products/rstudio/. 13.1 Installing R R is the scientific computing language that we will use in this course to perform statistical compuations‚Äì reading and manipulating data, performing scientific computations, and creating automated reports. Install R on your Windows, MacOS or Linux platform by going to https://cran.rstudio.com/. Choose the correct link based on your operating system: This tutorial proceeds with Windows and Mac; if you‚Äôre a Linux user (good for you!), I‚Äôm assuming you‚Äôre familiar with how to install software like this. Windows: click ‚Äúbase‚Äù: Mac: scroll down and click on the link for the ‚Äúlatest version‚Äù. At the time of this writing, it‚Äôs 3.6.1, but don‚Äôt worry if it‚Äôs increased when you‚Äôre doing this. Follow the instructions for your operating system to install the software. There should be a new icon for R, like this on a Mac: and similar on Windows. Click it to open the software. You should see something like: on a Mac, and again similar on Windows. Type print(&quot;Hello world!&quot;) in the console and press Enter: If it runs without errors, good job, you‚Äôve installed R! 13.2 Installing RStudio R is the underlying platform that executes code that you write and returns results. However, you won‚Äôt usually use R directly. You will use R through RStudio, the most popular Integrated Development Environment (IDE) for R. You can download RStudio for your platform here. Select RStudio Desktop, free version, and make sure to install the correct one for your platform. I think this is displayed automatically; when I go there on my Mac, I see: Download the installation file and follow the instructions for your system. An icon will appear: Click this icon to open RStudio. You should see something like this (note: I have set my favourite visual settings, so yours will look slightly different): The IDE contains four panels. The two important ones for now are: The console. You can type R code here, and press Enter to run it. It is the same as the console from R that you saw in the previous step. The editor. This is where you will write your programs which involve more than one command‚Äì so, all of your programs. This is where I am currently writing this tutorial! It is just a text editor. You can write code, highlight it, and then press Cmd+Enter or Shift+Enter to run it. Give it a try: click the + icon in the top right corner of RStudio, and click ‚ÄúR script‚Äù: A blank script will open. Type print(&quot;Hello World!&quot;), highlight the code, and press Cmd+Enter (mac) or Ctrl+Enter (windows): If this works, you are successfully using RStudio! 13.3 Using RMarkdown One of the real powers of using R and RStudio is the ability to automatically create typeset documents containing the results of running your code combined with text. First, make sure the rmarkdown package is installed. Open RStudio and in the Console, type install.packages(&quot;rmarkdown&quot;). The rmarkdown package will be installed. Now, click the + icon in the top-right corner, and select ‚ÄúR Markdown‚Äù: Leave all the default settings as they are. You should see a new script open up, prepopulated with some text and code: Click anywhere in this document (so the cursor is in the editor) and press Cmd+Shift+K or Ctrl+Shift+K. You‚Äôll be prompted to save the script; do this, and wait a moment, and then a document will magically be created and appear in the Viewer pane: This document is itself a tutorial for using RMarkdown. Read this; it explains how to add text, and code, right in the editor, and then magically create a report. When you change the code or the text, you create the report again, and everything is typeset properly. Not only does this save you time, but it ensures that all your output matches the code that was used to create it, reducing human error. This notion is part of the broader concept of reproducibility, which is very important in modern scientific practice. As a test of your use of RMarkdown, try to add the following three items to this report: The title ‚ÄúMy First RMarkdown Report‚Äù. Look up at the top where it says ‚Äútitle‚Äù. The first few rows of the cars dataset. You can get this with the head(cars) command. My output looks like: A pairs plot of the iris dataset. You can add this with pairs(iris). My output looks like: "]
]
