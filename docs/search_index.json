[["index.html", "Probability, Statistics, and Data Analysis Chapter 1 Introduction", " Probability, Statistics, and Data Analysis Alison Gibbs and Alex Stringer 2020-11-09 Chapter 1 Introduction This materials cover concepts from a traditional mathematical statistics course with less of a focus on theory and more on simulation and data analysis. They are designed to accompany or supplement such a course and were first used in STA238: Probability, Statistics and Data Analysis, Winter 2020 at the University of Toronto, taught by Alison Gibbs and Alex Stringer. These notes are designed to stand alone, however the content pulls heavily from the following sources: [MIPS] F.M. Dekking, C. Kraaikamp H.P. Lopuha ̈a and L.E. Meester (2005). A modern Introduction to Probability and Statistics: Understanding How and Why. Springer-Verlag. This is the primary reference for the course. This book is available in the University of Toronto bookstore. A pdf version of this textbook is freely available through the University of Toronto library website. [E&amp;R] M.J. Evans and J.S. Rosenthal (2003). Probability and Statistics: The Science of Uncertainty. W.H. Freeman and Co.Available in pdf here:http://www.utstat.toronto.edu/mikevans/jeffrosenthal. [ISL] G. James, D. Witten, T. Hastie and R. Tibshirani (2013). An Introduction to Statistical Learning with Applications in R. Springer. Available in pdf here:http://faculty.marshall.usc.edu/gareth-james/ISL Suggested practice problems from these sources are listed in the final chapter of these materials. You can find the code used to create this book here. All of the data is stored in the data folder in this repository. You can look at the code for each chapter and copy bits and run them, though of course we recommend typing them out yourself! Thanks and enjoy! Alex Stringer, Alison Gibbs, and Sam Caetano "],["section-introduction-to-data-analysis-data-input-and-basic-summaries.html", "Chapter 2 Introduction to Data Analysis: Data Input and Basic Summaries 2.1 Old Faithful 2.2 Drilling 2.3 Exercises 2.4 Extended example: smoking and age and mortality 2.5 Case study: rental housing in Toronto", " Chapter 2 Introduction to Data Analysis: Data Input and Basic Summaries This chapter shows you how to read in simple datasets and make simple numerical and graphical summaries of them. We’ll use data from the book A Modern Introduction to Probability and Statistics (MIPS) which can be found here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. This will be referred to as “the book data folder”. This chapter corresponds to Chapters 15 and 16 in MIPS. 2.1 Old Faithful The Old Faithful data is stored in the oldfaithful.txt file in the book data folder. To read it in to R, first you need to look at its contents. 2.1.1 Read in the data You can either open it in a text editor (since it’s small), or print out the first few lines on the command line. The latter is nicer: the data is printed properly, without added bits from notepad or whatever; and if you’re working with large data stored in big text files, trying to open the whole thing in an editor will crash your computer. It’s good to get in the habit of working a bit on the command line now. If you have a Mac or Linux-based system, open a terminal. If you have a Windows based system, get a Mac or Linux. Just kidding (not really though). Download Git Bash: https://git-scm.com/download/win and open a new terminal window. You can also use the terminal within RStudio by clicking on “Terminal” in the toolbar above the console (the console is where the code is run). To look at a text file on the command line, use the head command. You have to tell it where to look on your computer. You can navigate to the folder where the data is stored and just type the name of the file, or you can type a whole filepath, or a relative filepath. Right now, I am in the working directory for this book, and relative to this location, the data is stored in data/MIPSdata. If you are not in this directory, you have to navigate there. The pwd command tells you where you are, the ls -l command tells you what’s there, and the cd command lets you change to a new directory. So if you’re in /home/alex, you would type pwd and it would print /home/alex. If this location contains a folder called data, you can type ls -l and there should be a list of stuff, including a folder called data. You can type cd data to go there, and repeat the process. Navigate to where you downloaded the data to– it’s kind of like a game. A really, really repetative game. Anyways, once you’re there, type: head data/MIPSdata/oldfaithful.txt 216 108 200 137 272 173 282 216 117 261 A column of numbers is printed. This indicates that the text file oldfaithful.txt contains a single column of numbers, and no header row. A header row is what it’s called when the first row of the dataset contains the names of the columns. To read such a dataset into R, we’ll use the read_csv function in the readr package. The readr package is automatically loaded when we load the tidyverse package. If this sentence is confusing (no worries!) I recommend you check out my R tutorial. We’ll just do that: library(tidyverse) oldfaithful &lt;- readr::read_csv( file = &quot;data/MIPSdata/oldfaithful.txt&quot;, # Tell it where the file is col_names = &quot;time&quot;, # Tell it that the first row is NOT column names, and at the same time, tell it what name you want for the column. col_types = &quot;n&quot; # Tell it that there is one column, and it is &quot;numeric&quot; (n) ) # Check what was read in using the dplyr::glimpse() function dplyr::glimpse(oldfaithful) Observations: 272 Variables: 1 $ time &lt;dbl&gt; 216, 108, 200, 137, 272, 173, 282, 216, 117, 261, 110, 235, 252,… By glimpseing the data, we see that the format matches what we saw in the raw file, and we are given the number of rows too. Check that the number of rows matches what was in the raw file by printing out the number of rows on the command line using the wc -l command (“wc” = “word count” and “-l” means count “lines”): wc -l data/MIPSdata/oldfaithful.txt 271 data/MIPSdata/oldfaithful.txt What happened, why don’t they match? They do. The wc -l command actually counts the number of “newline” characters in the file. It is customary to end a data file with a newline character. This file doesn’t, though. How do I know? Type the following: tail data/MIPSdata/oldfaithful.txt 111 255 119 135 285 247 129 265 109 268 This prints out the last few lines of the file. In this book, these are printed as normal. But if you do it on the command line, you’ll see that the command prompt gets printed on the same line as the final number, 268. This indicates that the file does not end with a newline character, and hence the total number of newlines in the file is 271, corresponding to 272 actual lines of data. Remark: subtle issues like this come up all the time in data analysis. The only way to get good at it is practice. It might seem silly to focus on such a seemingly insignificant detail in an introductory chapter on the basics of data analysis. But catching and fixing seemingly insignificant details is a major part of the job and you have to get practice at it. 2.1.2 Graphical Summaries Now that the data has been read in and checked, we can make summaries of it. Histograms # Tabular display print(oldfaithful$time) [1] 216 108 200 137 272 173 282 216 117 261 110 235 252 105 282 130 105 288 [19] 96 255 108 105 207 184 272 216 118 245 231 266 258 268 202 242 230 121 [37] 112 290 110 287 261 113 274 105 272 199 230 126 278 120 288 283 110 290 [55] 104 293 223 100 274 259 134 270 105 288 109 264 250 282 124 282 242 118 [73] 270 240 119 304 121 274 233 216 248 260 246 158 244 296 237 271 130 240 [91] 132 260 112 289 110 258 280 225 112 294 149 262 126 270 243 112 282 107 [109] 291 221 284 138 294 265 102 278 139 276 109 265 157 244 255 118 276 226 [127] 115 270 136 279 112 250 168 260 110 263 113 296 122 224 254 134 272 289 [145] 260 119 278 121 306 108 302 240 144 276 214 240 270 245 108 238 132 249 [163] 120 230 210 275 142 300 116 277 115 125 275 200 250 260 270 145 240 250 [181] 113 275 255 226 122 266 245 110 265 131 288 110 288 246 238 254 210 262 [199] 135 280 126 261 248 112 276 107 262 231 116 270 143 282 112 230 205 254 [217] 144 288 120 249 112 256 105 269 240 247 245 256 235 273 245 145 251 133 [235] 267 113 111 257 237 140 249 141 296 174 275 230 125 262 128 261 132 267 [253] 214 270 249 229 235 267 120 257 286 272 111 255 119 135 285 247 129 265 [271] 109 268 # Ugly! You can use the View() function to open the data in a spreadsheet # Not run: # View(oldfaithful) # Histogram using base R hist(oldfaithful$time) # Base R graphics are outdated. Use ggplot2, which is loaded automatically with the tidyverse: oldfaithful %&gt;% ggplot(aes(x = time)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;blue&quot;,alpha = .3) + labs(title = &quot;Eruption times for Old Faithful geyser&quot;, x = &quot;Eruption time&quot;, y = &quot;Density&quot;) # Try different numbers of bins and you might see different patterns. Do you think this is a # good or bad thing, or both? # Their &quot;optimal&quot; bin width: s &lt;- sd(oldfaithful$time) n &lt;- nrow(oldfaithful) b &lt;- (24*sqrt(pi))^(1/3) * s * (n^(-1/3)) b [1] 36.89694 oldfaithful %&gt;% ggplot(aes(x = time)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = round(b),colour = &quot;black&quot;,fill = &quot;blue&quot;,alpha = .3) + labs(title = &quot;Eruption times for Old Faithful geyser&quot;, subtitle = stringr::str_c(&quot;&#39;Optimal&#39; bin width of b = &quot;,round(b)), x = &quot;Eruption time&quot;, y = &quot;Density&quot;) Kernel Density Estimates To get kernel density estimates, there are a couple different ways. The density function in R does the math for you, using a Gaussian kernel (the functions \\(K_{i}\\) are taken to be Gaussian density functions with mean and standard deviation determined by the data). You can plot the output of density using base R or ggplot. You can also use the ggplot geom_density function to do something similar automatically. # Kernel density estimation in R dens &lt;- density(oldfaithful$time) dens Call: density.default(x = oldfaithful$time) Data: oldfaithful$time (272 obs.); Bandwidth &#39;bw&#39; = 20.09 x y Min. : 35.74 Min. :3.771e-06 1st Qu.:118.37 1st Qu.:8.571e-04 Median :201.00 Median :2.412e-03 Mean :201.00 Mean :3.022e-03 3rd Qu.:283.63 3rd Qu.:5.143e-03 Max. :366.26 Max. :8.070e-03 plot(dens) # Okay... ggplot? tibble(x = dens$x,y = dens$y) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_line() + labs(title = &quot;Kernel density estimate, Old Faithful data&quot;, subtitle = &quot;Manually-calculated values&quot;, x = &quot;Eruption time&quot;, y = &quot;Density&quot;) # Can also do automatically: oldfaithful %&gt;% ggplot(aes(x = time)) + theme_classic() + geom_density() + labs(title = &quot;Kernel density estimate, Old Faithful data&quot;, subtitle = &quot;Automatically-calculated values&quot;, x = &quot;Eruption time&quot;, y = &quot;Density&quot;) # The reason to manually calculate the values is because you have more control. # I don&#39;t know what&#39;s happening at the endpoints there, and it&#39;s too much work # to go and figure out how to make ggplot not do that. # When you calculate the plotting values yourself and then put them into ggplot, # you have total control! Empirical Distribution Function For the empirical distribution function, we use the ecdf function in R. You would think this function should behave in a similar manner to the density function, but it doesn’t. It returns a function which computes the ecdf. It still has a plot method, but to use it with ggplot we have to use stat_function: faithful_ecdf &lt;- ecdf(oldfaithful$time) plot(faithful_ecdf) # ggplot tibble(x = c(100,300)) %&gt;% # Tell ggplot we want to plot the ecdf from 100 to 300 ggplot(aes(x = x)) + theme_classic() + stat_function(fun = faithful_ecdf) + labs(title = &quot;Empirical CDF for Old Faithful Eruption Times&quot;, x = &quot;Eruption Time&quot;, y = &quot;Empirical probability that an eruption time is less than x&quot;) Advanced: the CDF is the integrated pdf: \\[ F(x) = \\int_{-\\infty}^{x}f(s)ds \\] So why don’t we integrate the kernel density estimate to get the empirical CDF? One of the great benefits of taking a computation-forward approach to statistical inference is that we can “shoot first and ask questions later”- just try it, and then (maybe) use math to explain the results. Here is the world’s most naive numerical integration-based estimate of a CDF: tibble( x = dens$x[-1], y = cumsum(dens$y[-1]) * diff(dens$x) # Quick and dirty numerical integration. Can you put something better? ) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_line() + labs(title = &quot;Numerical integration-based empirical CDF for Old Faithful data&quot;, x = &quot;Eruption time&quot;, y = &quot;Empirical probability that an eruption time is less than x&quot;) What do you think? Is this better, worse, or just different than the ECDF? Boxplots To get a boxplot, again, you can use base R or ggplot. We have: # Base R boxplot(oldfaithful$time) # ggplot oldfaithful %&gt;% ggplot(aes(y = time)) + theme_classic() + geom_boxplot(width = .1) + labs(title = &quot;Boxplot of eruption times, Old Faithful data&quot;, y = &quot;Eruption time&quot;) + # Have to play around with the x axis to get it to look nice *shrug* coord_cartesian(xlim = c(-.2,.2)) + theme(axis.text.x = element_blank()) 2.1.3 Numerical Summaries A numerical summary is any number or numbers you calculate from the data. The basic numerical summaries of interest include the sample mean and median, sample standard deviation and mean absolute deviation, and quantiles. # Mean mean(oldfaithful$time) [1] 209.2684 # Median median(oldfaithful$time) [1] 240 # Standard deviation sd(oldfaithful$time) [1] 68.48329 # Quantiles: tell it which ones you want. I want 0, 25, 50, 75, 100 quantile(oldfaithful$time,probs = c(0,.25,.50,.75,1.00)) 0% 25% 50% 75% 100% 96.00 129.75 240.00 267.25 306.00 # The zeroth and hundredth quantiles are the sample minimum and maximum: min(oldfaithful$time) [1] 96 max(oldfaithful$time) [1] 306 # Actually, you can get all this with the summary() function: summary(oldfaithful$time) Min. 1st Qu. Median Mean 3rd Qu. Max. 96.0 129.8 240.0 209.3 267.2 306.0 # Mean absolute deviation # I don&#39;t know an R function for this off the top of my head (maybe you can find one?) # So let&#39;s calculate it manually. # Actually, let&#39;s calculate them ALL manually! # Mean: sum(oldfaithful$time) / length(oldfaithful$time) [1] 209.2684 # Median: it&#39;s the 50th percentile quantile(oldfaithful$time,probs = .5) 50% 240 # Can get it manually too: sort(oldfaithful$time)[length(oldfaithful$time)/2] [1] 240 # Better to use the quantile() function though. # Standard deviation. Need to save the mean to a variable first: mn &lt;- mean(oldfaithful$time) sqrt( sum( (oldfaithful$time - mn)^2 ) / ( length(oldfaithful$time) - 1 ) ) [1] 68.48329 # MAD. Similar to sd: md &lt;- median(oldfaithful$time) median( abs(oldfaithful$time - md) ) [1] 38.5 # IQR: quantile(oldfaithful$time,probs = .75) - quantile(oldfaithful$time,probs = .25) 75% 137.5 # Note that there are various ways to correct for the fact that not all quantiles # are exact (you may not have a datapoint which has EXACTLY 25% of the data below it, # like if the sample size isn&#39;t divisible by 4). R probably uses a different method # than the book, so the results here are slightly different. 2.2 Drilling The drilling data containg more than one variable and so are ideal for illustrating summaries of data containing more than one variable. 2.2.1 Read in data Read in the drilling.txt file: head data/MIPSdata/drilling.txt 5 640.67 830 10 674.67 800 15 708 711.33 20 735.67 867.67 25 754.33 940.67 30 723.33 941.33 35 664.33 924.33 40 727.67 873 45 658.67 874.67 50 658 843.33 # Use the read_tsv (not csv), because this file is &quot;tab-delimited&quot;; the spaces # between columns contain tab characters. drilling &lt;- readr::read_tsv( file = &quot;data/MIPSdata/drilling.txt&quot;, col_names = c(&quot;depth&quot;,&quot;dry&quot;,&quot;wet&quot;), col_types = &quot;nnn&quot; ) glimpse(drilling) Observations: 80 Variables: 3 $ depth &lt;dbl&gt; 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, … $ dry &lt;dbl&gt; 640.67, 674.67, 708.00, 735.67, 754.33, 723.33, 664.33, 727.67,… $ wet &lt;dbl&gt; 830.00, 800.00, 711.33, 867.67, 940.67, 941.33, 924.33, 873.00,… 2.2.2 Graphical summaries Scatterplots To make a scatterplot, you can again use base R or ggplot. We want separate plots for dry and wet holes. You can do this by plotting these data separately, or you can re-format the data and have ggplot do it automatically: # Base R par(mfrow = c(1,2)) # Plots on a 1 x 2 grid plot(dry~depth,data = drilling) plot(wet~depth,data = drilling) # ggplot # Two separate plots: dryplt &lt;- drilling %&gt;% ggplot(aes(x = depth,y = dry)) + theme_classic() + geom_point(pch = 21) + # pch=21 is the magic command to give you hollow points labs(title = &quot;Dry Holes&quot;, x = &quot;Depth&quot;, y = &quot;Mean drill time&quot;) + scale_y_continuous(breaks = seq(500,1300,by = 200)) + coord_cartesian(ylim = c(500,1300)) wetplt &lt;- drilling %&gt;% ggplot(aes(x = depth,y = wet)) + theme_classic() + geom_point(pch = 21) + # pch=21 is the magic command to give you hollow points labs(title = &quot;Wet Holes&quot;, x = &quot;Depth&quot;, y = &quot;Mean drill time&quot;) + scale_y_continuous(breaks = seq(500,1300,by = 200)) + coord_cartesian(ylim = c(500,1300)) cowplot::plot_grid(dryplt,wetplt,nrow = 1) # There is a lot of repeated code here. For a better way to make these two # plots, first create a base plot object and then reuse it: drillingplt &lt;- drilling %&gt;% ggplot(aes(x = depth)) + theme_classic() + labs(x = &quot;Depth&quot;, y = &quot;Mean drill time&quot;) + scale_y_continuous(breaks = seq(500,1300,by = 200)) + coord_cartesian(ylim = c(500,1300)) dryplt2 &lt;- drillingplt + labs(title = &quot;Dry holes&quot;) + geom_point(aes(y = dry),pch = 21) wetplt2 &lt;- drillingplt + labs(title = &quot;Wet holes&quot;) + geom_point(aes(y = wet),pch = 21) cowplot::plot_grid(dryplt2,wetplt2,nrow = 1) # Much better # Another option is to reformat the data and create the plot # with a single command. To do this we stack the dry and wet measurements # on top of each other, and create a new variable which indicates whether the # measurements are dry or wet. This is called putting the data into &quot;long&quot; format. # ggplot then knows how to &quot;facet&quot; the plots according to this new variable. # Check it out: drilling_long &lt;- drilling %&gt;% tidyr::pivot_longer( dry:wet, # Gather up the columns &quot;dry&quot; and &quot;wet&quot; names_to = &quot;type&quot;, # Create a new column called &quot;type&quot; which says whether a hole is dry or wet values_to = &quot;time&quot; # Create a new column called &quot;time&quot; with the drilling time for each hole ) %&gt;% dplyr::mutate(type = case_when( # Rename the type values for plotting type == &quot;dry&quot; ~ &quot;Dry Holes&quot;, type == &quot;wet&quot; ~ &quot;Wet Holes&quot; )) dplyr::glimpse(drilling_long) Observations: 160 Variables: 3 $ depth &lt;dbl&gt; 5, 5, 10, 10, 15, 15, 20, 20, 25, 25, 30, 30, 35, 35, 40, 40, 4… $ type &lt;chr&gt; &quot;Dry Holes&quot;, &quot;Wet Holes&quot;, &quot;Dry Holes&quot;, &quot;Wet Holes&quot;, &quot;Dry Holes&quot;… $ time &lt;dbl&gt; 640.67, 830.00, 674.67, 800.00, 708.00, 711.33, 735.67, 867.67,… drilling_long %&gt;% ggplot(aes(x = depth,y = time)) + theme_classic() + facet_wrap(~type) + geom_point() + labs(title = &quot;Mean drilling times&quot;, x = &quot;Depth&quot;, y = &quot;Mean drilling time&quot;) # Even though there is more overhead with this method, I recommend it because # it scales to more variables. If you wanted to make 20 plots, you&#39;d have to # have 20 plots in the previous method but here, the code is actually identical. To make boxplots, transform the data in the same way as for the side-by-side scatterplots, and give to ggplot: drilling_long %&gt;% ggplot(aes(x = type,y = time)) + theme_classic() + geom_boxplot() + labs(title = &quot;Boxplots for wet and dry drilling times, Drilling data&quot;, x = &quot;Hole Type&quot;, y = &quot;Mean drilling time&quot;) 2.2.3 Numerical summaries We can compute numerical summaries of drilling times in the same way as for the eruption times from the Old Faithful data. However, the drilling data are naturally grouped, so we should compute our summaries by group— i.e. by hole type, wet or dry. To do this requires a bit more machinery; we will operate on a dataframe and use formal grouping operations. We need the data in long format for this, which we have already done. The following is a bit more work but is the basis of a really powerful strategy for summarizing complex datasets. Check it out: drilling_long %&gt;% group_by(type) %&gt;% # Everything that happens now happens separately for Wet Holes and Dry Holes summarize( mean = mean(time), sd = sd(time), min = min(time), median = median(time), quant_25 = quantile(time,probs = .25), quant_75 = quantile(time,probs = .75), max = max(time) ) # A tibble: 2 x 8 type mean sd min median quant_25 quant_75 max &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Dry Holes 806. 154. 584 758. 688. 912. 1238 2 Wet Holes 944. 124. 697. 918. 851. 1030. 1238. Grouped operations are fundamental to modern data analysis. Imagine if instead of 2 groups you had 100 and your boss asked you to make a report. Or better yet: imagine you made a report on 2 groups and it took you a day, and your boss asked you to make one for 100 groups and gave you a week to do it. You could give it to them 2 days early and still have 2 days off ;). 2.3 Exercises The Janka Hardness data is in the file jankahardness.txt. Read it in and compute appropriate summaries: five-number summaries, boxplots, scatterplots, histograms, kernel density estimates and empirical CDFs. You may also choose to read the analysis in the MIPS book and reproduce it. 2.4 Extended example: smoking and age and mortality You now have some tools. How do you put them to use in practice? Analyzing data “in the wild” involves a lot of decision making, and this can impact the conclusions you make. Consider a famous dataset containing information on smoking and mortality. The data is available in the R package faraway. We may load the package and data and retrieve information on it as follows: # install.packages(&quot;faraway&quot;) # Run this to install the faraway package, which has useful datasets library(faraway) # Attach the faraway package data(&quot;femsmoke&quot;) # Load the &quot;femsmoke&quot; data # ?femsmoke # Run this to open the help page for the dataset. We see from the help page, and associated reference to the paper in the American Statistician, that the data comes from asking women in Whickham, England, whether they smoke or not, and then following up in 20 years to see if they died. Let’s perform a descriptive analysis of these data. We need some more quantitative pieces of descriptive information. What might we want to know about our data? Some ideas: How many observations are there in the data, and what does an observation represent in the context of how the data was collected? How many variables are present in the data, and what does each variable represent in the context of how the data was collected? How might we summarize each variable? We might compute a mean and a five-number summary for “continuous” variables, and a table of counts for “categorical” variables (more on this later…). Let’s see how we can obtain these descriptive measures in R: # Get the number of observations (rows), variables, and an idea # of what the data looks like: glimpse(femsmoke) Observations: 28 Variables: 4 $ y &lt;dbl&gt; 2, 1, 3, 5, 14, 7, 27, 12, 51, 40, 29, 101, 13, 64, 53, 61, 12… $ smoker &lt;fct&gt; yes, no, yes, no, yes, no, yes, no, yes, no, yes, no, yes, no,… $ dead &lt;fct&gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, ye… $ age &lt;fct&gt; 18-24, 18-24, 25-34, 25-34, 35-44, 35-44, 45-54, 45-54, 55-64,… # One observation represents a count of people in each category. # How many people? femsmoke %&gt;% summarize(num_people = sum(y)) # The summarize() function lets you compute summaries of variables in your dataframe num_people 1 1314 # How many smokers? femsmoke %&gt;% filter(smoker == &quot;yes&quot;) %&gt;% # filter() lets you choose which rows to keep summarize(num_smokers = sum(y)) num_smokers 1 582 # How many non-smokers? femsmoke %&gt;% filter(smoker == &quot;no&quot;) %&gt;% summarize(num_non_smokers = sum(y)) num_non_smokers 1 732 # We can get both those numbers at the same time: femsmoke %&gt;% group_by(smoker) %&gt;% # group_by() makes summarize() compute summaries within levels of a variable summarize(num_people = sum(y)) # A tibble: 2 x 2 smoker num_people &lt;fct&gt; &lt;dbl&gt; 1 yes 582 2 no 732 Do the following exercises to get practice with descriptive statistics: 2.4.1 Exercises How many non-smoking 18-24 year olds are there in the femsmoke data? Answer using filter(). How many smokers died? Answer using filter(). How many 45-55 year olds did not die? Compute the following table using group_by() and summarize(): # A tibble: 7 x 2 age num_people &lt;fct&gt; &lt;dbl&gt; 1 18-24 117 2 25-34 281 3 35-44 230 4 45-54 208 5 55-64 236 6 65-74 165 7 75+ 77 2.4.2 Association between smoking and mortality Let’s go into a bit more detail. I want to see if there is any apparent association between smoking and mortality. # Compute the mortality rate for smokers and non-smokers. # To do this, create a dataframe containing the numbers of smokers # and non-smokers smoker_numbers &lt;- femsmoke %&gt;% # The %&gt;% operator lets you form sequences of operations group_by(smoker) %&gt;% # group_by() makes all the following operations happen within groups summarize(num_people = sum(y)) # Count the number of people who are smokers and not smokers smoker_numbers # A tibble: 2 x 2 smoker num_people &lt;fct&gt; &lt;dbl&gt; 1 yes 582 2 no 732 # Now, compute the number of people who died out of the smokers and non-smokers # This looks the same as above, except we now filter() only the people who died. smoker_numbers_dead &lt;- femsmoke %&gt;% filter(dead == &quot;yes&quot;) %&gt;% # Retains rows where dead == &quot;yes&quot; only group_by(smoker) %&gt;% summarize(num_dead = sum(y)) smoker_numbers_dead # A tibble: 2 x 2 smoker num_dead &lt;fct&gt; &lt;dbl&gt; 1 yes 139 2 no 230 # Now, we join these two tables together and compute the mortality rates by group. smoker_numbers %&gt;% inner_join(smoker_numbers_dead,by = &quot;smoker&quot;) %&gt;% # Joins rows with the same value of &quot;smoker&quot; mutate(mort_rate = num_dead/num_people) # mutate() creates a new variable, which can be a function of the other variables in the dataframe. # A tibble: 2 x 4 smoker num_people num_dead mort_rate &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 yes 582 139 0.239 2 no 732 230 0.314 See anything interesting? What went wrong? Why are we observing that smokers have a lower mortality rate than non-smokers? This contradicts the context surrounding this analysis, which in this case is the large body of formal and anecdotal evidence suggesting that smoking is harmful to health. Did we make a mistake? One thing we definitely did was ignore some present information. Specifically, we also know how old the women were. How can we include this information in our exploratory analysis? We can compute mortality rates by age: smoker_numbers_age &lt;- femsmoke %&gt;% group_by(smoker,age) %&gt;% # Now we&#39;re grouping by smoker AND age. The rest of the code remains unchanged. summarize(num_people = sum(y)) smoker_numbers_age_dead &lt;- femsmoke %&gt;% filter(dead == &quot;yes&quot;) %&gt;% group_by(smoker,age) %&gt;% summarize(num_dead = sum(y)) smoker_numbers_age %&gt;% inner_join(smoker_numbers_age_dead,by = c(&quot;smoker&quot;,&quot;age&quot;)) %&gt;% mutate(mort_rate = num_dead/num_people) # A tibble: 14 x 5 # Groups: smoker [2] smoker age num_people num_dead mort_rate &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 yes 18-24 55 2 0.0364 2 yes 25-34 124 3 0.0242 3 yes 35-44 109 14 0.128 4 yes 45-54 130 27 0.208 5 yes 55-64 115 51 0.443 6 yes 65-74 36 29 0.806 7 yes 75+ 13 13 1 8 no 18-24 62 1 0.0161 9 no 25-34 157 5 0.0318 10 no 35-44 121 7 0.0579 11 no 45-54 78 12 0.154 12 no 55-64 121 40 0.331 13 no 65-74 129 101 0.783 14 no 75+ 64 64 1 Older people are more likely to die within the 20 year followup period. However, examining the raw counts of people in each group, we also see that in these data, older people are less likely to smoke than younger people. So in these data, less smokers died, because less smokers were old, and more old people died. Before moving on, get some practice doing exploratory analysis with the following exercises: 2.4.3 Exercises What is the relative risk of mortality—the ratio of the mortality rates—for smoking 18-24 year olds vs non-smoking 18-24 year olds? Compute the answer manually by reading the numbers off the above table. Then compute it using R by doing the following: Create two datasets using filter(): one containing smokers and one containing non-smokers. filter() out only the 18-24 year olds. This gives you two datasets each with only one row. For example, smokers &lt;- femsmoke %&gt;% filter(smoker == \"yes\",age = \"18-24\"). inner_join() the two datasets together, using age as the by variable: smokers %&gt;% inner_join(???,by = \"age\") Advanced: modify the above steps to create the following table of relative mortality rates. You should start from a cleaned up version of the mortality rate by age table: rates_by_age &lt;- smoker_numbers_age %&gt;% inner_join(smoker_numbers_age_dead,by = c(&quot;smoker&quot;,&quot;age&quot;)) %&gt;% mutate(mort_rate = num_dead/num_people) %&gt;% ungroup() # The data was previously grouped, we don&#39;t want this anymore Use dplyr::select() to remove and rename columns, see ?dplyr::select. You should get the following: # A tibble: 7 x 4 age smoker_mort_rate nonsmoker_mort_rate relative_risk &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 18-24 0.0364 0.0161 2.25 2 25-34 0.0242 0.0318 0.760 3 35-44 0.128 0.0579 2.22 4 45-54 0.208 0.154 1.35 5 55-64 0.443 0.331 1.34 6 65-74 0.806 0.783 1.03 7 75+ 1 1 1 2.5 Case study: rental housing in Toronto The RentSafeTO: Apartment Building Standards program is designed to help renters in the city of Toronto make informed choices about where to live, and to enforce a minimum standard of quality upon rental units within the city. With rents skyrocketing and home ownership not a reasonable option for most, having an informed view of the rental market is imperative for Toronto residents. It also helps keep leaders accountable, specifically if we focus on social and community housing buildings. Comprehensive and fairly clean data from the program, along with specific information, is available at https://open.toronto.ca/dataset/apartment-building-evaluation/. Data for the following were downloaded on 2019/09/16. To start your analysis, go now and download the data and open it in a spreadsheet and have a look. Familiarize yourselves with the variable descriptions and how the data were collected; the documentation. This somewhat tedious task is a first step of any data analysis, in academia, industry, government, or wherever. 2.5.1 Load the data The data are stored in a .csv file, which stands for “comma-separated-values”. Storing data in a text file with a separator, usually a comma, is very common. These are referred to as “flat files” in an industrial context, to distinguish them from data stored in databases. We may read the data into R using the read_csv function in the readr package. The readr package is part of the tidyverse package that we used before, so if you installed that package, you have it loaded. # https://open.toronto.ca/dataset/apartment-building-evaluation/ # install.packages(&quot;readr&quot;) # Read the data in. This means call the readr::read_csv() function, point it # to where you saved the data on your computer, and then save the result to a # variable. I am naming this variable &#39;apartmentdata&#39;. # Type ?readr::read_csv if you want to read about this function. apartmentdata &lt;- readr::read_csv( file = &quot;data/apartment-data/toronto-apartment-building-evaluations.csv&quot; ) Parsed with column specification: cols( .default = col_double(), EVALUATION_COMPLETED_ON = col_character(), PROPERTY_TYPE = col_character(), RESULTS_OF_SCORE = col_character(), SITE_ADDRESS = col_character(), WARD = col_character() ) See spec(...) for full column specifications. The message displayed is telling you that readr::read_csv() guessed at what kind of data were in each column, i.e. numbers, letters, dates, etc. You should make sure, as I have while writing, that these are what you expect. You can get a concise view of this dataset using the glimpse function in the dplyr package, which is automatically loaded when you load the tidyverse: glimpse(apartmentdata) Observations: 3,446 Variables: 32 $ `_id` &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13… $ BALCONY_GUARDS &lt;dbl&gt; NA, NA, NA, NA, 5, NA, 5, 3, 4, 4, 3, NA,… $ CONFIRMED_STOREYS &lt;dbl&gt; 28, 4, 3, 3, 29, 3, 7, 18, 17, 32, 4, 43,… $ CONFIRMED_UNITS &lt;dbl&gt; 457, 15, 26, 10, 272, 12, 95, 287, 327, 5… $ ELEVATORS &lt;dbl&gt; 4, NA, NA, NA, 5, NA, 5, 4, 5, 4, NA, 4, … $ ENTRANCE_DOORS_WINDOWS &lt;dbl&gt; 3, 3, 3, 4, 5, 4, 4, 4, 3, 4, 4, 3, 2, 4,… $ ENTRANCE_LOBBY &lt;dbl&gt; 4, 3, 3, 4, 5, 4, 4, 4, 4, 4, 4, 4, 3, 4,… $ EVALUATION_COMPLETED_ON &lt;chr&gt; &quot;04/03/2019&quot;, &quot;05/24/2018&quot;, &quot;07/11/2018&quot;,… $ EXTERIOR_CLADDING &lt;dbl&gt; 3, 4, 4, 4, 5, 4, 5, 4, 4, 3, 3, 4, 3, 4,… $ EXTERIOR_GROUNDS &lt;dbl&gt; 3, 4, 3, 3, 5, 4, 5, 4, 3, 4, 3, 4, 2, 4,… $ EXTERIOR_WALKWAYS &lt;dbl&gt; 3, 5, 4, 4, 5, 4, 5, 4, 3, 4, 4, 3, 3, 4,… $ GARBAGE_BIN_STORAGE_AREA &lt;dbl&gt; 3, 4, 3, 3, 4, 3, 3, 3, 4, 4, 4, 4, 3, 2,… $ GARBAGE_CHUTE_ROOMS &lt;dbl&gt; 3, NA, NA, NA, 5, NA, 5, 4, 3, 4, 5, 4, N… $ GRAFFITI &lt;dbl&gt; 5, 5, 5, 5, 5, 4, 5, 4, 3, 4, 5, 5, 4, 5,… $ INTERIOR_LIGHTING_LEVELS &lt;dbl&gt; 3, 4, 4, 4, 5, 4, 4, 3, 3, 4, 3, 4, 3, 4,… $ INTERIOR_WALL_CEILING_FLOOR &lt;dbl&gt; 4, 3, 4, 4, 5, 4, 4, 3, 4, 4, 4, 3, 3, 4,… $ INTERNAL_GUARDS_HANDRAILS &lt;dbl&gt; 3, 4, 3, 4, 5, 4, 5, 4, 4, 4, 5, 4, 3, 2,… $ NO_OF_AREAS_EVALUATED &lt;dbl&gt; 18, 14, 14, 13, 19, 16, 17, 18, 19, 19, 1… $ OTHER_FACILITIES &lt;dbl&gt; 4, NA, NA, NA, 5, NA, NA, NA, NA, NA, NA,… $ PARKING_AREA &lt;dbl&gt; 2, NA, NA, NA, 4, 3, 5, 2, 4, 4, 2, 3, 2,… $ PROPERTY_TYPE &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SOCIAL … $ RESULTS_OF_SCORE &lt;chr&gt; &quot;Evaluation needs to be conducted in 2 ye… $ RSN &lt;dbl&gt; 4365723, 4364249, 4408585, 4288126, 42882… $ SCORE &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, 78, 7… $ SECURITY &lt;dbl&gt; 4, 3, 3, 4, 5, 4, 5, 3, 4, 4, 4, 4, 3, 4,… $ SITE_ADDRESS &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;… $ STAIRWELLS &lt;dbl&gt; 4, 4, 3, 4, 5, 4, 5, 4, 4, 4, 3, 4, 3, 3,… $ STORAGE_AREAS_LOCKERS &lt;dbl&gt; NA, NA, NA, NA, NA, 4, NA, NA, 3, 4, 4, 4… $ WARD &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;,… $ WATER_PEN_EXT_BLDG_ELEMENTS &lt;dbl&gt; 4, 4, 4, 4, 5, 4, 5, 4, 5, 3, 3, 4, 3, 4,… $ YEAR_BUILT &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, 2015,… $ YEAR_REGISTERED &lt;dbl&gt; 2018, 2018, 2018, 2017, 2018, 2017, 2017,… That’s bigger than the smoking data! 3,446 rental apartment buildings, each with 32 factors measured. The buliding’s address and Ward number are in there, which are helpful for characterizing neighbourhoods. 2.5.2 Analysis I: what does the data look like? As a first step, we want to get an idea of what our data “looks like”. This typically means picking some interesting variables and summarizing their distributions somehow. Which variables to pick will depend on the context. Often it will be clear which variables are important, and sometimes not. Because you read the documentation and familiarized yourselves with the variables in the dataset, you know that there is a variable called SCORE which sums up the individual category scores for each building. In the context of determining building quality, this seems like an important variable to look at. We’ll summarize the distribution of SCORE using a five-number summary and mean, and a histogram with a kernel density estimate. First, prepare the data for analysis: # First, select only the columns you want # This isn&#39;t strictly necessary but trust me, it makes # debugging WAY easier. # I&#39;m also renaming the columns so the dataframe looks prettier. # Again, trust me. This stuff matters. apartmentclean &lt;- apartmentdata %&gt;% filter(!is.na(SCORE)) %&gt;% # Remove apartments with missing scores dplyr::select(ward = WARD, score = SCORE, property_type = PROPERTY_TYPE, year_built = YEAR_BUILT, address = SITE_ADDRESS ) glimpse(apartmentclean) # Much nicer! Observations: 3,437 Variables: 5 $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;08&quot;, &quot;… $ score &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, 78, 73, 76, 57, 70,… $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SOCIAL HOUSING&quot;, &quot;PRI… $ year_built &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, 2015, 1970, 1976, 1… $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACPHERSO… To compute the five-number summary (plus mean), use the summary() function in R. I also want to know the standard deviation of SCORE: summary(apartmentclean$score) Min. 1st Qu. Median Mean 3rd Qu. Max. 37.00 68.00 72.00 72.28 77.00 99.00 sd(apartmentclean$score,na.rm = TRUE) [1] 7.117172 The worst building in the city has a total score of 37, and the best gets 99. The median score—half the buildings in the city have a lower score, and half a higher score than this—is 72, and this roughly equals the mean of 72.28. 25% of buildings score higher than 77, and 25% score lower than 68. So most buildings seem to fall within less than one standard deviation of the mean, which indicates that these data are fairly concentrated about their mean. To provide some context, go look up your own building (if you live in a rental building) or that of a friend in the data. Where does your building fall in terms of quality within Toronto? So far we have used tabular displays to summarize our data, for both the smoking and the apartment data. We also learned about graphical displays. Let’s see a histogram of the scores, with a kernel density estimate: We can make a histogram in R as follows: # The ggplot2 package is loaded as part of the tidyverse score_histogram &lt;- apartmentclean %&gt;% ggplot(aes(x = score)) + # Tell ggplot to use score on the x axis theme_classic() + # Make the plot pretty geom_histogram( # Makes a histogram aes(y = ..density..), bins = 20, colour = &quot;black&quot;, fill = &quot;lightgrey&quot; ) + geom_density() + labs(title = &quot;Distribution of RentSafeTO Apartment Building Standards score&quot;, x = &quot;Score&quot;, y = &quot;Density&quot;) + scale_x_continuous(breaks = seq(30,100,by = 5)) score_histogram It appears that most buildings are in the 65 to 85 range. I actually just moved from a building that has a 66 to a building that has an 86. The difference is substantial! 2.5.3 Analysis II: Do different wards have different quality housing? A Ward is an administrative district within the city that has a single city counsellor. If I’m thinking about moving to, or within, Toronto, I want to know: Do different wards have different quality housing?. In order to address this question we need to decide on the following: Variable of interest. How do we quantify our research question? We need to pick a measure of quality. Picking different measures can lead to different conclusions. Filters. Do we look at all apartment buildings? Should we look only at those built after, or before, a certain date? Only those that meet a certain minimum, or maximum, standard of quality according to our definition? Are there any other kinds of decisions we might have to consider? Methods. What kind of statistical tools should we use to address our research question? We need to pick descriptive statistics to report, and decide whether we want to include other auxillary variables in the analysis. Conclusions. How do we report our results? Tables, charts, maps? Should we include subjective, editorial commentary, or let the data speak for themselves? This is already overwhelming! Let’s make an attempt at it. I propose: Our variable of interest should be SCORE, which you know (because you read the documentation…) is the “overall score of the buliding”. Higher is better. The actual formula is included in the documentation of the data. We will filter the data to only include buildings where PROPERTY_TYPE == 'PRIVATE', which will restrict our analysis to not include social housing. The quality of social housing is an important social justice issue (that you will investigate in the exercises) but it’s somewhat separate (?) from the question of where to look for rental housing. Our methods will include looking at a table of average scores for each ward. We will also look at whether older or newer buildings receive better scores. We will summarize our conclusions through a subjective assessment of the above table of average scores. With these decisions made, we may proceed with our analysis using the tidyverse as follows: # Apply filter(s). apartmentfiltered &lt;- apartmentclean %&gt;% filter(property_type == &quot;PRIVATE&quot;) # When filtering, always compare the filtered and unfiltered data to ensure # the result is as expected: glimpse(apartmentclean) Observations: 3,437 Variables: 5 $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;08&quot;, &quot;… $ score &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, 78, 73, 76, 57, 70,… $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SOCIAL HOUSING&quot;, &quot;PRI… $ year_built &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, 2015, 1970, 1976, 1… $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACPHERSO… glimpse(apartmentfiltered) Observations: 2,873 Variables: 5 $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;08&quot;, &quot;13&quot;, &quot;… $ score &lt;dbl&gt; 71, 77, 71, 98, 76, 93, 72, 74, 78, 73, 76, 57, 70, 57,… $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, … $ year_built &lt;dbl&gt; 1976, 1953, 1948, 2017, 1967, 2015, 1970, 1976, 1968, 1… $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACPHERSO… nrow(apartmentclean) - nrow(apartmentfiltered) # Dropped 567 rows. [1] 564 # Now create the table of averages: apartmentfiltered %&gt;% group_by(ward) %&gt;% # na.rm = TRUE: ignore missing values summarize(avg_score = mean(score,na.rm = TRUE)) # A tibble: 26 x 2 ward avg_score &lt;chr&gt; &lt;dbl&gt; 1 01 71.5 2 02 73.0 3 03 70.5 4 04 68.2 5 05 71.7 6 06 72.1 7 07 69.8 8 08 73.5 9 09 67.5 10 10 72.2 # … with 16 more rows This isn’t a super friendly way of comparing these 26 numbers. I’d rather use a graphical display, like the boxplots we learned about in chapters 15 and 16: apartmentfiltered %&gt;% ggplot(aes(x = ward,y = score)) + theme_classic() + geom_boxplot() + labs(title = &quot;ABS score shows moderate variability across wards in Toronto&quot;, x = &quot;Ward&quot;, y = &quot;ABS Score&quot;) It looks like some wards are better than others. Or are they? Can we make any definitive conclusions based on this? 2.5.4 Analysis III: trends in quality over time Let’s go further and analyze some other interesting aspects of these data. I’m interested in knowing: Are newer buildings higher quality? We have the score and the year_built, and we’d like to investigate whether newer buildings (higher year_built) have higher scores. We have another decision to make. We could consider year_built to be a categorical variable, and make a bar chart. Or, we could consider it to be a continuous variable. Because values of year_built are inherently comparable, and because our research question involves making such comparisons, we will consider year_built to be a continuous variable. One type of plot used to compare continuous variables is a scatterplot. A scatterplot has continuous variables on the x- and y-axes, and draws a point (or bubble) at each place in the two-dimensional plane where a datapoint occurs. We can make this kind of plot in ggplot2 as well. This time, we use the raw (well, cleaned and filtered) data: apartmentfiltered %&gt;% filter(year_built &gt; 1900) %&gt;% ggplot(aes(x = year_built,y = score)) + theme_classic() + geom_point(pch = 21,colour = &quot;black&quot;,fill = &quot;grey&quot;) + # pch=21 makes the bubbles hollow, looks nice scale_x_continuous(breaks = seq(1900,2020,by=10)) + # Set the x-axis range labs(title = &quot;Less rental buildings are being built recently, but they are of higher quality&quot;, x = &quot;Year Built&quot;, y = &quot;ABS Score&quot;) Very interesting. You can clearly see the baby boom of the 1950’s to 1970’s, followed by a massive slowdown in construction during the economic slump in the 1980’s, and a complete stop when rent control was introduced in 1991 (remember, these are rental buildings only). Then, we see a new wave of rental building construction, and the new buildings seem to be of higher quality. What are the highest and lowest quality rental buildings in Toronto? # Get the 10 highest scoring buildings apartmentfiltered %&gt;% arrange(desc(score)) %&gt;% # Sort the data, descending, by score slice(1:10) # Take the first ten- i.e. the top ten # A tibble: 10 x 5 ward score property_type year_built address &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 YY 99 PRIVATE 2018 561 SHERBOURNE ST 2 17 99 PRIVATE 2017 123 PARKWAY FOREST DR 3 16 99 PRIVATE 1963 70 PARKWOODS VILLAGE DR 4 07 98 PRIVATE 2017 2 VENA WAY 5 17 97 PRIVATE 1968 24 FOREST MANOR RD 6 12 96 PRIVATE 1960 42 GLEN ELM AVE 7 13 95 PRIVATE 2017 252 VICTORIA ST 8 02 95 PRIVATE 1969 500 SCARLETT RD 9 16 95 PRIVATE 1962 67 PARKWOODS VILLAGE DR 10 07 95 PRIVATE 2016 6 VENA WAY Wow. I know where I want to live. 2.5.5 Summary We have seen how even something simple like trying to figure out whether different areas of the city have different quality housing can require a lot of decision making. And these decisions require expertise. By taking a principled approach to learning data analysis, you are empowering yourself to live a life that is better informed. But notice that we didn’t really answer any questions in this chapter. We saw some rough patterns, but were they real? If we made different decisions, or if we sampled different data, would we have seen different patterns? Quantifying uncertainty in a data analysis like this is what the field of Statistics is about. 2.5.6 Exercises What is that “YY” ward that shows up in the dot plot? Investigate this unusual observation. Read the documentation online and choose three variables that you find the most interesting. Reproduce the analyses I, II and III using your variables. Is there more or less variability across wards than with score? What is the ward with the highest average score? In what ward is/are the building(s) with the highest score(s)? Is this the same ward, or not? Would you expect the ward with the highest average to also have the highest-scoring buildings? Repeat this question with the lowest scoring buildings instead of the highest. If you live in a rental apartment, find it in these data. If not, find a friend’s place. How does your building compare to other buildings in your ward? Does it score higher or lower? The filter() function is your friend here, or you can use apartmentfiltered %&gt;% arrange(SITE_ADDRESS) %&gt;% print(n = Inf) and then find yours in the list manually. Combine the analyses of sections 2.3.2 and 2.3.3. with that of 2.3.4. Specifically, make a table and a boxplot of the average score by year. This means replace ward by year_built in the analysis of sections 2.3.2. and 2.3.3. Do your conclusions change when comparing with 2.3.4? Why or why not? Would you expect this to always be the case? Advanced: analyze the quality of social housing in Toronto. Perform a similar analysis to what we performed here for PROPERTY_TYPE == 'PRIVATE', but instead for PROPERTY_TYPE %in% c('SOCIAL HOUSING','TCHC') (equivalent to PROPERTY_TYPE != 'PRIVATE'). Does the quality of social housing in Toronto vary greatly across different wards? Is it improving or degrading over time? Do you think we have enough information here to definitively answer these questions? "],["section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html", "Chapter 3 Introduction to Statistics: Law of Large Numbers and Central Limit Theorem 3.1 Law of Large Numbers (Chapter 13) 3.2 Central Limit Theorem (Chapter 14)", " Chapter 3 Introduction to Statistics: Law of Large Numbers and Central Limit Theorem Statistics is the study of uncertainty and variability. This chapter introduces the two main physical laws which govern variability: the Law of Large Numbers and the Central Limit Theorem; and describes how these laws are used in the study of uncertainty. This chapter follows chapters 13 and 14 in A Modern Introduction to Probability and Statistics (MIPS). 3.1 Law of Large Numbers (Chapter 13) The Law of Large Numbers (LLN) states that if you take independent samples from a well-behaved probability distribution, their sample mean converges in probability to the true mean. Specifically, if \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}F\\) and \\(EX_{1} = \\mu\\) and \\(\\text{Var}(X_{1}) = \\sigma^{2} &lt; \\infty\\) and \\(\\bar{X}_{n} = (1/n)\\sum_{i=1}^{n}X_{i}\\) then \\[ \\bar{X}_{n}\\overset{p}{\\to}\\mu \\] as \\(n\\to\\infty\\). We will investigate the LLN computationally through simulations. Load the packages we need: library(tidyverse) library(patchwork) # For arranging plots options(digits = 2) # Print numbers with only 2 digits Consider \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}\\text{Gamma}(\\alpha,\\beta)\\). Their mean is \\(EX_{1} = \\alpha/\\beta\\). If we take a sample from this distribution and calculate \\(\\bar{X}_{n}\\), we should get something close to \\(\\alpha/\\beta\\). If we make \\(n\\) larger, we should get something closer. The LLN tells us that we can always make \\(n\\) large enough such that we get as close as we want with as high a probability as we want. To simulate from a \\(\\text{Gamma}(\\alpha,\\beta)\\) distribution in , use the rgamma function: # Simulate from a gamma # Type ?rgamma to get information on the parametrization # There are three named parameters: shape, scale, and rate. Rate = 1/scale. # The distribution has mean shape * scale, or shape / rate. # The parametrization in the book is the &quot;rate&quot; parametrization. # Always read the docs to understand what the parameters are. # # Simulate with shape = 2 and rate = 1, so mean = 2 and variance = ? (exercise) rgamma(1,shape = 2,rate = 1) [1] 3.4 # The first &quot;1&quot; in the call gives the number of values to simulate: rgamma(10,shape = 2,rate = 1) [1] 2.76 0.43 3.93 1.66 1.32 5.39 0.55 2.73 2.90 0.25 We can plot the density of the gamma sample mean for various \\(n\\). These densities should have mean \\(\\alpha/\\beta = 2/1 = 2\\). Exercise: if \\(X_{1},\\ldots,X_{n}\\overset{ind}{\\sim}\\text{Gamma}(\\alpha_{i},\\beta)\\) then for any \\(b\\), \\(b\\times \\sum_{i=1}^{n}X_{i} \\sim \\text{Gamma}\\left(\\sum_{i=1}^{n}\\alpha_{i},\\beta/b\\right)\\). Use this to derive the distribution of \\(\\bar{X}\\) and (hence) explain what the below code is doing. This is the left side of Figure 13.1 in MIPS: # Define a function to compute the density # Fix the scale and shape arguments at defaults of what&#39;s used in the book # You can play around with these. gamma_samplemean_density &lt;- function(x,n,shape = 2,rate = 1) { # x: point to evaluate the density at # n: sample size # Just use the dgamma function dgamma(x,shape = n * shape,rate = n*rate) } # Plot it for various n # Define a function to make the plot plot_for_n &lt;- function(n) { # Create a function with the n argument fixed densfun &lt;- purrr::partial(gamma_samplemean_density,n = n) # Plot using ggplot tibble(x = c(0,4)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = densfun) + coord_cartesian(xlim = c(0,4),ylim = c(0,1.5)) + labs(title = stringr::str_c(&quot;n = &quot;,n)) } # Create the plots and arrange them # You need to have the &#39;patchwork&#39; package # loaded for this to work # library(patchwork) (plot_for_n(1) | plot_for_n(2)) / (plot_for_n(4) | plot_for_n(9)) / (plot_for_n(16) | plot_for_n(400)) The density of the sample mean appears centred at \\(2\\) and also appears to concentrate around \\(2\\). The LLN says that the sample mean should converge in probability to \\(2\\) in this example which would correspond to a density which has a point mass at \\(2\\) and is zero everywhere else. It appears like this might be happening. We can get a better look at “where the sample mean is going” by doing the following: Simulate a \\(\\text{Gamma}(2,1)\\) sample of size \\(n\\) for some really big \\(n\\), Calculate the the running average of this: for each \\(m = 1,\\ldots,n\\), compute \\(\\bar{X}_{m} = (1/m)\\sum_{i=1}^{m}X_{i}\\), Plot \\(\\bar{X}_{m}\\) against \\(m\\). \\(\\bar{X}_{m}\\) should get closer and closer to \\(2\\) as \\(m\\to n\\) (…probably). We do this as follows: set.seed(54768798) # So I can reproduce these results # Simulate one experiment of size 500 n &lt;- 100 alpha &lt;- 2 beta &lt;- 1 gamma_experiment &lt;- rgamma(n = n,shape = alpha,rate = beta) # Compute the running average- a vector where the nth component is the average # of the first n terms in gamma_experiment runningaverage &lt;- cumsum(gamma_experiment) / 1:length(gamma_experiment) # Plot, remembering that the true mean is 2 / 1 = 2 tibble(x = 1:length(runningaverage), y = runningaverage) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_point(pch = &quot;.&quot;) + geom_hline(yintercept = alpha / beta,colour = &quot;red&quot;,size = .5,linetype = &quot;dotted&quot;) Is this what you expected to see? Exercise: What happens when you increase the number? Try it for \\(n = 1,000, n = 10,000\\), and so on. What happens if you break the assumptions of the LLN, that \\(\\text{Var}(X_{1}) &lt; \\infty\\)? The Cauchy distribution has some of the worst mathematical properties of any “named” probability distribution, and so is often used to illustrate what happens when assumptions of theorems aren’t met. It has \\(EX^{r} = \\infty\\) for any \\(r\\geq 1\\) so has no finite mean, variance, etc. Let’s recreate the running average experiment for a random sample \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}\\text{Cauchy}(\\mu,\\sigma)\\): set.seed(4235) # So I can reproduce these results # Simulate one experiment of size 500 n &lt;- 500 mu &lt;- 2 sigma &lt;- 1 cauchy_experiment &lt;- rcauchy(n = n,location = mu,scale = sigma) # Compute the running average- a vector where the nth component is the average # of the first n terms in gamma_experiment runningaverage &lt;- cumsum(cauchy_experiment) / 1:length(cauchy_experiment) # Plot, remembering that the true mean is 2 / 1 = 2 tibble(x = 1:length(runningaverage), y = runningaverage) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_point(pch = &quot;.&quot;) + geom_hline(yintercept = mu,colour = &quot;red&quot;,size = .5,linetype = &quot;dotted&quot;) Is this what you expected to see? Should you expect to see anything in particular? Exercise: repeat this for the Pareto distribution. You can simulate from the Pareto distribution using the rpareto function in the actuar package. Type install.packages(\"actuar\") and then actuar::rpareto. Type ?actuar::rpareto to get help on using this function. Figuring out how to use the function is part of the exercise. Read up on the pareto distribution in MIPS or on Wikipedia or whatever. What do you expect this plot to show? Reading up on the distribution is also part of the exercise. 3.1.1 Extended example: the probability of heads As an extended example, consider trying to figure out what the probability of heads is for a fair coin, just based on flipping the coin a bunch of times. We can use the LLN to address this challenge. Let \\(X\\) be a random variable which takes values \\(0\\) and \\(1\\) if the coin comes up tails or heads on any given flip. The specific event that we are interested in is whether the coin comes up heads on any given flip. \\(p = \\text{P}(X = 1)\\) is hence the probability of heads and \\(1-p = \\text{P}(X = 0)\\) is the probability of tails. We also have \\(EX = 0\\times (1-p) + 1\\times p = p\\). We can hence see what \\(p\\) must be by flipping the coin a bunch of times, computing the sample proportion of flips that are heads, and then seeing where this sample proportion appears to be “going”. We do this as follows: # Function to flip the coin n times, and return a sequence of 1 if heads and 0 if tails # for each flip. # Use the rbinom function to simulate from a bernoulli/binomial distribution. flip_the_coin &lt;- function(n,p) { # n: number of times to flip the coin. # p: probability of heads # Returns a vector of length n containing the results of each flip. rbinom(n,1,p) } # Function to flip the coin n times and compute the # sample proportion of heads sample_proportion_of_heads &lt;- function(n,p) { # Returns a number representing the sample proportion of heads # in n flips mean(flip_the_coin(n,p)) } # Try it out: sample_proportion_of_heads(10,.5) [1] 0.4 sample_proportion_of_heads(100,.5) [1] 0.53 sample_proportion_of_heads(1000,.5) [1] 0.47 Exercise: create a plot of the running average of sample proportions of heads, similar to the above plots for the Gamma and Cauchy. How many times do you think you need to flip the coin before the result is an accurate estimate? Does this change for different values of \\(p\\)? 3.2 Central Limit Theorem (Chapter 14) The LLN says where the sample mean goes, and the Central Limit Theorem (CLT) says how it gets there. The CLT lets us approximate the probability that the sample mean is any distance we like from the true mean. We will use this later when we are using the sample mean to estimate the true mean, and want to quantify the uncertainty in our estimate. Under the same conditions as the LLN, the CLT says that the distribution of the standardized sample mean: \\[ Z_{n} = n^{1/2}\\left(\\frac{\\bar{X} - \\mu}{\\sigma}\\right) \\] converges to a standard Normal: \\[ Z_{n}\\overset{d}{\\to} Z\\sim \\text{N}(0,1) \\] as \\(n\\to\\infty\\). The concept of “converging to” is a little bit less intuitive for a whole distribution compared to just for a single number. It means that for any set \\(A\\subseteq\\mathbb{R}\\), we have \\(\\lim_{n\\to\\infty}P(Z_{n}\\in A) = P(Z\\in A)\\). This is very convenient, because we often don’t know how to use \\(Z_{n}\\) to calculate probabilities, but \\(Z\\) has a very well-known distribution so it is much easier to use it to calculate probabilities. We use probabilities calculated from \\(Z\\) to replace those which we would like to calculate about \\(Z_{n}\\). We can better understand this concept through simulation. First, let’s investigate the scaling power on \\(n\\) when standardizing averages. In order for convergence in distribution to occur, the variance of the converging variable \\(Z_{n}\\) must stabilize, \\(\\text{Var}(Z_{n})\\to\\text{constant}\\). If you multiply by the wrong power of \\(n\\), the variance of \\(Z_{n}\\) will go either to \\(0\\) or \\(\\infty\\) and its distribution will not converge. Exercise: derive the probability density of \\(Y = n^{p}(\\bar{X} - \\mu)\\) when \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}\\text{Gamma}(2,1)\\). Hint: look at the code below. How do I compute this? What formula am I using? Multiplying \\((\\bar{X} - \\mu)\\) by \\(n^{1/4}\\) isn’t enough to to stabilize the variance in the distribution, \\(n^{1}\\) is too much, and \\(n^{1/2}\\) is just right. We can see this as follows: scalingdensity &lt;- function(y,n,p) { dgamma( x = y*n^(-p) + 2, shape = 2 * n, rate = n ) * n^(-p) } plotscalingdensity &lt;- function(n,p) { dens &lt;- purrr::partial(scalingdensity,n = n,p = p) tibble(x = c(-3,3)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = dens) + coord_cartesian(xlim = c(-3,3),ylim = c(0,.4)) + labs(title = stringr::str_c(&quot;n = &quot;,n,&quot;; scaling = &quot;,p)) + theme(text = element_text(size = 8)) } # Plot them all and arrange ( plotscalingdensity(1,.25) | plotscalingdensity(1,.5) | plotscalingdensity(1,1) ) / ( plotscalingdensity(2,.25) | plotscalingdensity(2,.5) | plotscalingdensity(2,1) ) / ( plotscalingdensity(4,.25) | plotscalingdensity(4,.5) | plotscalingdensity(4,1) ) / ( plotscalingdensity(16,.25) | plotscalingdensity(16,.5) | plotscalingdensity(16,1) ) / ( plotscalingdensity(100,.25) | plotscalingdensity(100,.5) | plotscalingdensity(100,1) ) On the left the scaling is too little, and the distribution will become infinitely peaked (zero variance) as \\(n\\) gets bigger (try it!). This distribution is not useful for calculating probabilities. On the right the scaling is too much and the distribution becomes flat (infinite variance). This distribution is not useful for calculating probabilities. In the middle, we have scaled by a good amount. The distribution appears to become closer and closer to a standard Normal distribution as \\(n\\) gets bigger. This distribution is useful for calculating probabilities. You can play around with different scaling values and sample sizes. Figure 14.2 is like the centre column of Figure 14.1 with a normal density curve overlayed. Exercise: recreate the left column of Figure 14.2 by doing the following: Replace expand.grid(n = c(1,2,4,16,100),p = c(1/4,1/2,1)) by expand.grid(n = c(1,2,4,16,100),p = c(1/2)) in the above code that generates the plots (and set the values of nrow and ncol appropriately as well). Add a normal density line. You have to modify the plotscalingdensity. Add a layer as follows: stat_function(fun = dnorm,linetype = \"dotted\"). We can compute probabilities involving the standard normal distribution function in R using the pnorm function. Suppose we compute the sample mean from a \\(\\text{Gamma}(2,1)\\) random sample: set.seed(8257899) n &lt;- 500 gammamean &lt;- mean(rgamma(n,2,1)) gammamean [1] 2 We know the true mean is \\(2\\). How “close” is 2.05 to \\(2\\)? We can quantify this by asking: how probable is it to get a sample mean of 2.05 or greater in a sample of size 500 from a \\(\\text{Gamma}(2,1)\\) distribution? # The actual probability is (why? remember a previous exercise...): 1 - pgamma(gammamean,shape = 2*n,rate = 1*n) [1] 0.22 # The CLT probability is as follows: Zn &lt;- sqrt(n) * (gammamean - 2) / sqrt(2) 1 - pnorm(Zn) [1] 0.22 # Why are we doing 1 - pnorm()? pnorm() gives P(X &lt; x) for X ~ N(0,1) # To get P(X &gt; x), you can do pnorm(Zn,lower.tail = FALSE) [1] 0.22 # but it&#39;s easier to just do P(X &gt; x) = 1 - P(X &lt; x) Pretty close approximation! We could also approximate this probability by generating a bunch of gamma random samples and seeing how often their averages are &gt; 2.05: N &lt;- 10000 exceeded &lt;- numeric(N) for (i in 1:N) { samp &lt;- rgamma(n,shape = 2,rate = 1) mn &lt;- mean(samp) exceeded[i] &lt;- as.numeric(mn &gt; gammamean) } mean(exceeded) [1] 0.22 Exercise: why does this simulation give us an idea of \\(P(\\bar{X} &gt; 2.05)\\)? Hint: recall the coin flipping example: consider \\(\\bar{X} &gt; 2.05\\) to be like a coin coming up heads and estimate this probability using the method we came up with in that example. 3.2.1 Extended example: the probability of heads In our coin example from the LLN section, we investigated how many flips were needed to get an average number of heads that was close (in probability) to the true probability of heads. Using the CLT, we can get a probabilistic quantification of the error rate– how far away from the truth the proportion of heads is likely to be. Similar to the LLN experiment, we’re goin to flip the coin a bunch of times and calculate the sample proportion of heads; then we’re going to do that a bunch of times and plot a histogram of the sample proportions. The CLT tells us that as long as each experiment has enough flips, the resulting probability density of the sample proportion of heads should be approximately Normal. N &lt;- 1000 # Number of experiments to do n &lt;- 100 # Number of times to flip the coin in each experiment p &lt;- .5 # True probability of heads experiments &lt;- numeric(N) for (i in 1:N) { experiments[i] &lt;- sample_proportion_of_heads(n,p) } # Plot them tibble(x = experiments) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),bins=30,colour=&quot;black&quot;,fill=&quot;orange&quot;,alpha = .5) + stat_function(fun = dnorm,args = list(mean = p,sd = sqrt(p*(1-p)/n)),colour = &quot;purple&quot;) + labs(title = &quot;Empirical distribution of sample proportions of heads&quot;, subtitle = stringr::str_c(&quot;# of flips: &quot;,n,&quot;, true probability of heads: &quot;,p), x = &quot;Proportion of heads&quot;, y = &quot;Empirical Density&quot;) + scale_x_continuous(breaks = seq(0,1,by=.1)) Exercises: Recreate the above plot with \\(n = 10, 50, 100, 1000\\) and \\(p = .4, .2, .8, .01, .99\\). What do you see? Is the accuracy of the normal approximation to this distribution affected by \\(n\\) or \\(p\\)? What are the mean and variance of the distribution of sample proportions of heads? (Hint: what are the mean and variance of a \\(\\text{Binom}(n,p)\\) random variable?) Recreate the above plot, but scale the sample proportion of heads appropriately such that it has mean \\(0\\) and variance \\(1\\). "],["section-statistical-models.html", "Chapter 4 Statistical Models 4.1 Statistical models (Chapter 17) 4.2 Unbiased Estimators (Chapter 19)", " Chapter 4 Statistical Models This chapter covers statistical models and plotting regression lines (although most of the math of linear regression is left to a later chapter) and unbiased estimators. We’ll use data from the book A Modern Introduction to Probability and Statistics (MIPS) which can be found here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. This will be referred to as “the book data folder”. This chapter corresponds to Chapters 17 and 19 in MIPS. 4.1 Statistical models (Chapter 17) A statistical model is a probability distribution contructed to enable inferences to be drawn or decisions made from data. Statistical Models (2008), A.C. Davison, Cambridge Series in Statistical and Probabilistic Mathematics Data are realizations of random variables. The probability distribution of these random variables can be used to reason about properties of the universe which are not directly observable. This is called making an inference. These “properties” are usually unknown numbers which we call parameters. So Statistical models are probability distributions for observable data, which depend on one or more unknown parameters, which represent unobservable, unknown properties of the universe, that we want to know. Statistical inference is the science of estimating parameters in statistical models using data. Example: a drug company wants to know if their new drug leads to different cholesterol levels in peoples’ blood than a placebo. The difference in cholesterol levels in peoples’ blood who take the new drug vs those who take the placebo is a single, unknown, unobservable number, that the drug company would like to know– a parameter. The company gives some people the drug and some people a placebo, and observes each person’s blood cholesterol– data. The drug company then uses these data along with a statistical model to estimate the unknown parameter, that is, the difference in cholesterol levels in peoples’ blood who take the new drug vs those who take the placebo. Example: astrophysicists want to know the mass of the Milky Way galaxy. It’s really, really heavy (they measure it in units of “the mass of one trillion suns”), so they can’t just go out and measure its weight directly. The mass of the Milky Way is a single, unknown, unobservable number– a parameter. The astrophysicists can measure the position and velocity of stars (actually clusters of stars) which orbit the the Galaxy (data) and they have a statistical model which relates these position and velocity measurements to the mass of the Galaxy. They go out, measure the position and velocities of these star clusters, and use them and this model to estimate the mass of the Milky Way. 4.1.1 Linear Regression A very common introductory statistical model is Linear Regression. Suppose you have data \\(y_{i}\\overset{ind}{\\sim}\\text{N}(\\mu_{i},\\sigma^{2})\\)– normally distributed observations with different means and the same standard deviation. Suppose you have a covariate \\(x_{i}\\) and you want to see how the mean of \\(y_{i}\\) depends on \\(x_{i}\\). Linear regression says that this dependence is \\[ \\mu_{i} = \\beta_{0} + \\beta_{1}x_{i} \\] where \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are unknown parameters. The data \\(y_{i}\\) are used to estimate these parameters within this statistical model. The Janka Hardness dataset contains measurements of the density and hardness of the wood from some trees. It is of interest to who-and-or-whomever took these measurements to relate the average hardness of the wood to its density. So \\(y_{i}\\) is hardness and \\(x_{i}\\) is density and we will do inference for the two unknown parameters in the linear regression model. First read in the data. Print it out on the command line: head data/MIPSdata/jankahardness.txt 24.7 484 24.8 427 27.3 413 28.4 517 28.4 549 29 648 30.3 587 32.7 704 35.6 979 38.5 914 By printing it out on the command line, you can tell that the file is tab-delimited. Use readr::read_delim() to read it in: library(tidyverse) library(patchwork) janka &lt;- readr::read_delim( file = &quot;data/MIPSdata/jankahardness.txt&quot;, delim = &quot;\\t&quot;, col_names = c(&quot;density&quot;,&quot;hardness&quot;), col_types = &quot;nn&quot; ) glimpse(janka) Observations: 36 Variables: 2 $ density &lt;dbl&gt; 25, 25, 27, 28, 28, 29, 30, 33, 36, 38, 39, 39, 39, 40, 40, … $ hardness &lt;dbl&gt; 484, 427, 413, 517, 549, 648, 587, 704, 979, 914, 1070, 1020… Create a scatterplot with ggplot2: jankascatter &lt;- janka %&gt;% ggplot(aes(x = density,y = hardness)) + theme_classic() + geom_point() + scale_x_continuous(breaks = seq(20,80,by=10)) + scale_y_continuous(breaks = seq(0,3500,by=500)) + coord_cartesian(xlim = c(20,80),ylim = c(0,3500)) + labs(x = &quot;Wood density&quot;, y = &quot;Hardness&quot;) jankascatter It looks like on average, the hardness of the wood increases linearly with its density. A linear regression model might be appropriate. To add a line to a plot, use geom_abline(): jankascatter + geom_abline(slope = 57.51,intercept = -1160.5) In a later chapter, you will learn how to calculate the \\(-1160.5\\) and the \\(57.51\\) yourself. But for now, it would still be nice to get the computer to compute these values for us rather than typing them in manually. We can do this using the geom_smooth() function in ggplot2: jankascatter + geom_smooth(method = &quot;lm&quot;,se = FALSE,colour = &quot;black&quot;,size = .5) The “lm” stands for “linear model” and the “se” stands for “standard error”; leaving this at its default of “TRUE” would add error bars to the line. We haven’t learned about error bars yet. Aside: there are other types of lines you can add with \\texttt{geom_smooth}. The “loess” non-linear regression line is a type of nonlinear regression obtained by breaking the \\(x\\) axis up into chunks and then doing linear regression in each chunk and then joining the resulting lines together. It roughly stands for “local regression and smoothing splines”. We can add this using ggplot2 as well: jankascatter + geom_smooth(method = &quot;loess&quot;,se = FALSE,colour = &quot;black&quot;,size = .5) For these data, this isn’t much different. 4.1.2 Extended example: TTC ridership revenues Toronto’s population is growing over time. This puts strain on our outdated public transit system. But it should also lead to increased revenues. According to (https://globalnews.ca/news/1670796/how-does-the-ttcs-funding-compare-to-other-transit-agencies/)[a news article from a few years back], the TTC is the least-subsidized major transit agency in North America, which means that its operating budget is the most dependent on fare revenue out of any in all of the US and Canada. Tracking how ridership revenues are changing over time is very important. The city of Toronto does do this. Go to the City of Toronto Progress Portal and type “TTC” and click on the box that says “TTC Ridership Revenues” to see a report. You can download the data from here, but since it’s a bit tricky to describe exactly how, I have put the file ttc-ridership-revenues.csv in the data folder. We are going to read these data into R and analyze the relationship between year and revenue. If you’re thinking “that sounds really easy, we just did that!”… just keep reading. First, print the data out and count the number of rows on the command line: head data/ttc-ridership-revenues.csv wc -l data/ttc-ridership-revenues.csv Year,Jan,Feb,Mar,Apr,May,Jun,Jul,Aug,Sep,Oct,Nov,Dec 2007 YTD Actual,$70600000,$131200000,$204600000,$264900000,$322000000,$395100000,$452100000,$507500000,$585600000,$646900000,$712500000,$774700000 2008 YTD Actual,$72700000,$137600000,$217500000,$278200000,$340600000,$419600000,$482400000,$544100000,$629000000,$696400000,$766600000,$837000000 2009 YTD Actual,$69300000,$135400000,$216600000,$280500000,$344000000,$422100000,$483400000,$543500000,$627200000,$693900000,$762400000,$834900000 2010 YTD Actual,$72200000,$143400000,$230700000,$302400000,$372500000,$459100000,$528800000,$595700000,$689100000,$764500000,$842000000,$929300000 2011 YTD Actual,$75300000,$150800000,$244400000,$318300000,$392400000,$484800000,$557300000,$625500000,$722000000,$799500000,$879100000,$969900000 2012 YTD Actual,$75500000,$154800000,$253900000,$331600000,$408300000,$507100000,$581800000,$654300000,$755800000,$835900000,$919100000,$1017600000 2013 YTD Actual,$93200000,$176600000,$278200000,$360300000,$439700000,$539700000,$617400000,$693500000,$799600000,$882500000,$968900000,$1052100000 2014 YTD Actual,$92200000,$178900000,$284400000,$367200000,$449700000,$552300000,$633200000,$712200000,$822000000,$907900000,$998200000,$1086500000 2015 YTD Actual,$90600000,$178200000,$284100000,$370500000,$455300000,$559600000,$641700000,$721400000,$833200000,$920800000,$1011600000,$1107300000 14 data/ttc-ridership-revenues.csv Yikes! Real data is messy. This data isn’t even that messy and it still seems messy. We see that the file is comma-separated and has a header. The first column is text and the others are… well, they’re supposed to be numeric, but they are stored in the file with dollar signs. WHY?! This kind of thing is super annoying and super common. We could remove the dollar signs from the text file directly using sed or a similar UNIX-based tool, but I prefer whenever possible to keep all my analysis on one platform. We’ll read it into R as-is and then parse and change datatypes there: # Read in the data ridership &lt;- readr::read_csv( file = &quot;data/ttc-ridership-revenues.csv&quot;, col_names = TRUE, # Tells readr to read the column names from the first line of the file. col_types = stringr::str_c(rep(&quot;c&quot;,13),collapse = &quot;&quot;) # Read all 13 columns as &quot;c&quot;haracter ) glimpse(ridership) Observations: 13 Variables: 13 $ Year &lt;chr&gt; &quot;2007 YTD Actual&quot;, &quot;2008 YTD Actual&quot;, &quot;2009 YTD Actual&quot;, &quot;2010 Y… $ Jan &lt;chr&gt; &quot;$70600000&quot;, &quot;$72700000&quot;, &quot;$69300000&quot;, &quot;$72200000&quot;, &quot;$75300000&quot;,… $ Feb &lt;chr&gt; &quot;$131200000&quot;, &quot;$137600000&quot;, &quot;$135400000&quot;, &quot;$143400000&quot;, &quot;$150800… $ Mar &lt;chr&gt; &quot;$204600000&quot;, &quot;$217500000&quot;, &quot;$216600000&quot;, &quot;$230700000&quot;, &quot;$244400… $ Apr &lt;chr&gt; &quot;$264900000&quot;, &quot;$278200000&quot;, &quot;$280500000&quot;, &quot;$302400000&quot;, &quot;$318300… $ May &lt;chr&gt; &quot;$322000000&quot;, &quot;$340600000&quot;, &quot;$344000000&quot;, &quot;$372500000&quot;, &quot;$392400… $ Jun &lt;chr&gt; &quot;$395100000&quot;, &quot;$419600000&quot;, &quot;$422100000&quot;, &quot;$459100000&quot;, &quot;$484800… $ Jul &lt;chr&gt; &quot;$452100000&quot;, &quot;$482400000&quot;, &quot;$483400000&quot;, &quot;$528800000&quot;, &quot;$557300… $ Aug &lt;chr&gt; &quot;$507500000&quot;, &quot;$544100000&quot;, &quot;$543500000&quot;, &quot;$595700000&quot;, &quot;$625500… $ Sep &lt;chr&gt; &quot;$585600000&quot;, &quot;$629000000&quot;, &quot;$627200000&quot;, &quot;$689100000&quot;, &quot;$722000… $ Oct &lt;chr&gt; &quot;$646900000&quot;, &quot;$696400000&quot;, &quot;$693900000&quot;, &quot;$764500000&quot;, &quot;$799500… $ Nov &lt;chr&gt; &quot;$712500000&quot;, &quot;$766600000&quot;, &quot;$762400000&quot;, &quot;$842000000&quot;, &quot;$879100… $ Dec &lt;chr&gt; &quot;$774700000&quot;, &quot;$837000000&quot;, &quot;$834900000&quot;, &quot;$929300000&quot;, &quot;$969900… This does not look like it’s in a form ready to analyze. Some problems: The Year has unwanted text in it. We just want the number representing what year it is. The revenue is stored across 12 columns, one for each month. We want the annual revenue for our analysis. The actual numeric revenue is stored as text with a dollar sign. We need to parse out the number part and convert to a numeric datatype before we can analyze it. The numbers in the sheet are cumulative revenue for the whole year (“YTD” = “Year To Date”). We want the monthly revenues. Problems 1 and 3 require a bit of text parsing; Problem 2 requires converting from “wide” to “long” format. Let’s do it: # PROBLEM 1: Year # To parse out only the number part, use a regular expression. # Our string starts with a four digit number which starts with 20. We want to capture this number # and nothing else. # The ^ means &quot;the start of the string&quot;. # The [20]{2} means &quot;a 0 or a 2, exactly twice&quot; # The [0-9]{2} means &quot;anything from 0 - 9, exactly twice&quot; year_regex &lt;- &quot;^[20]{2}[0-9]{2}&quot; # Use stringr::str_extract to extract a substring matching the regular expression: stringr::str_extract(&quot;2007 YTD Actual&quot;,year_regex) [1] &quot;2007&quot; # PROBLEM 2: wide to long # Use the tidyr::pivot_longer() function for gathering columns and putting them # into one column: ridership %&gt;% tidyr::pivot_longer( Jan:Dec, # Collect columns Jan through Dec, including everything between names_to = &quot;month&quot;, # Create a new column called &quot;month&quot; which contains the names of the old columns values_to = &quot;revenue&quot; # Create a new column called &quot;revenue&quot; which contains the values of the old columns ) # A tibble: 156 x 3 Year month revenue &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 2007 YTD Actual Jan $70600000 2 2007 YTD Actual Feb $131200000 3 2007 YTD Actual Mar $204600000 4 2007 YTD Actual Apr $264900000 5 2007 YTD Actual May $322000000 6 2007 YTD Actual Jun $395100000 7 2007 YTD Actual Jul $452100000 8 2007 YTD Actual Aug $507500000 9 2007 YTD Actual Sep $585600000 10 2007 YTD Actual Oct $646900000 # … with 146 more rows # PROBLEM 3: removing the dollar sign # Again, use text matching. Because $ is itself a special character, # to match it, you have to &quot;escape&quot; it using a backslash dollar_regex &lt;- &quot;\\\\$&quot; # Remove matching strings using stringr::str_remove() stringr::str_remove(&quot;$1234&quot;,dollar_regex) [1] &quot;1234&quot; # PROBLEM 4: going from cumulative to monthly revenue # The cumulative revenue for 2007 is: cumrev2007 &lt;- as.numeric(stringr::str_remove(as.character(ridership[1,-1]),dollar_regex)) cumrev2007 [1] 7.1e+07 1.3e+08 2.0e+08 2.6e+08 3.2e+08 4.0e+08 4.5e+08 5.1e+08 5.9e+08 [10] 6.5e+08 7.1e+08 7.7e+08 # The monthly revenue for (e.g.) Feb is cumrev2007[2] - cumrev2007[1] [1] 6.1e+07 # Can do this all at once like c(cumrev2007[1],cumrev2007[2:length(cumrev2007)] - cumrev2007[1:(length(cumrev2007)-1)]) [1] 7.1e+07 6.1e+07 7.3e+07 6.0e+07 5.7e+07 7.3e+07 5.7e+07 5.5e+07 7.8e+07 [10] 6.1e+07 6.6e+07 6.2e+07 # or c(cumrev2007[1],diff(cumrev2007)) [1] 7.1e+07 6.1e+07 7.3e+07 6.0e+07 5.7e+07 7.3e+07 5.7e+07 5.5e+07 7.8e+07 [10] 6.1e+07 6.6e+07 6.2e+07 # but both those are a bit clumsy, and won&#39;t work inside a data processing # pipeline with grouping. The lag() function in the dplyr package # is meant for use within a data processing pipeline: cumrev2007 - lag(cumrev2007,n=1,default = 0) [1] 7.1e+07 6.1e+07 7.3e+07 6.0e+07 5.7e+07 7.3e+07 5.7e+07 5.5e+07 7.8e+07 [10] 6.1e+07 6.6e+07 6.2e+07 # and will let us do this for all years at once using one line of code below. # Now, combine all these into one data cleaning pipeline. # Remember we have monthly revenue, so to get yearly revenue, we sum # over months. ridership_clean &lt;- ridership %&gt;% tidyr::pivot_longer( Jan:Dec, names_to = &quot;month&quot;, values_to = &quot;revenue&quot; ) %&gt;% # &quot;transmute&quot; is like mutate, but it deletes all original columns mutate(year = stringr::str_extract(Year,year_regex), revenue = stringr::str_remove(revenue,dollar_regex)) %&gt;% mutate_at(c(&quot;year&quot;,&quot;revenue&quot;),as.numeric) %&gt;% # Turn both year and revenue into numeric variables # Compute the monthly revenue, by year # Grouping by year... group_by(year) %&gt;% # ...causes everything in the mutate() to be done separately for each year. mutate(monthly_revenue = revenue - lag(revenue,n=1,default=0)) glimpse(ridership_clean) Observations: 156 Variables: 5 Groups: year [13] $ Year &lt;chr&gt; &quot;2007 YTD Actual&quot;, &quot;2007 YTD Actual&quot;, &quot;2007 YTD Actua… $ month &lt;chr&gt; &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug… $ revenue &lt;dbl&gt; 7.1e+07, 1.3e+08, 2.0e+08, 2.6e+08, 3.2e+08, 4.0e+08,… $ year &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,… $ monthly_revenue &lt;dbl&gt; 7.1e+07, 6.1e+07, 7.3e+07, 6.0e+07, 5.7e+07, 7.3e+07,… That looks a lot better! As usual, you should run each line of code one by one to understand what is happening. Because we went to the effort of cleaning the data, we can now plot it. We want to plot the monthly revenue over time. This requires creating a variable that represents time– currently we have two, year and month. We can do this using the function: # Cumulative revenue ridershipscattermonthly &lt;- ridership_clean %&gt;% mutate(date = lubridate::ymd(paste0(year,month,&quot;01&quot;))) %&gt;% ggplot(aes(x = date,y = monthly_revenue)) + theme_classic() + geom_point() + geom_line() + labs(title = &quot;Monthly ridership revenues for the TTC&quot;, x = &quot;Date&quot;, y = &quot;Revenue&quot;) + scale_y_continuous(labels = scales::dollar_format()) # Make the y-axis pretty ridershipscattermonthly Warning: Removed 6 rows containing missing values (geom_point). Warning: Removed 6 rows containing missing values (geom_path). I added a line joining the points together to make the pattern clearer. It looks like monthly revenue is pretty cyclical. Can we add a regression line? Sure! ridershipscattermonthly + geom_smooth(method = &quot;lm&quot;,se = FALSE) Warning: Removed 6 rows containing non-finite values (stat_smooth). Warning: Removed 6 rows containing missing values (geom_point). Warning: Removed 6 rows containing missing values (geom_path). While revenue doesn’t necessarily increase every month, it looks like it is increasing on average. Exercise: repeate this analysis using annual revenue. You need to create the following dataset: Observations: 13 Variables: 2 $ year &lt;dbl&gt; 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, … $ annual_revenue &lt;dbl&gt; 7.7e+08, 8.4e+08, 8.3e+08, 9.3e+08, 9.7e+08, 1.0e+09, … by grouping by and then summing , and then make the following plot: Once the monthly variation is smoothed out, it looks like revenues are increasing over years. 4.2 Unbiased Estimators (Chapter 19) Much of Statistics involves studying the mathematical properties of estimators. Studying the mathematical properties of estimators lets us reason about how well those estimators should represent the underlying true unknown parameters for any given sample of data, and hence how to reason about how “good” our inferences are. Unbiasedness is one property of an estimator that may be attractive. Suppose we have a statistical model for random variables \\(X_{i}\\overset{iid}{\\sim}F_{\\theta}\\) depending on parameter \\(\\theta\\) and we estimate \\(\\theta\\) with an estimator \\(\\hat{\\theta}: \\mathbb{R}^{n}\\to\\mathbb{R}\\). The estimator \\(\\hat{\\theta}\\) is said to be unbiased if \\[ \\mathbb{E}\\hat{\\theta} = \\theta \\] The expectation is with respect to the probability distribution of \\(\\hat{\\theta}\\), which depends on the whole sample \\(X_{1},\\ldots,X_{n}\\) (“sampling distribution”). On average, over all possible samples, \\(\\hat{\\theta}\\) “gets it right”. This is why some people consider the property of unbiasedness to be a desirable property for an estimator. We can use simulation to get a feel for what unbiasedness means by sampling a bunch of datasets, calculating the estimator for each one, and then calculating the average of those estimators. 4.2.1 Simulated data Consider samples \\(X_{1},\\ldots,X_{n}\\) of size \\(n=30\\) from a \\(X\\sim\\text{Poisson}(\\lambda)\\) distribution with \\(\\lambda = \\log 10\\). We want to estimate the parameter \\(p_{0}\\), which is the probability that \\(X = 0\\): \\[\\begin{equation} p_{0} = P(X = 0) = e^{-\\lambda} \\end{equation}\\] where \\(\\lambda = E(X) = \\log 10\\) in this example, so \\(p_{0} = 0.1\\) (this would be unknown for real data!). Let’s look at two estimators: \\[ S = (1/n)\\times\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 0) \\] and \\[ T = e^{-\\bar{X}_{n}} \\] \\(S\\) corresponds to calculating the sample proportion of times \\(X_{i}=0\\), and \\(T\\) corresponds to estimating the population mean \\(\\lambda\\) using the sample mean \\(\\bar{X}_{n}\\) and then plugging this in to the actual formula for the value \\(p_{0}\\). How we come up with stuff like this will be revealed in later chapters. Let’s investigate their sampling distributions and (hence) unbiasedness via simulations: set.seed(6574564) # Simulate 500 random samples of size 30 from a poisson(log(10)) N &lt;- 500 n &lt;- 30 lambda &lt;- log(10) p0 &lt;- exp(-lambda) # True value of p0 # Write functions to compute each estimator compute_S &lt;- function(samp) mean(samp == 0) compute_T &lt;- function(samp) exp(-mean(samp)) # Simulate the samples and calculate the estimators for each sample samples &lt;- vector(mode = &quot;list&quot;,length = N) SS &lt;- TT &lt;- numeric(N) for (i in 1:N) { samples[[i]] &lt;- rpois(n,lambda) SS[i] &lt;- compute_S(samples[[i]]) TT[i] &lt;- compute_T(samples[[i]]) } # Create the plots plt_S &lt;- tibble(SS = SS) %&gt;% ggplot(aes(x = SS)) + theme_classic() + geom_histogram(colour = &quot;black&quot;,fill = &quot;transparent&quot;,bins = 7) + coord_cartesian(ylim = c(0,250)) + geom_vline(xintercept = p0,colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) plt_T &lt;- tibble(TT = TT) %&gt;% ggplot(aes(x = TT)) + theme_classic() + geom_histogram(colour = &quot;black&quot;,fill = &quot;transparent&quot;,bins = 7) + coord_cartesian(ylim = c(0,250)) + geom_vline(xintercept = p0,colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) plt_S | plt_T # Compute the mean of each: mean(SS) [1] 0.1 mean(TT) [1] 0.1 Exercise: it turns out that, mathematically, \\(S\\) is unbiased and \\(T\\) is biased. Which estimator would you prefer? Compute a five number summary for \\(S\\) and \\(T\\) from our simulations, recreating the following (make sure to use the same random seed as me, using : Min. 1st Qu. Median Mean 3rd Qu. Max. 0.000 0.067 0.100 0.101 0.133 0.267 Min. 1st Qu. Median Mean 3rd Qu. Max. 0.031 0.082 0.100 0.103 0.122 0.231 Do you see any meaningful differences? Do the sampling distributions of \\(S\\) and \\(T\\) concentrate around \\(p0\\) in the same way? Now, compute the mode (most frequently-observed value) of \\(S\\) and \\(T\\). You should get the following: The mode of S is 0.067 The mode of T is 0.097 (You’re going to have to figure out how to compute a mode in R. That’s part of the exercise). What do you think about this? Does this contradict \\(S\\) being unbiased and \\(T\\) being biased? Does it change your opinion about which is a better estimator? "],["section-evaluating-estimators-efficiency-and-mean-squared-error.html", "Chapter 5 Evaluating Estimators: Efficiency and Mean Squared Error 5.1 Estimating a Uniform Maximum 5.2 Efficiency 5.3 Mean Squared Error", " Chapter 5 Evaluating Estimators: Efficiency and Mean Squared Error This chapter introduces the concepts of Efficiency and Mean Square Error and shows how to use them to evaluate the performance of an estimator. This material is covered in Chapter 20 of A Modern Introduction to Probability and Statistics. 5.1 Estimating a Uniform Maximum In World War II, it was of interest to the Allied forces to estimate the number of tanks that the German army had produced. German tanks had serial numbers, which were assigned sequentially at production. That means that the largest serial number of any tank in existence at any given time is also equal to the number of tanks having been produced up to that point in time. Whenever Allied forces captured or destroyed a tank, they would record the serial number. Call \\(X_{i}\\) the serial number of the \\(i^{th}\\) tank. Assuming that the probability of capturing any particular tank is the same as any other tank, and that this occurs independently of the capture of other tanks, and that a tank can only be captured once, we would have a sample of size \\(n\\) from a discrete uniform distribution with maximum value \\(N\\), taken without replacement. The unknown value \\(N\\) happens to equal the number of German tanks produced, and the Allies wanted to estimate this number using the recorded serial numbers \\(X_{1},\\ldots,X_{n}\\). This requires constructing an estimator for \\(N\\) using a sample of \\(\\text{Unif}(1,N)\\) random variables. However, these samples are dependent, because the sampling is done without replacement: once a particular tank appears in the sample, it can’t appear again. Let \\(X_{i}\\) be the serial numbers of the captured tanks; it can be shown that \\[\\begin{equation}\\begin{aligned} E\\left( \\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right) &amp;= \\frac{N+1}{2} \\\\ E\\left(\\text{max} X_{i}\\right) &amp;= \\frac{n}{n+1}\\times(N+1) \\end{aligned}\\end{equation}\\] Perhaps we should construct an unbiased estimator, given our discussion in the previous chapter. Exercise: construct two unbiased estimators for \\(N\\), using these two results on the expected values of the sample mean and maximum. Do this before you look at the answer below. If you look at the answer below first, then you will learn less from this exercise. Next, let’s write functions to compute these two estimators, and use simulation to verify that they are unbiased. At this point in the course, you should start feeling comfortable approaching this yourself. Again, I encourage you to try this before looking at my answer as follows: library(tidyverse) library(patchwork) set.seed(432354675) # Functions to compute the estimators T1 &lt;- function(x) 2 * mean(x) - 1 T2 &lt;- function(x) ( (length(x) + 1)/length(x) ) * max(x) - 1 # Now, simulate in order to assess their bias. # This goes as follows (try this yourself before looking): # - Choose a true value of N, the parameter to be estimated # - Draw a sample of size n from 1:N without replacement # - Compute T1 and T2 # - Repeat this M times, and compare the average of T1 and T2 to N. N &lt;- 1000 n &lt;- 100 M &lt;- 1000 # One million simulations # Run the simulations. Use the sample.int() function to generate from a DISCRETE # uniform distribution thesimulations &lt;- list( T1 = numeric(M), T2 = numeric(M), T3 = numeric(M) ) for (i in 1:M) { # Do the simulation # Sample from a discrete uniform (?sample.int): thesample &lt;- sample.int(N,n,replace = FALSE) # Record the values of the two estimators: thesimulations$T1[i] &lt;- T1(thesample) thesimulations$T2[i] &lt;- T2(thesample) } # Evaluate the bias of T1 and T2: mean(thesimulations$T1) - N [1] -1.7 mean(thesimulations$T2) - N [1] 0.31 Exercise: What do you think about the bias of \\(T_{1}\\) and \\(T_{2}\\)? Repeat this whole simulation multiple times and plot a histogram of the biases. What do you conclude? Try the simulation again with a higher value of \\(M\\), like \\(M = 10^{6}\\) or whatever your computer can handle. What do you conclude? Finally, let’s look at the sampled values of \\(T_{1}\\) and \\(T_{2}\\): # Recreate the plots in Figure 20.1: leftplot &lt;- tibble(T1 = thesimulations$T1) %&gt;% ggplot(aes(x = T1)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + scale_x_continuous(breaks = c(300,700,1000,1300,1600)) + coord_cartesian(xlim = c(300,1600)) rightplot &lt;- tibble(T2 = thesimulations$T2) %&gt;% ggplot(aes(x = T2)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + scale_x_continuous(breaks = c(300,700,1000,1300,1600)) + coord_cartesian(xlim = c(300,1600)) leftplot | rightplot Remembering that \\(N = 1000\\) in this example, do you prefer either \\(T_{1}\\) or \\(T_{2}\\)? 5.2 Efficiency You can often construct multiple unbiased estimators for the same problem. Are they all as “good” as one another, or is there some reason to prefer one over another? One way to compare estimators is by looking at their variance. If one unbiased estimator has lower variance than another unbiased estimator, we say that the one with lower variance is more efficient than the one with higher variance. Estimators are random variables and you can calculate their variances mathematically. Exercise: let \\(X_{i},\\overset{iid}{\\sim}\\text{N}(\\mu,1)\\) and suppose we have the following two estimators for \\(\\mu\\): \\[\\hat{\\mu}_{1} = X_{1}, \\hat{\\mu}_{2} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i} = \\bar{X}.\\] Show that \\(\\hat{\\mu}_{1}\\) and \\(\\hat{\\mu}_{2}\\) are both unbiased estimators of \\(\\mu\\), Show that \\(\\hat{\\mu}_{2}\\) is more efficient than \\(\\hat{\\mu}_{1}\\). Remarkably, it can be shown that there is actually a lower bound on the variance of any unbiased estimator for some likelihoods. This is called the Cramer-Rao lower bound, and states that for a sample \\(X_{i}\\overset{iid}{\\sim}F_{\\theta}, i=1,\\ldots,n\\) with log-likelihood \\(\\ell(\\theta)\\), and \\(\\hat{\\theta}\\) any unbiased estimator of \\(\\theta\\), that \\[\\text{Var}(\\theta) \\geq \\frac{n}{-\\partial^{2}\\ell(\\tilde{\\theta}) / \\partial\\theta^{2}},\\] where \\(\\tilde{\\theta} = \\text{argmax}\\ell(\\theta)\\), the maximum value of \\(\\ell(\\theta)\\). In order for this bound to hold, the log-likelihood has to satisfy certain mathematical “regularity” conditions which are outside the scope of this course. When an unbiased estimator has variance which equals the lower bound, we say that this estimator is efficient. Exercise: show that \\(\\hat{\\mu}_{1}\\) is not efficient and that \\(\\hat{\\mu}_{2}\\) is efficient, as follows: Write down the log-likelihood \\(\\ell(\\theta)\\) and the second derivative \\(\\partial^{2}\\ell(\\theta) / \\partial\\theta^{2}\\) for this example, Compute the maximum \\(\\tilde{\\theta} = \\text{argmax}\\ell(\\theta)\\) and the curvature \\(\\partial^{2}\\ell(\\tilde{\\theta}) / \\partial\\theta^{2}\\), Compare the variance of \\(\\hat{\\mu}_{1}\\) and \\(\\hat{\\mu}_{2}\\) to the bound \\(\\frac{n}{-\\partial^{2}\\ell(\\tilde{\\theta}) / \\partial\\theta^{2}}\\). The discrete Uniform likelihood does not satisfy the conditions for the lower bound to hold. However, we can compare the efficiency of \\(T_{1}\\) and \\(T_{2}\\) using simulation: var(thesimulations$T1) [1] 2985 var(thesimulations$T2) [1] 71 Exercise: which estimator appears more efficient based on these simulations? Discussion: is the existence of a lower bound on the variance of an unbiased estimator a good or bad thing? Consider the following points and talk about it with your classmates: It’s a good thing, because if we want to pick an unbiased estimator to use, we just need to find one with variance that meets the lower bound (you will see in a subsequent chapter that this is actually really, really easy to do), It’s a bad thing, since it limits how “good” an unbiased estimator can be. 5.3 Mean Squared Error We have seen two examples of how to evaluate the quality of an estimator \\(\\hat{\\theta}\\) of a parameter \\(\\theta\\): its bias: \\[\\text{bias} = E\\hat{\\theta} - \\theta,\\] and its variance: \\[\\text{Var}(\\hat{\\theta}) = E\\left[\\left(\\hat{\\theta} - E\\hat{\\theta}\\right)^{2}\\right].\\] We can combine these two into a single measure of the quality of \\(\\hat{\\theta}\\). Definition: the mean squared error of an estimator \\(\\hat{\\theta}\\) is the mean of the squared error in using \\(\\hat{\\theta}\\) to estimate \\(\\theta\\): \\[\\text{MSE}(\\hat{\\theta}) = E\\left[\\left(\\hat{\\theta} - \\theta\\right)^{2}\\right].\\] Exercise: show that \\(\\hat{\\theta}\\) is unbiased if and only if \\(\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta})\\). The MSE combines the variance and the bias of an estimator as follows: Exercise: prove that \\[\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + \\text{bias}(\\hat{\\theta})^{2}.\\] Exercise: for the Normal random sample from before, compute the MSE of \\(\\hat{\\mu}_{1}\\) and \\(\\hat{\\mu}_{2}\\). Exercise: for the Uniform random sample from before, Compute the MSE of \\(T_{1}\\) and \\(T_{2}\\) from the simulations, Show that \\(T_{3} = \\text{max}(X_{i})\\) is a biased estimator of \\(N\\). Even though \\(T_{3}\\) is a biased estimator, is it a worse estimator that \\(T_{1}\\) or \\(T_{2}\\)? We can check through simulation: T3 &lt;- function(x) max(x) mse &lt;- function(x) var(x) + (mean(x) - N)^2 thesimulations$T3 &lt;- numeric(M) for (i in 1:M) { thesample &lt;- sample.int(N,n) thesimulations$T3[i] &lt;- T3(thesample) } While \\(T_{3}\\) appears to have far higher bias than \\(T_{1}\\) or \\(T_{2}\\)… mean(thesimulations$T1) - N [1] -1.7 mean(thesimulations$T2) - N [1] 0.31 mean(thesimulations$T3) - N [1] -8.9 …it has far lower variance than \\(T_{1}\\)… var(thesimulations$T1) [1] 2985 var(thesimulations$T2) [1] 71 var(thesimulations$T3) [1] 77 …and hence its MSE is far better than \\(T_{1}\\)… mse(thesimulations$T1) [1] 2988 mse(thesimulations$T2) [1] 72 mse(thesimulations$T3) [1] 156 …although \\(T_{2}\\) is still better. library(tidyverse) library(patchwork) "],["section-introduction-to-bayesian-inference.html", "Chapter 6 Introduction to Bayesian Inference 6.1 Tutorial 6.2 Interactive App", " Chapter 6 Introduction to Bayesian Inference This chapter introduces the foundations of Bayesian inference. Materials in this tutorial are taken from Alex Stringer’s comprehensive tutorial on Bayesian Inference, which is very long and outside the scope of this course. 6.1 Tutorial In this tutorial we will discuss at length the Beta-Bernoulli example from section 7.1. First follow along with this tutorial, then check out the interactive app. Review: given data generated from some family of probability distributions indexed by an unknown parameter, statistical inference is concerned with estimating these parameters- finding reasonable values for them, given the observed data, and quantifying the uncertainty in these estimates. The central notion is that of uncertainty: we simply don’t know the values of the parameters that generated the data we observed, and we do know that several different values could reasonably have generated these data. Probability is the mathematical construct used to represent uncertainty. 6.1.1 Frequentist/Likelihood Perspective Classically, the approach to this problem is taught from the frequentist perspective. Uncertainty in the values of the parameters that generated the data is represented by probability via the notion of repeated sampling: under the given probability model with the given parameter values, what is the relative frequency with which these same data would be observed, if the experiment that generated the data were repeated again and again? Values of the parameters that have a higher probability of having generated the observed data are thought to be more likely than other values. “Likelihood” is NOT a synonym of “probability”: probability refers to the realization of a random variable (what is the probability we see 4 heads in 10 flips of a coin?), while likelihood refers to the value of a parameter (what is the likelihood of having seen 4 heads in 10 flips of a coin which has a .4 chance of coming up heads?). As an example, consider a coin with unknown parameter \\(\\theta\\) representing the probability of heads. We toss the coin once, and observe random outcome (data) \\(X = 1\\) if the toss is heads and \\(X = 0\\) if not. For simplicity, suppose we know we have chosen one of two possible coins, either having \\(\\theta = 0.7\\) or \\(\\theta = 0.3\\). How do we use the observed data to infer which of these two coins we threw? For any \\(0 &lt; \\theta &lt; 1\\), the probability distribution of the single coin toss is given by \\[ P(X = x) = \\theta^{x}(1-\\theta)^{1-x} \\] This just says that \\(P(X = 1) = \\theta\\) and \\(P(X = 0) = 1-\\theta\\). Let’s say we throw the coin once, and observe \\(X = 1\\). If \\(\\theta = 0.7\\) the probabilty of observing this result is \\(P(X = 1|\\theta = 0.7) = 0.7\\). That is if \\(\\theta = 0.7\\), we would expect roughly \\(70\\%\\) of repetitions of this experiment to yield the same results as we observed in our data. If \\(\\theta = 0.3\\) on the other hand, \\(P(X = 1|\\theta = 0.3) = 0.3\\); only \\(30\\%\\) of the repetitions of this experiment would yield the observed data if \\(\\theta = 0.3\\). Because \\(\\theta = 0.7\\) would yield the observed data more frequently than \\(\\theta = 0.3\\), we say that \\(\\theta = 0.7\\) is more likely to have generated the observed data than \\(\\theta=0.3\\), and our inference favours \\(\\theta =0.7\\). Exercise: What is the probability of observing two heads in two flips if \\(\\theta = 0.7\\)? Suppose you observe two heads and one tails in three flips. Which is more likely to have generated these data, \\(\\theta = 0.3\\) or \\(\\theta = 0.7\\)? 6.1.2 Bayesian Inference: introduction One criticism of the above approach is that is depends not only on the observed data, but also on infinitely many other possible datasets that are not observed. This is an artifact of the manner in which probability is used to represent uncertainty. In contrast, Bayesian statistics represents uncertainty about the value of a parameter directly using probability distributions. In Bayesian inference, a prior distribution is placed on the parameter, representing the probable values of that parameter before data is observed. Having observed the data, the prior is updated via Bayes’ Rule, yielding the posterior distribution of the parameter, given the data. The choice of prior distribution is based either on subject-matter knowledge or mathematical convenience, and is a subjective choice on the part of the analyst. Don’t worry too much about it in this course– you should get comfortable with the idea and the math. We’d talk more about it in a more advanced applied statistics course. But be warned: you may hear criticisms like “there is no way to pick a good prior”. While rooted in valid concern, such criticisms are a bit outdated. Bayesian inference is very useful in modern practice. To see how this works, suppose that we have a pocket full of coins (woohoo!) and we think about \\(4/5\\) coins in our pocket are the \\(\\theta = 0.3\\) coins, and only \\(1/5\\) are the \\(\\theta = 0.7\\) coins. The parameter space here is \\(\\Theta = \\left\\{ 0.3, 0.7\\right\\}\\). Our prior distribution on \\(\\theta\\) is then \\[ P(\\theta = q) = 0.2^{I(q = 0.7)}0.8^{I(q = 0.3)}, \\ q\\in\\Theta \\] where \\(I(q = 0.7) = 1\\) if \\(q = 0.7\\) and \\(0\\) otherwise. This, like the probability distribution of the actual result of the coin toss, just encodes our notion that \\(P(\\theta = 0.3) = 0.8\\) and \\(P(\\theta = 0.7) = 0.2\\). So without knowing the result of the coin toss, we think there is a \\(20\\%\\) chance that \\(\\theta = 0.7\\). We know from above that if we observe heads on the coin toss, we have observed a result that would occur about \\(70\\%\\) of the time if \\(\\theta = 0.7\\). In probability terms, we have a marginal distribution for \\(\\theta\\), and a conditional distribution for \\(X|\\theta\\). These two ideas are combined by computing the conditional distribution of \\(\\theta|X\\), known as the posterior distribution for \\(\\theta\\) having observed \\(X\\). This is obtained (explaining the name) via Bayes’ Rule: \\[ p(\\theta|X) = \\frac{p(X|\\theta)\\times p(\\theta)}{p(X)} \\] where the marginal distribution of \\(X\\), or the normalizing constant or marginal likelihood or model evidence (this thing has a lot of names) is given by \\[ p(X) = \\sum_{\\theta\\in\\Theta}p(X|\\theta)\\times p(\\theta) \\] and ensures \\(p(\\theta|X)\\) is a proper probability distribution. Exercise: verify that \\(p(\\theta|X)\\) is a valid probability density on \\(\\Theta\\) by verifying that \\(\\sum_{\\theta\\in\\Theta}p(\\theta|X) = 1\\). In our example, the prior probability of \\(\\theta = 0.7\\) is only \\(20\\%\\). But we flip the coin and observe \\(X = 1\\). We can see how this observation updates our belief about the likely values of \\(\\theta\\) by computing the posterior distribution of \\(\\theta\\) given the observed data: \\[ \\begin{aligned} &amp;p(\\theta|X) = \\frac{\\theta^{x}(1-\\theta)^{1-x}\\times 0.2^{I(\\theta = 0.7)}0.8^{I(\\theta = 0.3)}}{\\sum_{\\theta = 0.3,0.7}\\theta^{x}(1-\\theta)^{1-x}\\times 0.2^{I(\\theta = 0.7)}0.8^{I(\\theta = 0.3)}} \\\\ \\implies&amp; P(\\theta = 0.7 | X = 1) = \\frac{0.7 \\times 0.2}{0.7\\times0.2 + 0.3\\times0.8} \\\\ &amp;= 0.368 \\end{aligned} \\] Before observing heads, we would have thought the \\(\\theta = 0.7\\) coin to be very unlikely, but because the observed data favours \\(\\theta = 0.7\\) more strongly than \\(\\theta = 0.3\\), after observing these data we feel that \\(\\theta = 0.7\\) is more likely than before. 6.1.3 Flipping More Coins Suppose now that we flip \\(n\\) coins, obtaining a dataset \\(X = (X_{1},\\ldots,X_{n})\\) of heads or tails, represented by 0’s and 1’s. If we’re still considering only two candidate values \\(\\theta = 0.7\\) or \\(\\theta = 0.3\\), we may still ask the question “which value is more likely to have generated the observed data?”. We again form the likelihood function for each value of \\(\\theta\\), the relative frequency with which each value of \\(\\theta\\) would have generated the observed sample. Exercise: assuming the tosses are statistically independent: Write down the complete statistical model: identify all random quantities, their probability distributions, and the unknown parameters, Show that the likelihood for the data is: \\[ p(X|\\theta) = \\theta^{\\sum_{i=1}^{n}X_{i}} \\times (1 - \\theta)^{n - \\sum_{i=1}^{n}X_{i}} \\] where \\(\\sum_{i=1}^{n}X_{i}\\) is just the number of heads observed in the sample. We see that any two samples that have the same number of heads will lead to the same inferences about \\(\\theta\\) in this manner. Suppose we throw the coin \\(10\\) times and observe \\(6\\) heads. The likelihood function for each candidate value of \\(\\theta\\) is \\[ \\begin{aligned} p(X|\\theta = 0.7) &amp;= 0.7^{6} \\times 0.3^{4} = 0.000953 \\\\ p(X|\\theta = 0.3) &amp;= 0.3^{6} \\times 0.7^{4} = 0.000175 \\\\ \\end{aligned} \\] It is much more likely to observe \\(6\\) heads when \\(\\theta = 0.7\\) than when \\(\\theta = 0.3\\). Remark: it doesn’t look very “likely” to observe these data using either value of \\(\\theta\\)! It turns out the actual value of the likelihood for a particular \\(\\theta\\) holds little meaning: what is relevant is the relative values for different \\(\\theta\\)’s. In this example, \\(\\theta = 0.7\\) is \\(0.000953 / 0.000175 = 5.45\\) times more likely to have generated the observed data than \\(\\theta = 0.3\\). Exercise: calculate the likelihood function for \\(\\theta\\) when \\(n = 100\\) and \\(\\sum_{i=1}^{n}X_{i} = 60\\). How much more likely is \\(\\theta = 0.7\\) than \\(\\theta = 0.3\\) now? In the Bayesian setting, with our prior distribution on \\(\\theta\\) from above, we would form the posterior distribution as follows: \\[ p(\\theta|X) = \\frac{\\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}}\\times 0.2^{I(\\theta = 0.7)}0.8^{I(\\theta = 0.3)}}{\\sum_{\\theta = 0.3,0.7}\\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}}\\times 0.2^{I(\\theta = 0.7)}0.8^{I(\\theta = 0.3)}} \\] Computing this for our observed data of \\(\\sum_{i=1}^{10}x_{i} = 6\\) yields \\[ \\begin{aligned} p(\\theta = 0.7 |X) = \\frac{0.7^{6}0.3^{4}\\times 0.2}{0.7^{6}0.3^{4}\\times 0.2 + 0.3^{6}0.7^4\\times0.8} = 0.576 \\\\ p(\\theta = 0.3 |X) = \\frac{0.3^{6}0.7^{4}\\times 0.2}{0.3^{6}0.7^{4}\\times 0.8 + 0.7^{6}0.3^4\\times0.2} = 0.424 \\\\ \\end{aligned} \\] We can see that the data “updates” our prior belief that \\(\\theta = 0.3\\) was more probable than \\(\\theta = 0.7\\), because the observed data was more likely to have occurred if \\(\\theta = 0.7\\) than if \\(\\theta = 0.3\\). Exercise: compute the posterior distribution with \\(n = 100\\) and \\(\\sum_{i=1}^{100}x_{i} = 60\\). 6.1.4 Visualization It is helpful to visualize the prior and posterior, for the observed data. Because both prior and posterior only allow two values, we can do this using a simple bar chart: visualize_binomial_priorposterior &lt;- function(sumx,n) { prior &lt;- function(theta) { if (theta == .3) { return(.8) } else if (theta == .7) { return(.2) } 0 } likelihood &lt;- function(theta) theta^sumx * (1-theta)^(n - sumx) marginal_likelihood &lt;- prior(.7) * likelihood(.7) + prior(.3) * likelihood(.3) posterior &lt;- function(theta) likelihood(theta) * prior(theta) / marginal_likelihood # Plot of the prior and posterior distributions for these observed data tibble( theta = c(.3,.7,.3,.7), value = c(prior(.3),prior(.7),posterior(.3),posterior(.7)), type = c(&quot;Prior&quot;,&quot;Prior&quot;,&quot;Posterior&quot;,&quot;Posterior&quot;) ) %&gt;% ggplot(aes(x = theta,y = value,fill = type)) + theme_classic() + geom_bar(stat = &quot;identity&quot;,position = &quot;dodge&quot;,colour = &quot;black&quot;) + labs(title = &quot;Prior and Posterior for theta&quot;, subtitle = str_c(&quot;Observed data: &quot;,sumx,&quot; flips in &quot;,n,&quot; throws&quot;), x = &quot;Theta, probability of heads&quot;, y = &quot;Prior/Posterior Probability&quot;, fill = &quot;&quot;) + scale_x_continuous(breaks = c(0.30,0.70),labels = c(&quot;0.30&quot;,&quot;0.70&quot;)) + scale_y_continuous(labels = scales::percent_format()) + scale_fill_brewer(palette = &quot;Reds&quot;) } Plotting is nice as it lets us compare how different observed data, and different experiments (number of throws) affect the prior/posterior balance of belief: # library(patchwork) (visualize_binomial_priorposterior(6,6) | visualize_binomial_priorposterior(6,10)) / (visualize_binomial_priorposterior(6,20) | visualize_binomial_priorposterior(6,50)) / (visualize_binomial_priorposterior(0,10) | visualize_binomial_priorposterior(1,10)) / (visualize_binomial_priorposterior(7,10) | visualize_binomial_priorposterior(10,10)) 6.2 Interactive App Go to the app: http://shiny.sta220.utstat.utoronto.ca:88/BayesianApp/ The app lets you flip coins and estimate the probability of heads using Frequentist and Bayesian methods. We haven’t covered estimation yet, but we have covered the model for coin flipping in both contexts now, so you should be able to tell what’s happening. Also shown are interval estimates, which measure the strength of the conclusions about \\(p\\) that are made based on the data and model. Narrower interval estimates mean we’re more sure about the value of \\(p\\), after seeing the data. The app lets you change the following: The number of times you flip the coin, The true probability of heads, \\(p\\), Your prior belief about the probability of heads, the “prior mean”, and The strength of your prior beliefs, as measured by the prior standard deviation. Lower standard deviation means you’re more sure about the value of \\(p\\), before seeing any flips. You should answer the following questions: How many flips do you need before the Bayesian and frequentist inferences agree closely? Does this depend on the true value of \\(p\\), your prior belief, and the strength of your prior belief? Intuitively: why are the Bayesian interval estimates narrower than the frequentist ones? Is this always the case? Can you “break” the Bayesian answer by expressing really strong and wrong prior beliefs? Can you “fix” it by flipping the coin more times? "],["section-the-bootstrap.html", "Chapter 7 The Bootstrap 7.1 The Bootstrap (Chapter 18)", " Chapter 7 The Bootstrap This chapter introduces the Bootstrap and shows how you can use simulation to enhance your statistical thinking. We use datasets from MIPS which can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. 7.1 The Bootstrap (Chapter 18) The bootstrap is one of the most foundational computational tools in modern statistics. It as important on the computation side as basic calculus is on the math side. Any time you have an estimator and are interested in its repeated sampling properties (which is much of what statistics is), you can simulate data, compute the estimator, and just look at the results to get an idea of what’s going on. This is called the “parametric” bootstrap, and you have been doing it for the whole course in these supplementary notes. This chapter introduces the “empirical” bootstrap, which allows you to use this idea when you don’t know the distribution of the data! The idea is to sample with replacement from the data that you have. A lot of the statistical properties of estimators are still recoverable under this type of sampling. It seems like magic. Indeed, the name “bootstrap” comes from the old expression “lift yourself up by your bootstraps”, which roughly means to do something seemingly impossible or contradictory in order to better your situation. One of the best-named concepts in all of stats, in my opinion. 7.1.1 Empirical bootstrap: Old Faithful data Recall the Old Faithful data: \\(n = 272\\) measurements of the waiting time to eruption \\(X_{i}\\) of the Old Faithful Geyser. A statistical model for these data was given as \\[ X_{i} \\overset{iid}{\\sim}F_{\\mu}, i = 1,\\ldots,n \\] where \\(\\mu = E(X_{i})\\) is the unknown parameter to be estimated. We can estimate \\(\\mu\\) using the sample mean: \\(\\hat{\\mu} = \\bar{X}\\) which for these data gives \\(\\hat{\\mu} = 209.27\\). But is this the whole story? No. The data are random: if we repeated this experiment we would get different measurements, and a different value for \\(\\hat{\\mu}\\). So \\(\\hat{\\mu}\\) is random too, because it depends on the data, which are random. So \\(\\hat{\\mu}\\) has a probability distribution, called its sampling distribution, and we can quantify uncertainty in our estimate for \\(\\hat{\\mu}\\) by analyzing this distribution. Or we could, if we knew what it was! For this simple example, we actually do know the sampling distribution of \\(\\hat{\\mu}\\): by the CLT, it’s \\[\\hat{\\mu} \\overset{approx}{\\sim}\\text{N}\\left(\\mu,\\sigma/\\sqrt{n}\\right)\\] where \\(\\mu = E(X_{i})\\) and \\(\\sigma^{2} = \\text{Var}(X_{i})\\). In more complicated problems, we wouldn’t know this. In fact, it’s almost never known. So how can the notion of using the sampling distribution to assess uncertainty in an estimate be useful in practice? This is where the bootstrap comes in. The empirical bootstrap refers to the following algorithm: Input: sample \\(X_{1},\\ldots,X_{n}\\), estimate \\(\\hat{\\mu}\\), number of desired bootstrap replicates \\(B\\), For: \\(b = 1,\\ldots,B\\), do: Take a bootstrap sample \\(X^{*}_{1},\\ldots,X^{*}_{n}\\) independently with replacement from \\(X_{1},\\ldots,X_{n}\\), Compute the bootstrap estimate \\(\\hat{\\mu}^{*}_{b}\\) using the bootstrap sample, Output: \\(\\hat{\\mu}^{*}_{1},\\ldots,\\hat{\\mu}^{*}_{B}\\), a sample from the sampling distribution of \\(\\hat{\\mu}\\). This seems like magic: we got a sample from the sampling distribution of our estimator, without knowing anything about it mathematically. Let’s do this in code: library(tidyverse) library(patchwork) # Read in old faithful, copied from chapter 1: oldfaithful &lt;- readr::read_csv( file = &quot;data/MIPSdata/oldfaithful.txt&quot;, # Tell it where the file is col_names = &quot;time&quot;, # Tell it that the first row is NOT column names, and at the same time, tell it what name you want for the column. col_types = &quot;n&quot; # Tell it that there is one column, and it is &quot;numeric&quot; (n) ) # Check what was read in using the dplyr::glimpse() function dplyr::glimpse(oldfaithful) Observations: 272 Variables: 1 $ time &lt;dbl&gt; 216, 108, 200, 137, 272, 173, 282, 216, 117, 261, 110, 235, 252,… # Bootstrap: sample with replacement set.seed(45356) # So you can reproduce my results B &lt;- 1000 # Number of bootstrap resamples to take n &lt;- nrow(oldfaithful) # Sample size bootstrapmeans &lt;- numeric(B) for (b in 1:B) { # Draw a bootstrap sample bootstrapsample &lt;- sample(oldfaithful$time,n,replace = TRUE) # Compute the bootstrap estimate bootstrapmeans[b] &lt;- mean(bootstrapsample) } # Plot a hisrogram and density estimate, along with # the normal approximation tibble(x = bootstrapmeans) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 20,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + geom_density() + stat_function(fun = dnorm,args = list(mean = normalapproxmean,sd = normalapproxsd),col = &quot;red&quot;) + scale_x_continuous(breaks = seq(190,230,by=5)) The histogram and black kernel density estimate curve are obtained using the empirical bootstrap algorithm. The red curve is obtained using the CLT approximation to the sampling distribution of \\(\\hat{\\mu}\\). Exercise: compute the mean and standard deviation that I secretly computed to fit that normal approximation. What are the values normalapproxmean and normalapproxsd? An even more remarkable use of the empirical bootstrap is as follows. Suppose we are interested in the error in our estimate: \\[ \\hat{\\mu} - \\mu \\] This is a random variable with an unknown sampling distribution, and further, it depends on the unknown parameter \\(\\mu\\). We can’t even compute the value of this random variable from our sample! But, we can use the empirical bootstrap to get its sampling distribution. Consider the following: Input: sample \\(X_{1},\\ldots,X_{n}\\), estimate \\(\\hat{\\mu}\\), number of desired bootstrap replicates \\(B\\), For: \\(b = 1,\\ldots,B\\), do: Take a bootstrap sample \\(X^{*}_{1},\\ldots,X^{*}_{n}\\) independently with replacement from \\(X_{1},\\ldots,X_{n}\\), Compute the bootstrap estimate \\(\\hat{\\mu}^{*}_{b}\\) using the bootstrap sample, Compute the bootstrap estimate \\((\\hat{\\mu} - \\mu)^{*}_{b} = \\hat{\\mu}^{*}_{b} - \\hat{\\mu}\\) using the bootstrap sample, Output: \\((\\hat{\\mu} - \\mu)^{*}_{1},\\ldots,(\\hat{\\mu} - \\mu)^{*}_{B}\\), a sample from the sampling distribution of \\((\\hat{\\mu} - \\mu)\\). Notice that we compute \\((\\hat{\\mu} - \\mu)^{*}_{b}\\) without knowing \\(\\mu\\). We can do this in code as follows: # Bootstrap: sample with replacement set.seed(45356) # So you can reproduce my results B &lt;- 1000 # Number of bootstrap resamples to take n &lt;- nrow(oldfaithful) # Sample size bootstraperrors &lt;- numeric(B) for (b in 1:B) { # Draw a bootstrap sample bootstrapsample &lt;- sample(oldfaithful$time,n,replace = TRUE) # Compute the bootstrap mean bootstrapmean &lt;- mean(bootstrapsample) # Compute the bootstrap estimate bootstraperrors[b] &lt;- bootstrapmean - mean(oldfaithful$time) } # For the record, we could have just done # bootstraperrors &lt;- bootstrapmeans - mean(oldfaithful$time) # Plot a hisrogram and density estimate, along with # the normal approximation tibble(x = bootstraperrors) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 20,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + geom_density() + stat_function(fun = dnorm,args = list(mean = normalapproxmeanerror,sd = normalapproxsderror),col = &quot;red&quot;) + scale_x_continuous(breaks = seq(-14,14,by=2)) Exercise: again, how did I get the values of normalapproxmeanerror and normalapproxsderror? Exercise: compute an estimate of \\(P(|\\bar{X}_{n} - \\mu| &gt; 5)\\). Using the above set.seed(45356) I got the following: [1] 0.22 Hint: consider the sample proportion of \\(\\bar{X}_{n} - \\mu\\) that have absolute value greater than \\(5\\). Exercise: even this \\(P(|\\bar{X}_{n} - \\mu| &gt; 5)\\) is just an estimate, and we can use the bootstrap to sample from its sampling distribution to quantify uncertainty in it. Now, repeat the entire bootstrap procedure \\(1000\\) times and plot a histogram and kernel density estimate of \\(P(|\\bar{X}_{n} - \\mu| &gt; 5)\\). I got the following with set.seed(8768432) and \\(200\\) bootstrap replicates: So it looks like we can be pretty sure the absolute error won’t be more than \\(0.27\\). 7.1.2 Parametric Bootstrap: software data The empirical bootstrap is used to sample from the sampling distribution of an estimator without knowing the distribution of the data. The parametric bootstrap is used to sample from the sampling distribution of an estimator when you know the family of distributions \\(F_{\\theta}\\) (like “Exponential” or “Normal” or “Poisson”) from which the data is drawn, but you don’t know the value of the parameter \\(\\theta\\). Recall the **software data*: head data/MIPSdata/software.txt wc -l data/MIPSdata/software.txt 30 113 81 115 9 2 91 112 15 138 136 data/MIPSdata/software.txt software &lt;- readr::read_csv( file = &quot;data/MIPSdata/software.txt&quot;, col_names = &quot;time&quot;, col_types = &quot;n&quot; ) glimpse(software) Observations: 136 Variables: 1 $ time &lt;dbl&gt; 30, 113, 81, 115, 9, 2, 91, 112, 15, 138, 50, 77, 24, 108, 88, 6… One possible model for these data is \\(X_{i}\\overset{iid}{\\sim}\\text{Exponential}(\\lambda)\\) with \\(\\lambda\\) being the rate of failures of the software: \\(E(X_{i}) = \\lambda^{-1}\\). We can estimate \\(\\hat{\\lambda} = \\bar{X}^{-1}\\) and plot the CDF \\(F_{\\hat{\\theta}}\\) of the estimated \\(\\text{Exponential}(\\hat{\\lambda})\\) distribution against the empirical CDF \\(\\hat{F}_{n}\\) of the data. Of interest is the maximum possible error in estimating the CDF: \\(T_{ks} = \\text{sup}_{x\\in\\mathbb{R}}|F_{\\hat{\\theta}}(x) - \\hat{F}_{n}(x)|\\). The “KS” stands for “Kolmogorov-Smirnov”, who used this error in some statistical theory in the early 20th century, and so \\(T_{ks}\\) is sometimes called the “Kolmogorov-Smirnov distance”. It’s also sometimes called the “Total Variation” distance. It is the length of the red segment in the following plot: lambdahat &lt;- 1/mean(software$time) samppoints &lt;- seq(0,3000,by=.1) expcdf &lt;- pexp(samppoints,rate = lambdahat) empcdf &lt;- ecdf(software$time) tksestimate &lt;- max(abs(expcdf - empcdf(samppoints))) whereisthetks &lt;- which.max(abs(expcdf - empcdf(samppoints))) tibble(x = samppoints,y = expcdf) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_line() + stat_function(fun = empcdf,linetype = &quot;dotdash&quot;) + geom_segment(aes(x = samppoints[whereisthetks],y = expcdf[whereisthetks],xend = samppoints[whereisthetks],yend = empcdf(samppoints[whereisthetks])),colour = &#39;red&#39;) What a complicated statistic! The statistic \\(T_{ks}\\) is a random variable because it depends on the data through the statistic \\(\\hat{F}_{n}\\). We can use the parametric bootstrap to sample from its sampling distribution. Consider the following algorithm: Input: sample \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}F_{\\lambda}\\) with \\(F\\) known and \\(\\theta\\) unknown, estimate \\(\\hat{\\lambda}\\), number of desired bootstrap replicates \\(B\\), For: \\(b = 1,\\ldots,B\\), do: Take a bootstrap sample \\(X^{*}_{1},\\ldots,X^{*}_{n}\\overset{iid}{\\sim}F_{\\hat{\\lambda}}\\) Compute the bootstrap estimate \\(\\hat{\\lambda}^{*}_{b}\\) using the bootstrap sample, Output: \\(\\hat{\\lambda}^{*}_{1},\\ldots,\\hat{\\lambda}^{*}_{B}\\), a sample from the sampling distribution of \\(\\hat{\\lambda}^{*}\\). Let’s use this algorithm to get a bootstrap sample from the sampling distribution of \\(T_{ks}\\): set.seed(97876856) compute_tks &lt;- function() { samppoints &lt;- seq(0,3000,by=.1) n &lt;- nrow(software) samp &lt;- rexp(n,rate = lambdahat) ecdfboot &lt;- ecdf(samp)(samppoints) expcdf &lt;- pexp(samppoints,rate = lambdahat) max(abs(ecdfboot - expcdf)) } B &lt;- 1000 tksboot &lt;- numeric(B) for (b in 1:B) tksboot[b] &lt;- compute_tks() tibble(x = tksboot) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 20,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + geom_density() + scale_x_continuous(breaks = c(seq(0,.2,by=.05),.176)) + coord_cartesian(xlim = c(0,.2)) Exercise: the above compute_tks() is a bit tricky. Write out the exact algorithm I use to compute \\(T_{ks}\\). Basically I compute both the ECDF and the parametric estimated CDF at a really fine grid of points and then take the max difference– this question is asking you to formalize this into an algorithm (sequence of steps) using mathematical notation, like I’ve done for the bootstraps. 7.1.3 Extended example: the standard error of a proportion Dozens of news articles are published every day with claims that “xx% of Canadians say that yy”, where xx is some percentage and yy is some claim that the news company wants to make. These news articles most often only report point estimates– the “xx%”. They don’t tend to report the standard error of these estimates, which give you a measure of the variability, and hence uncertainty, in the conclusions that are made based off of them. We can use the bootstrap to get an idea of the variability in estimates based off of surveys. Because most news articles don’t link to the raw data, we’ll use a general budget survey from the Government of Canada, with responses from Alberta residents. You can get the data in excel format here. You should open the file in excel and save it as a .csv, or if you don’t have excel, I put the .csv file I used in the data folder. Let’s read the data into R. We know it has a header row (because we opened it in excel). There is a numeric ID column, two date columns (which we don’t need), and 58 numeric response columns. I removed the first three (blank) and fourth (information) rows from my .csv, and you should do the same in excel if you have it. col_types &lt;- c( &quot;ncc&quot;, stringr::str_c(rep(&quot;n&quot;,58),collapse = &quot;&quot;) ) %&gt;% stringr::str_c(collapse = &quot;&quot;) col_types # Understand the above code by running it in pieces and looking at the results. [1] &quot;nccnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn&quot; budget &lt;- readr::read_csv( file = &quot;data/budget.csv&quot;, col_names = TRUE, col_types = col_types ) Warning: Missing column names filled in: &#39;X17&#39; [17], &#39;X18&#39; [18], &#39;X19&#39; [19], &#39;X20&#39; [20], &#39;X21&#39; [21], &#39;X22&#39; [22], &#39;X23&#39; [23], &#39;X24&#39; [24], &#39;X26&#39; [26], &#39;X27&#39; [27], &#39;X28&#39; [28], &#39;X29&#39; [29], &#39;X30&#39; [30], &#39;X31&#39; [31], &#39;X33&#39; [33], &#39;X34&#39; [34], &#39;X35&#39; [35], &#39;X36&#39; [36], &#39;X37&#39; [37], &#39;X38&#39; [38], &#39;X39&#39; [39], &#39;X40&#39; [40], &#39;X41&#39; [41], &#39;X42&#39; [42], &#39;X44&#39; [44], &#39;X45&#39; [45], &#39;X46&#39; [46], &#39;X47&#39; [47], &#39;X48&#39; [48], &#39;X49&#39; [49], &#39;X50&#39; [50], &#39;X51&#39; [51], &#39;X52&#39; [52], &#39;X53&#39; [53], &#39;X55&#39; [55], &#39;X56&#39; [56], &#39;X57&#39; [57], &#39;X58&#39; [58], &#39;X59&#39; [59] glimpse(budget) Observations: 40,513 Variables: 61 $ `Respond-ent ID` &lt;dbl&gt; … $ StartDate &lt;chr&gt; … $ CompletedDate &lt;chr&gt; … $ `Q1: To what extent do you think low oil prices impact the Alberta government&#39;s ability to budget?` &lt;dbl&gt; … $ `Q2: How concerned are you about the $7B shortfall?` &lt;dbl&gt; … $ `Q3: Should government take action now, wait six months, or hold out for high oil prices?` &lt;dbl&gt; … $ `Q4: Government needs:` &lt;dbl&gt; … $ `Question 5 a) Cut spending` &lt;dbl&gt; … $ `Question 5 b) Raise taxes and user fees` &lt;dbl&gt; … $ `Question 5 c) Borrow money` &lt;dbl&gt; … $ `Question 6 a) Reduce expenditures` &lt;dbl&gt; … $ `Question 6 b) Raise taxes and user fees` &lt;dbl&gt; … $ `Question 6 c) Run a deficit` &lt;dbl&gt; … $ `Q7: Are current tax rates in Alberta higher, above average or lower?` &lt;dbl&gt; … $ `Q8: How important is it that AB taxes are lower?` &lt;dbl&gt; … $ `Q9: In what ways can the government act to increase revenue without jeopardizing Alberta&#39;s competitive position? (1 = selected; 0 = not selected)` &lt;dbl&gt; … $ X17 &lt;dbl&gt; … $ X18 &lt;dbl&gt; … $ X19 &lt;dbl&gt; … $ X20 &lt;dbl&gt; … $ X21 &lt;dbl&gt; … $ X22 &lt;dbl&gt; … $ X23 &lt;dbl&gt; … $ X24 &lt;dbl&gt; … $ `Q10: If the government needs to increase its revenues through taxation, are there options you feel should NOT be considered? (1 = selected; 0 = not selected)` &lt;dbl&gt; … $ X26 &lt;dbl&gt; … $ X27 &lt;dbl&gt; … $ X28 &lt;dbl&gt; … $ X29 &lt;dbl&gt; … $ X30 &lt;dbl&gt; … $ X31 &lt;dbl&gt; … $ `Q11: Where would you tolerate cuts? (1 = Selected; 0 = Not Selected)` &lt;dbl&gt; … $ X33 &lt;dbl&gt; … $ X34 &lt;dbl&gt; … $ X35 &lt;dbl&gt; … $ X36 &lt;dbl&gt; … $ X37 &lt;dbl&gt; … $ X38 &lt;dbl&gt; … $ X39 &lt;dbl&gt; … $ X40 &lt;dbl&gt; … $ X41 &lt;dbl&gt; … $ X42 &lt;dbl&gt; … $ `Q12: And are there options you feel should NOT be touched? (1 = Selected; 0 = Not selected)` &lt;dbl&gt; … $ X44 &lt;dbl&gt; … $ X45 &lt;dbl&gt; … $ X46 &lt;dbl&gt; … $ X47 &lt;dbl&gt; … $ X48 &lt;dbl&gt; … $ X49 &lt;dbl&gt; … $ X50 &lt;dbl&gt; … $ X51 &lt;dbl&gt; … $ X52 &lt;dbl&gt; … $ X53 &lt;dbl&gt; … $ `Q13: Indicate whether you Don&#39;t Know, Disagree or Agree. (1 = Don&#39;t know; 2 = Disagree; 3 = Agree)` &lt;dbl&gt; … $ X55 &lt;dbl&gt; … $ X56 &lt;dbl&gt; … $ X57 &lt;dbl&gt; … $ X58 &lt;dbl&gt; … $ X59 &lt;dbl&gt; … $ `Q15: Years you&#39;ve lived in AB` &lt;dbl&gt; … $ `Q16: What part of the province do you live in?` &lt;dbl&gt; … Ugly! There are some missing column names that were in the fourth row. If we needed all these data, we would have to go in excel and manually label the columns correctly, since the analysts at the government didn’t. Look at the column names and start to think about the kind of news headlines they could generate. I like the first one: “Q1: To what extent do you think low oil prices impact the Alberta government’s ability to budget?”. Here, let’s estimate the proportion of Albertans who would answer \\(1\\) (“A Great Deal”) to the above question. Exercise: there are 40513 survey responses. The number who respond 1 can be modelled as a Binomial random variable. Derive and calculate the standard deviation of the sample proportion of respondents who answered 1. I got \\(0.0025\\). Is this higher or lower than you would expect with this number of respondents? set.seed(29394032) origsamp &lt;- as.numeric(budget[[&quot;Q1: To what extent do you think low oil prices impact the Alberta government&#39;s ability to budget?&quot;]] == 1) n &lt;- length(origsamp) # Convert to 0/1 indicator of response == 1 # Sample proportion: phat &lt;- mean(origsamp) phat [1] 0.54 # Standard error (you should DERIVE this formula): sqrt(phat*(1-phat)/n) [1] 0.0025 # So the mean is .5378 and the standard error is about 0.0025 (theoretically). # Bootstrap: repeatedly resample and calculate the mean. The estimated standard error # of the sample mean is then the standard error of these means. # This code is a bit more concise than the previous ones, and I put it in a function. doboot &lt;- function(B) { boot &lt;- numeric(B) for (b in 1:B) boot[b] &lt;- mean(sample(origsamp,n,replace = TRUE)) boot } bootmeans &lt;- doboot(1000) mean(bootmeans) # Pretty close! [1] 0.54 sd(bootmeans) # Not bad at all! [1] 0.0024 tibble(x = bootmeans) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + stat_function(fun = rlang::as_function(~n * dbinom(round(n * .x),size = n,prob = phat)),col = &quot;purple&quot;) + labs(title = &quot;Bootstrap distribution of the sample mean&quot;, subtitle = &quot;purple: theoretical distribution. red: CLT approximation&quot;) Exercise: inside stat_function(fun = rlang::as_function(~n * dbinom(round(n * .x),size = n,prob = phat))) I specify the theoretical density of the sample mean. Derive this formula yourself using the change of variables formula. Exercise: derive an appropriate normal approximation to the binomial distribution for use in this example. Add it to the above plot using another stat_function call, with colour = \"red\" and linetype = \"dotdash\". I got the following: The normal approximation looks pretty good! The sample is really big so this is to be expected. Exercise: suppose that based on this survey, a newspaper claims that “Albertans have given up hope: more than half of Albertans believe their province is unable to budget due to the low price of oil.”. Use the bootstrap to calculate the probability that their claim is true, by considering whether each resampled sample mean is greater than .5. Using set.seed(5647244), and \\(B = 1000\\) resamples, I got the following: [1] 1 What do you think about this? Is the newspaper’s claim substantiated by the data? Exercise: surveys usually aren’t this big. Repeat the above calculation but on a subset of only \\(n = 200\\) people. What do you get now? Use newsample &lt;- sample(origsamp,200,replace = FALSE) and then repeate the bootstrapping on the newsample. Use the same random seed. [1] 0.69 "],["section-maximum-likelihood.html", "Chapter 8 Maximum Likelihood 8.1 Maximum Likelihood (Chapter 21) 8.2 Extended example: rental housing in Toronto", " Chapter 8 Maximum Likelihood This chapter introduces maximum likelihood. This chapter uses some data from MIPS, which can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. library(tidyverse) library(patchwork) 8.1 Maximum Likelihood (Chapter 21) The concept of Maximum Likelihood, and the Likelihood function itself, is one of the single most important concepts in all of statistics. You need to understand this concept. I find a simulation is helpful. 8.1.1 Example: two coins Suppose I have two coins in my pocket. One of them has probability of heads \\(p_{1} = 0.7\\) and the other has probability of heads \\(p_{2} = 0.4\\). I pull one out and hand it to you. Your task is to guess which coin it is. You are only allowed to flip it once. To make this concrete, you flip the coin once and observe one realization of a random variable \\(X\\), which equals \\(1\\) (heads) or \\(0\\) (tails) with probability given either by \\(p = p_{1}\\) or \\(p = p_{2}\\). But you don’t know which one. Your task is to guess (infer) the value of the probability of heads, \\(p\\), based on the data, \\(X\\). The Maximum Likelihood Principle says that you should pick the value of \\(p\\) under which your observed data is the most likely. That means would occur with the highest relative frequency if the data-generating experiment were repeated over and over again. The likelihood function is the probability distribution of the observed data \\(X\\), treated as a function of the unknown parameter \\(p\\). That means for every sample you get, you get a different likelihood function. It’s always a function of \\(p\\), but it’s a different function of \\(p\\) for different observed data \\(X\\). The likelihood function is (hence) a random function of the unknown parameter. We observe a sample, get a likelihood function, and use that likelihood function to estimate and quantify uncertainty in the value of the unknown parameter. In our example, our likelihood functions for \\(X=1\\) and \\(X=0\\) are functions from the set \\(\\{p_{1},p_{2}\\}\\mapsto\\mathbb{R}\\), i.e. they are only defined at the two points \\(p = p_{1}\\) and \\(p = p_{2}\\). The distribution of \\(X\\) is \\(\\text{Bernoulli}(p)\\). Suppose \\(X=1\\). The likelihood function \\(L(p)\\) is then defined by \\[\\begin{equation} L(p_{1};x = 1) = 0.7, \\ L(p_{2}; x = 1) = 0.4 \\end{equation}\\] Exercise: derive the likelihood function for tails, \\(L(p;x = 0)\\). The likelihood function \\(L(p;x)\\) is the relative frequency with which the observed value \\(X = x\\) would be observed in repeated sampling at that value of the parameter \\(p\\). This is kind of a mouthful. Because the likelihood function is defined in terms of a relative frequency, we can write a simulation that illustrates it. Since The likelihood function \\(L(p;x)\\) is the relative frequency with which the observed value \\(X = x\\) would be observed in repeated sampling at that value of the parameter \\(p\\) I propose that a good way to simulate the likelihood function is to Choose a value of \\(p\\), Simulate a bunch of data from a \\(\\text{Bernoulli}(p)\\) distribution, and See how often the simulated data equals our observed data. Let’s not worry too much about whether this is going to work in every example… for this example, I think it will work. I will do this for \\(X = 1\\) and then you can do it for \\(X = 0\\): # Simulation to recreate the likelihood function for the two-coin example. # The likelihood function is the relative frequency with which your sample # would occur in repeated sampling, for a particular value of the parameter. # # Suppose you flip the coin and get heads. The likelihood function at p1 = 0.7 # is the relative frequency with which you would get heads if you flipped the coin # over and over, if p really equalled 0.7. # # The likelihood function at p1 = 0.4 is the relative frequency with which you # would get heads if you flipped the coin over and over, if p really equalled 0.4. # # Let&#39;s do it: simulate_likelihood_x1 &lt;- function(p) { # Make sure p is in the range of possible values if (!(p %in% c(.7,.4))) stop(stringr::str_c(&quot;Wrong value for p. p should be .7 or .4. You gave p = &quot;,p)) # Sample the data repeatedly according to this p N &lt;- 1000 repeatedsampling &lt;- sample(c(1,0),N,replace = TRUE,prob = c(p,1-p)) # Sample 1/0 with prob p/(1-p), N times # Return the relative frequency with which x == 1 mean(repeatedsampling == 1) } # Check it out: set.seed(478032749) simulate_likelihood_x1(.7) # Should be around .7 [1] 0.69 simulate_likelihood_x1(.4) # Should be around .4 [1] 0.39 Exercise: write a simulation for the likelihood function for \\(x = 0\\). Call it simulate_likelihood_x0. You should get the following: set.seed(8907968) simulate_likelihood_x0(.7) [1] 0.3 simulate_likelihood_x0(.4) [1] 0.61 8.1.2 Example: unknown coins, \\(n = 2\\) The two-coin example illustrates that ML (Maximum Likelihood) follows human intuition: if I tell you to guess which of two coins I flipped based on the result of one flip, you’re going to try and maximize your chances of being right. You do this by choosing the coin that is most likely to give the result you observed. The likelihood function was popularized by R.A. Fisher in his 1922 paper, On the Mathematical Foundations of Theoretical Statistics. What he actually says is as follows: We must return to the actual fact that one value of \\(p\\), of the frequency of which we know nothing, would yield the observed result three times as frequently as would another value of \\(p\\). If we need a word to characterize this relative property of different values of \\(p\\), I suggest without confusion that we may speak of the likelihood of one value of \\(p\\) being thrice the likelihood of another, bearing in mind that likelihood here is not used loosely as a synonym of probability, but simply to express the relative frequencies with which such values of the hypothetical quantity \\(p\\) would in fact yield the observed sample. I like this quote, even though the language at that time was less direct than we’re used to now, I think Fisher explains the concept better than any modern textbook I’ve read. The two-coin example is a bit simple, but it is the most intuitive. Let’s extend it to the more realistic case where the parameter \\(p\\) can be any value in the open interval \\((0,1)\\). That is, I pull a coin out of my pocket, and you flip it once and have to tell me what you think the probability of heads is. Exercise: show that the likelihood function here for \\(X = 1\\) and \\(X = 0\\) is \\[\\begin{equation} L(p;x = 1) = p, \\ L(p;x = 0) = 1 - p \\end{equation}\\] or more generally, \\[\\begin{equation} L(p;x) = p^x (1-p)^{1-x} \\end{equation}\\] How do we use the observed data to estimate \\(p\\)? We find the value of \\(p\\) which would generate the sample we saw with the highest relative frequency. We maximize the likelihood function, which gives the maximum likelihood estimator \\(\\hat{p}\\). Let’s do this for this example. The log-likelihood is \\[\\begin{equation} \\ell(p;x) = \\log L(p;x) = x\\log p + (1-x)\\log (1-p) \\end{equation}\\] Exercise: show that the unique global maximum of \\(\\ell(p)\\) on the interval \\([0,1]\\) is \\(p = x\\). Why did I use a closed interval here and an open one above? There’s a problem: flipping the coin only once doesn’t really give us enough information to accurately estimate the probability of heads. If you get heads, your best guess is intuitively just going to be \\(p = 1\\)! We can see this by plotting the likelihood function: likelihood &lt;- function(p,x) (p^x) * (1-p)^(1-x) baseplot &lt;- tibble(x = c(0,1)) %&gt;% ggplot(aes(x = x)) + theme_classic() + labs(x = &quot;p&quot;,y = &quot;Likelihood&quot;) leftplot &lt;- baseplot + stat_function(fun = likelihood,args = list(x = 1)) + labs(title = &quot;x = 1&quot;) rightplot &lt;- baseplot + stat_function(fun = likelihood,args = list(x = 0)) + labs(title = &quot;x = 0&quot;) leftplot | rightplot The likelihood is maximized on the closed interval \\([0,1]\\) at \\(p = 1\\) when \\(x = 1\\) and \\(p = 0\\) when \\(x = 0\\). This is the mathematical representation of the idea that if the coin is heads, my best guess at the relative frequency of heads is simply that the coin is always heads, because well, I’ve never seen it come up tails! To get better inferences, we need to flip the coin more than once. Suppose I flip the coin \\(n\\) times and observe independent realizations of the random variable \\(X\\) which takes values \\(1\\) with probability \\(p\\) and \\(0\\) with probability \\(1 - p\\). I call these random variables \\(Y = (X_{1},\\ldots,X_{n})\\) and I denote their observed values by \\(y = (x_{1},\\ldots,x_{n})\\). So for example if \\(n = 2\\) then my random variable is \\(X = (X_{1},X_{2})\\) and if I observed a head and a tail (in that order), my realized values would be \\(y = (1,0)\\). Exercise: show that the likelihood function for the observed sample \\(y = (x_{1},\\ldots,x_{n})\\) is \\[\\begin{equation} L(p;y) = p^{\\sum_{i=1}^{n}x_{i}}(1-p)^{n - \\sum_{i=1}^{n}x_{i}} \\end{equation}\\] and the log likelihood is \\[\\begin{equation} \\ell(p;y) = \\sum_{i=1}^{n}x_{i}\\log p + \\left(n - \\sum_{i=1}^{n}x_{i}\\right)\\log (1-p) \\end{equation}\\] Use the log-likelihood to show that the maximum likelihood estimator \\(\\hat{p}\\) is \\[\\begin{equation} \\hat{p} = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i} \\end{equation}\\] Suppose now that \\(n = 2\\). There are four possible samples we could get: \\[\\begin{equation}\\begin{aligned} y_{1} &amp;= (0,0) \\\\ y_{2} &amp;= (1,0) \\\\ y_{3} &amp;= (0,1) \\\\ y_{4} &amp;= (1,1) \\\\ \\end{aligned}\\end{equation}\\] The likelihood functions for each of these are \\[\\begin{equation}\\begin{aligned} L(p;y_{1}) &amp;= (1-p)^{2} \\\\ L(p;y_{2}) &amp;= p(1-p) \\\\ L(p;y_{3}) &amp;= p(1-p) \\\\ L(p;y_{2}) &amp;= p^{2} \\\\ \\end{aligned}\\end{equation}\\] We can plot these four (three?) likelihood functions as follows. I’m using some more advanced code here; you should run it slowly, line-by-line, and get a feel for what’s happening. # Store the four samples in a (named) list ylist &lt;- list( &quot;y1&quot; = c(0,0), &quot;y2&quot; = c(1,0), &quot;y3&quot; = c(0,1), &quot;y4&quot; = c(1,1) ) likelihood &lt;- function(p,y) p^(sum(y)) * (1-p)^(sum(1-y)) makeplot &lt;- function(y) { # y = c(0,0) or c(0,1) or c(1,0) or c(1,1) plotname &lt;- as.character(y) %&gt;% stringr::str_c(collapse = &quot;,&quot;) baseplot + # baseplot was made above stat_function(fun = likelihood,args = list(y = y)) + labs(title = stringr::str_c(&quot;y = &quot;,plotname)) } ( makeplot(c(0,0)) | makeplot(c(0,1)) ) / ( makeplot(c(1,0)) | makeplot(c(1,1)) ) Exercise: compute the value of \\(\\hat{p}\\) for each sample. Add a verticle line to each plot at this point, using geom_vline(xintercept = ?,colour = \"red\",linetype = \"dotdash\") where you replace the ? with the appropriate value. It should look like: 8.1.3 Example: unknown coins, \\(n\\) bigger than \\(2\\) For our last example, let’s flip the coin more times and look just at the likelihood function for our sample. You can flip a coin with probability of heads p n times in R by using sample(c(1,0),n,replace = TRUE,prob = c(p,1-p)). You’re going to write a function that takes in \\(n\\), generates a sample of size \\(n\\), and plots the likelihood for \\(p\\) for this sample. To do this, you need to generate the sample with a known value of \\(p\\) which we will call \\(p_{0}\\). You can then compare how your likelihood and MLE (Maximum Likelihood Estimator) look to the true value of \\(p\\) that you’re trying to estimate, \\(p_{0}\\). Start with the following: plotlikelihood &lt;- function(n,p0) { # Code goes here } You can generate the sample using the statement above. Call it samp. You can then make the plot using baseplot + stat_function(fun = likelihood,args = list(y = samp)) + labs(title = stringr::str_c(&quot;n = &quot;,n)) + geom_vline(xintercept = p0,colour = &quot;blue&quot;,linetype = &quot;dotdash&quot;) + geom_vline(xintercept = mean(samp),colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) Notice I changed the title too. I also added two verticle lines: one for the true value of \\(p\\), \\(p_{0}\\), and one for the MLE from the sample, \\(\\hat{p}\\). Put these two code chunks together into the above skeleton of a function. Then call your function as follows, and you should get the following output: set.seed(56576) plotlikelihood(10,.5) Exercise: write the function necessary to produce this output. Then with your new function, run it repeatedly, and with different values of \\(n\\) and \\(p\\). Make sure not to reset the random seed each time you run it– you want to see what happens for different samples. Think about the following things: How does the shape and location of the likelihood change as you sample the data over and over, for the same \\(n\\)? Specifically, How does the shape and location of the likelihood change when you make \\(n\\) smaller or larger? How does the shape and location of the likelihood change when you change \\(p_{0}\\)? What happens when you push \\(p_{0}\\) really close to \\(0\\) or \\(1\\)? 8.2 Extended example: rental housing in Toronto Recall the Toronto Rental Housing dataset from Section 2.5 of these supplementary materials. We were interested in estimating the quality of rental housing for the different wards in Toronto, where “quality” is measured by the RentSafeTO score, which is composed of 20 sub-scores on various qualities of each building. We did this by computing the sample mean score in each ward, as follows: # Read in the data from disk apartmentdata &lt;- readr::read_csv( file = &quot;data/apartment-data/toronto-apartment-building-evaluations.csv&quot; ) Parsed with column specification: cols( .default = col_double(), EVALUATION_COMPLETED_ON = col_character(), PROPERTY_TYPE = col_character(), RESULTS_OF_SCORE = col_character(), SITE_ADDRESS = col_character(), WARD = col_character() ) See spec(...) for full column specifications. # Clean it up apartmentclean &lt;- apartmentdata %&gt;% filter(!is.na(SCORE)) %&gt;% # Remove apartments with missing scores dplyr::select(ward = WARD, score = SCORE, property_type = PROPERTY_TYPE, year_built = YEAR_BUILT, address = SITE_ADDRESS ) glimpse(apartmentclean) Observations: 3,437 Variables: 5 $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;08&quot;, &quot;… $ score &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, 78, 73, 76, 57, 70,… $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SOCIAL HOUSING&quot;, &quot;PRI… $ year_built &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, 2015, 1970, 1976, 1… $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACPHERSO… # Make a table of average scores for private residences (i.e. not social housing) apartmentclean %&gt;% filter(property_type == &quot;PRIVATE&quot;) %&gt;% group_by(ward) %&gt;% summarize(avg_score = mean(score)) %&gt;% knitr::kable() ward avg_score 01 71 02 73 03 71 04 68 05 72 06 72 07 70 08 74 09 67 10 72 11 70 12 73 13 74 14 73 15 74 16 77 17 76 18 76 19 72 20 74 21 72 22 75 23 74 24 72 25 73 YY 99 Exercise: suppose the data from any single ward follows a Normal model, \\[\\begin{equation} X_{i} \\overset{iid}{\\sim}\\text{Normal}\\left(\\mu,\\sigma^{2}\\right) \\end{equation}\\] Prove that the maximum likelihood estimator of \\(\\mu\\) is \\(\\hat{\\mu} = \\bar{X}\\), the sample mean. This justifies the use of the sample mean as an estimate of the average score of all buildings in the ward. Exercise: now prove that the maximum likelihood estimator for \\(\\sigma^{2}\\) under this model is \\[\\begin{equation} \\hat{\\sigma}^{2} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i} - \\bar{X} \\right)^{2} \\end{equation}\\] Argue that based on the “invariance” property of the MLE, this implies that the MLE for the \\(\\sigma = \\sqrt{\\sigma^{2}}\\) is \\[\\begin{equation} \\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i} - \\bar{X} \\right)^{2}} \\end{equation}\\] Exercise: use the sd() function in R to modify the above code to compute the MLE for the standard deviation for each ward. Exercise: why are we considering each ward separately? Now let \\(X_{ij}\\) be the score of the \\(i^{th}\\) building in the \\(j^{th}\\) ward. Suppose the data follows the model \\[\\begin{equation}\\begin{aligned} X_{ij} &amp;\\overset{ind}{\\sim}\\text{Normal}\\left(\\mu_{j},\\sigma^{2} \\right) \\\\ i &amp;= 1\\ldots n_{j} \\\\ j &amp;= 1\\ldots 25 \\end{aligned}\\end{equation}\\] Note that “iid” has been replaced by “ind”, to indicate that the data are independent but not identically distributed, because they have different means \\(\\mu_{j}\\). Note that I have taken the variance \\(\\sigma^{2}\\) to be the same for each ward, which may or may not be a reasonable thing to do. Prove that the MLE’s for the means and standard deviations are: \\[\\begin{equation}\\begin{aligned} \\hat{\\mu}_{j} &amp;= \\frac{1}{n_{j}}\\sum_{i=1}^{n_{j}}X_{ij} \\\\ \\hat{\\sigma} &amp;= \\sqrt{\\frac{\\sum_{j=1}^{25}\\sum_{i=1}^{n_{j}}\\left(X_{ij} - \\hat{\\mu}_{j} \\right)^{2}}{\\sum_{j=1}^{25}n_{j}}} \\end{aligned}\\end{equation}\\] Compute the MLEs in this way for these data. I got: themeans &lt;- apartmentclean %&gt;% filter(property_type == &quot;PRIVATE&quot;) %&gt;% group_by(ward) %&gt;% summarize(avg_score = mean(score)) thesd &lt;- apartmentclean %&gt;% filter(property_type == &quot;PRIVATE&quot;) %&gt;% summarize(sd = sd(score)) cat(&quot;The means: \\n&quot;) The means: knitr::kable(themeans) ward avg_score 01 71 02 73 03 71 04 68 05 72 06 72 07 70 08 74 09 67 10 72 11 70 12 73 13 74 14 73 15 74 16 77 17 76 18 76 19 72 20 74 21 72 22 75 23 74 24 72 25 73 YY 99 cat(&quot;The standard deviation: \\n&quot;) The standard deviation: thesd # A tibble: 1 x 1 sd &lt;dbl&gt; 1 7.11 "],["section-confidence-intervals-and-quantifying-uncertainty.html", "Chapter 9 Confidence Intervals and Quantifying Uncertainty 9.1 Confidence Intervals for the Mean (Chapter 23)", " Chapter 9 Confidence Intervals and Quantifying Uncertainty This chapter introduces confidence intervals and uses them to quantify uncertainty in estimates of parameters in statistical models. All datasets from MIPS can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. library(tidyverse) library(patchwork) options(digits = 5) 9.1 Confidence Intervals for the Mean (Chapter 23) Consider the following familiar scenario: we have a sample \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}\\text{N}(\\mu,1)\\), and we estimate the unknown parameter \\(\\mu\\) using the estimator \\(\\hat{\\mu} = \\bar{X}\\). The sample is random, so the estimator is also random. If we took another sample, we would get another value of \\(\\hat{\\mu}\\). The estimator \\(\\hat{\\mu}\\) is a random variable, so it has a probability distribution. We have seen several ways to get it: mathematical derivation, the Central Limit Theorem, and Bootstrapping. In this example, we know that \\[\\hat{\\mu} \\sim \\text{N}(\\mu,1/\\sqrt{n}).\\] The value \\(\\hat{\\mu}\\) is called a point estimate of the parameter \\(\\mu\\). In this chapter we are going to go one step further and quantify uncertainty in our point estimates for parameters. We are going to do this by using the sampling distributions of the estimators to construct random intervals which contain the true parameter value with high probability. These are called confidence intervals. 9.1.1 Simulation: an example How do we do this? We know that \\[\\hat{\\mu} \\sim \\text{N}(\\mu,1/\\sqrt{n}).\\] Rearrangement gives \\[Z_{n} = \\frac{\\hat{\\mu} - \\mu}{1/\\sqrt{n}} \\overset \\text{N}(0,1).\\] Let \\(q_{\\alpha}\\) be the \\(\\alpha\\)-quantile of \\(Z_{n}\\): the value such that \\(P(Z_{n} \\leq q_{\\alpha}) = \\alpha\\), for \\(0 \\leq \\alpha \\leq 1\\). Then we have \\[\\begin{equation}\\begin{aligned} 1 - \\alpha &amp;= P\\left( q_{\\alpha/2} \\leq Z_{n} \\leq q_{1 - \\alpha/2}\\right) \\\\ &amp;= P\\left( \\hat{\\mu} - q_{1 - \\alpha/2}\\times\\frac{1}{\\sqrt{n}} \\leq \\mu \\leq \\hat{\\mu} + q_{1 - \\alpha/2}\\times\\frac{1}{\\sqrt{n}}\\right) \\end{aligned}\\end{equation}\\] where we used \\(q_{1 - \\alpha/2} = -q_{\\alpha/2}\\). The values \\[\\begin{equation}\\begin{aligned} L_{n} &amp;= \\hat{\\mu} - q_{1 - \\alpha/2}\\times\\frac{1}{\\sqrt{n}} \\\\ U_{n} &amp;= \\hat{\\mu} + q_{1 - \\alpha/2}\\times\\frac{1}{\\sqrt{n}} \\\\ \\end{aligned}\\end{equation}\\] are random variables. We just showed that \\(P(L_{n} \\leq \\mu \\leq U_{n}) = 1-\\alpha\\). We call the random interval \\((L_{n},U_{n})\\) a \\(1-\\alpha\\) confidence interval for \\(\\mu\\). The interpretation of \\((L_{n},U_{n})\\) sometimes gets confused. \\(\\mu\\) is not random: it is a fixed, unknown quantity. \\((L_{n},U_{n})\\) are random: they change with the sample. If you took a different sample, you would get a different interval \\((L_{n},U_{n})\\). If you took many samples, about \\((1-\\alpha)\\times 100\\%\\) of them would contain the fixed, unknown value \\(\\mu\\). Let’s use a simulation to illustrate this concept. Take \\(\\mu = 0\\). We’ll generate a bunch of samples from a \\(\\text{N}(0,1)\\) distribution, compute their confidence intervals \\((L_{n},U_{n})\\), and plot them. We’ll use \\(\\alpha = 0.05\\) so that about \\(95\\%\\) of the calculated intervals should contain the value \\(0\\). set.seed(4329432) # How many samples to generate B &lt;- 50 # Sample size of each n &lt;- 20 # Confidence level alpha &lt;- .05 # Quantile critval &lt;- qnorm(1 - alpha/2) # For alpha = .05 this equals 1.96 # Perform the simulation theintervals &lt;- list( lower = numeric(B), upper = numeric(B) ) for (b in 1:B) { # Generate a sample xx &lt;- rnorm(n,0,1) # Compute the confidence interval Ln &lt;- mean(xx) - critval * (1/sqrt(n)) Un &lt;- mean(xx) + critval * (1/sqrt(n)) theintervals$lower[[b]] &lt;- Ln theintervals$upper[[b]] &lt;- Un } # Compute the proportion that don&#39;t contain zero # Should be close to alpha mean(theintervals$upper &lt; 0 | theintervals$lower &gt; 0) [1] 0.04 # Plot them. This is a more complicated plot than you&#39;re used to. # Run this code layer by layer to understand what each part does. as_tibble(theintervals) %&gt;% mutate(id = 1:n(), has_zero = if_else(lower &gt; 0 | upper &lt; 0,&quot;no&quot;,&quot;yes&quot;)) %&gt;% ggplot(aes(x = id)) + theme_classic() + geom_point(aes(y = (upper + lower)/2),pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;,size = 1) + geom_errorbar(aes(ymin = lower,ymax = upper,colour = has_zero),size = .2) + geom_hline(yintercept = 0,colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) + scale_y_continuous(breaks = seq(-1,1,by=.2)) + scale_color_manual(values = c(&quot;no&quot; = &quot;red&quot;,&quot;yes&quot; = &quot;black&quot;)) + coord_flip() + theme(axis.title.y = element_blank(),axis.text.y = element_blank(),axis.ticks.y = element_blank()) + guides(colour = FALSE) + labs(y = &quot;&quot;) Exercise: re-run this experiment several times with different random seeds. What kind of empirical coverage probabilities—the proportion of intervals that don’t contain zero—do you get? What about if you raise the sample size to n = 100? What about if you raise the number of simulations to B = 1000 (do NOT make the plot if you raise B this high!)? Exercise: suppose instead of \\(\\text{N}(\\mu,1)\\), the data are distributed as \\(\\text{N}(\\mu,\\sigma)\\) where \\(\\sigma\\) is a known constant. Show that a \\(1-\\alpha\\) confidence interval for \\(\\mu\\) is \\[ \\left(\\hat{\\mu} - q_{1 - \\alpha/2}\\times\\frac{\\sigma}{\\sqrt{n}},\\hat{\\mu} + q_{1 - \\alpha/2}\\times\\frac{\\sigma}{\\sqrt{n}}\\right) \\] 9.1.2 Gross calorific value measurements for Osterfeld 262DE27 The Osterfield data is made available with MIPS. It contains measurements of the gross calorific content of a type of coal. Its file name is misspelled, so be careful: head data/MIPSdata/grosscalOsterfeld.txt wc -l data/MIPSdata/grosscalOsterfeld.txt 23.870 23.730 23.712 23.760 23.640 23.850 23.840 23.860 23.940 23.830 23 data/MIPSdata/grosscalOsterfeld.txt Read it in. I’m leaving this as an exericse (note: not because I’m lazy, I still had to write the code. It’s for your learning). You should get the following: glimpse(osterfield) Observations: 23 Variables: 1 $ calorific_value &lt;dbl&gt; 23.870, 23.730, 23.712, 23.760, 23.640, 23.850, 23.84… Suppose that the measurements are normally distributed with known \\(\\sigma = .1\\). Then we can compute a \\(95\\%\\) confidence interval. Exercise: if we want a \\(95\\%\\) or \\(0.95\\) confidence interval, what is \\(\\alpha\\)? What is \\(q_{1 - \\alpha/2}\\)? # Compute the sample mean and size xbar &lt;- mean(osterfield$calorific_value) n &lt;- nrow(osterfield) # The population standard deviation, and the critical value/confidence level # are given as: sigma &lt;- .1 conf &lt;- .05 # Make sure to UNDERSTAND this calculation: critval &lt;- qnorm(1 - alpha/2) # 1.96 # Compute the interval c( &quot;lower&quot; = xbar - critval * sigma/sqrt(n), &quot;upper&quot; = xbar + critval * sigma/sqrt(n) ) lower upper 23.747 23.829 We are \\(95\\%\\) confident that \\(\\mu\\) lies between \\(23.747\\) and \\(23.829\\). This means that if we were to repeat this experiment over and over, obtaining each time a new sample and hence a new confidence interval, that about \\(95\\%\\) of those intervals ought to contain the true value of \\(\\mu\\), whatever it is. 9.1.3 When you don’t know \\(\\sigma\\) Now, suppose we have a normal sample but we don’t know \\(\\sigma\\). If we don’t know \\(\\sigma\\) we might think to estimate it using the sample standard deviation \\[ \\hat{\\sigma} = s_{n} = \\sqrt{ \\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{i} - \\bar{x}\\right)^2} \\] and then plug in \\(s_{n}\\) instead of \\(\\sigma\\) in the formula for a confidence interval. It turns out this is basically what we do, but with a catch. We now don’t only have uncertainty in \\(\\mu\\): we also have uncertainty in \\(\\sigma\\). Plugging in \\(s_{n}\\) and treating it as the truth ignores this uncertainty and leads to a confidence interval for \\(\\mu\\) that is too wide. This means that it will contain the true \\(\\mu\\) less than \\((1 - \\alpha)\\times 100\\%\\) of the time. Let’s see this using simulation: set.seed(7894236) n &lt;- 5 # Use a small sample to make this really obvious compute_ci_fixedsd &lt;- function(samp) { xbar &lt;- mean(samp) n &lt;- length(samp) critval &lt;- qnorm(1 - .05/2,0,1) c( &quot;lower&quot; = xbar - critval * 1/sqrt(n), &quot;upper&quot; = xbar + critval * 1/sqrt(n) ) } compute_ci_samplesd &lt;- function(samp) { xbar &lt;- mean(samp) sn &lt;- sd(samp) n &lt;- length(samp) critval &lt;- qnorm(1 - .05/2,0,1) c( &quot;lower&quot; = xbar - critval * sn/sqrt(n), &quot;upper&quot; = xbar + critval * sn/sqrt(n) ) } # True mu = 0, true sigma = 1 B &lt;- 1000 doesthefixedonecontainmu &lt;- numeric(B) doesthesampleonecontainmu &lt;- numeric(B) for (b in 1:B) { # Take a sample thesamp &lt;- rnorm(n,0,1) # Compute the intervals fixedsigmainterval &lt;- compute_ci_fixedsd(thesamp) samplesigmainterval &lt;- compute_ci_samplesd(thesamp) # Record whether they each contain zero doesthefixedonecontainmu[b] &lt;- as.numeric(fixedsigmainterval[&#39;lower&#39;] &lt; 0 &amp; fixedsigmainterval[&#39;upper&#39;] &gt; 0) doesthesampleonecontainmu[b] &lt;- as.numeric(samplesigmainterval[&#39;lower&#39;] &lt; 0 &amp; samplesigmainterval[&#39;upper&#39;] &gt; 0) } # Proportion of each type of interval that contains mu mean(doesthefixedonecontainmu) [1] 0.961 mean(doesthesampleonecontainmu) [1] 0.876 The proportion of known-\\(\\sigma\\) intervals containing the true \\(\\mu\\) is close to the “nominal” value of \\(0.95\\). The proportion of the estimated-\\(\\sigma\\) intervals containing the true \\(\\mu\\) is less, because these intervals are too narrow, because they ignore uncertainty in \\(\\sigma\\). To mitigate this, we widen the interval. We do so in a specific way. It turns out that \\[ t_{n} = \\frac{\\bar{X} - \\mu}{s_{n}/\\sqrt{n}}\\sim T_{n-1} \\] where the random variable \\(T_{n-1}\\) has what is called the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. For now, all you need to know is that you can obtain \\(q_{n-1,\\alpha}\\) satisfying \\(P(T_{n-1} \\leq q_{n-1,\\alpha}) = \\alpha\\) using the command qt(alpha,n-1) in R. The formula for a \\(1-\\alpha\\) confidence interval when the standard deviation is unknown is \\[ \\left(\\hat{\\mu} - q_{n-1,1 - \\alpha/2}\\times\\frac{s_{n}}{\\sqrt{n}},\\hat{\\mu} + q_{n-1,1 - \\alpha/2}\\times\\frac{s_{n}}{\\sqrt{n}}\\right) \\] Basically, when you swap \\(\\sigma\\) for \\(s_{n}\\), you swap \\(q_{\\alpha}\\) for \\(q_{n-1,\\alpha}\\). We can check this fixes the problem using simulation: set.seed(7894236) # Same seed as before n &lt;- 5 # Use a small sample to make this really obvious compute_ci_samplesd_T &lt;- function(samp) { xbar &lt;- mean(samp) sn &lt;- sd(samp) n &lt;- length(samp) critval &lt;- qt(1 - .05/2,n-1) c( &quot;lower&quot; = xbar - critval * sn/sqrt(n), &quot;upper&quot; = xbar + critval * sn/sqrt(n) ) } # True mu = 0, true sigma = 1 B &lt;- 1000 doesthenewsampleonecontainmu &lt;- numeric(B) doestheoldbadonecontainmu &lt;- numeric(B) for (b in 1:B) { # Take a sample thesamp &lt;- rnorm(n,0,1) # Compute the interval samplesigmainterval_oldandbad &lt;- compute_ci_samplesd(thesamp) samplesigmainterval_T &lt;- compute_ci_samplesd_T(thesamp) # Record whether they each contain zero doestheoldbadonecontainmu[b] &lt;- as.numeric(samplesigmainterval_oldandbad[&#39;lower&#39;] &lt; 0 &amp; samplesigmainterval_oldandbad[&#39;upper&#39;] &gt; 0) doesthenewsampleonecontainmu[b] &lt;- as.numeric(samplesigmainterval_T[&#39;lower&#39;] &lt; 0 &amp; samplesigmainterval_T[&#39;upper&#39;] &gt; 0) } # Proportion of intervals that contains mu mean(doestheoldbadonecontainmu) [1] 0.876 mean(doesthenewsampleonecontainmu) [1] 0.949 The proportion of \\(t\\)-intervals containing the true \\(\\mu\\) is now much closer to the nominal coverage of \\(.95\\). 9.1.4 Gross calorific value measurements for Daw Mill 258GB41 Now, for a different sample of measurements of the gross calorific content of some other type of coal, we can compute a confidence interval without assuming that the standard deviation is known. Read the data in from file grosscalDawMill.txt, call it dawmill. You can compute the sample standard deviation and appropriate critical value as follows: sn &lt;- sd(dawmill$calorific_value) alpha &lt;- .05 critval &lt;- qt(1 - alpha/2,df = nrow(dawmill) - 1) You should get: lower upper 0.6478 61.3754 9.1.5 Bootstrap Confidence Intervals The construction of the confidence interval assumes we know the distribution of \\[ Z_{n} = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\] or \\[ T_{n} = \\frac{\\bar{X} - \\mu}{s_{n}/\\sqrt{n}} \\] The CLT is very helpful, but it doesn’t always apply, and sometimes we won’t know the sampling distribution of \\(T_{n}\\). We can use the empirical bootstrap to construct confidence intervals too. The only thing that the distribution of \\(T_{n}\\) is actually used for is finding the quantiles: the \\(q_{\\alpha}\\) with \\(P(T_{n} \\leq q_{\\alpha}) = \\alpha\\). Consider the following algorithm: Input: sample \\(X_{1},\\ldots,X_{n}\\), For: \\(b = 1,\\ldots,B\\), do: Take a bootstrap sample \\(X^{*}_{1},\\ldots,X^{*}_{n}\\) independently from the original sample, and compute its mean \\(\\bar{X}^{*}\\) and standard deviation \\(s_{n}^{*}\\), Compute \\(T^{*}_{n,b} = (\\bar{X}^{*} - \\bar{X}) / (s_{n}^{*}/\\sqrt{n})\\), Output: \\(T^{*}_{n,1},\\ldots,T^{*}_{n,B}\\), a sample from the sampling distribution of \\(T_{n}\\). The sample quantiles of \\(T^{*}_{n,1},\\ldots,T^{*}_{n,B}\\) are then used in place of the theoretical quantiles we used before. First, let’s simulate a dataset to illustrate this idea and so we can compare the bootstrap and analytical answers. set.seed(43547803) B &lt;- 2000 n &lt;- 5000 # Simulate one dataset ds &lt;- rnorm(n,0,1) # Values alpha &lt;- .05 critval &lt;- qnorm(1 - alpha/2) # Now resample from it and calculate studentized statistics resampledTn &lt;- numeric(B) for (b in 1:B) { # Draw a bootstrap sample from the dataset ds boot &lt;- sample(ds,n,replace = TRUE) # Compute Tn resampledTn[b] &lt;- (mean(boot) - mean(ds)) / (sd(boot) / sqrt(n)) } # The confidence limits are obtained from the sample quantiles: samplequantiles &lt;- quantile(resampledTn,probs = c(alpha/2,1 - alpha/2)) truequantiles &lt;- qnorm(c(alpha/2,1-alpha/2),0,1) # Here&#39;s a plot that illustrates what these look like: tibble(x = resampledTn) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),colour = &quot;black&quot;,fill = &quot;lightgrey&quot;,bins = 100) + geom_vline(xintercept = samplequantiles[1],colour = &quot;orange&quot;,linetype = &quot;dotdash&quot;) + geom_vline(xintercept = samplequantiles[2],colour = &quot;orange&quot;,linetype = &quot;dotdash&quot;) + geom_vline(xintercept = truequantiles[1],colour = &quot;blue&quot;,linetype = &quot;dotdash&quot;) + geom_vline(xintercept = truequantiles[2],colour = &quot;blue&quot;,linetype = &quot;dotdash&quot;) + labs(title = &quot;Resampled student statistics with true (blue) and empirical (orange) confidence limits&quot;, x = &quot;&quot;,y = &quot;&quot;) Exercise: I deliberately chose a large sample size and number of bootstrap samples to make the results look good. I encourage you to change these numbers to try and break this simulation. The bootstrap-resampled confidence limits are close to the truth: samplequantiles 2.5% 97.5% -1.9999 2.0060 truequantiles [1] -1.96 1.96 Let’s apply this to the software data from a previous chapter. Recall we had measurements \\(X_{i}\\) of the time to failure of a software product, and we had a model \\(X_{i}\\overset{iid}{\\sim}\\text{Exponential}(\\lambda)\\). We can use the empirical bootstrap to compute a \\(95\\%\\) confidence interval for \\(E(X_{i}) = \\lambda^{-1}\\): # Read it in: software &lt;- readr::read_csv( file = &quot;data/MIPSdata/software.txt&quot;, col_names = &quot;time&quot;, col_types = &quot;n&quot; ) alpha &lt;- .05 n &lt;- nrow(software) mn &lt;- mean(software$time) sn &lt;- sd(software$time) # 95% CI for mu based on the empirical bootstrap: B &lt;- 1000 set.seed(821940379) resampledTn &lt;- numeric(B) for (b in 1:B) { # Draw a bootstrap sample from the dataset ds boot &lt;- sample(software$time,n,replace = TRUE) # Compute Tn resampledTn[b] &lt;- (mean(boot) - mean(software$time)) / (sd(boot) / sqrt(n)) } # The confidence limits are obtained from the sample quantiles: conflim &lt;- quantile(resampledTn,probs = c(alpha/2,1 - alpha/2)) # The confidence interval: c( &quot;lower&quot; = mn + conflim[1] * sn/sqrt(n), &quot;upper&quot; = mn + conflim[2] * sn/sqrt(n) ) lower.2.5% upper.97.5% 436.40 807.14 Exercise: compute a \\(95\\%\\) confidence interval for the mean for the software data by using the CLT to . This does NOT mean that you should use a normal distribution for calculating the critical values– if you don’t understand why, go back and read the “Variance Unknown” section on page 348. I got the following: lower upper 473.45 824.83 Does this lead to different conclusions in practice than the bootstrap interval? Exercise: what if we wanted a confidence interval for \\(\\lambda\\), instead of \\(\\lambda^{-1}\\)? Show in general that if \\((L_{n},U_{n})\\) is a \\(1-\\alpha\\) confidence interval for some parameter \\(\\theta\\), that \\((U_{n}^{-1},L_{n}^{-1})\\) is a \\(1-\\alpha\\) confidence interval for \\(\\theta^{-1}\\). Use this fact to construct a \\(1-\\alpha\\) confidence interval for \\(\\lambda\\) using both the empirical bootstrap and the \\(t\\)-distribution. I got: upper lower 0.0012124 0.0021122 upper.97.5% lower.2.5% 0.0012389 0.0022915 Advanced Exercise: extend that last result: show that if \\((L_{n},U_{n})\\) is a \\(1-\\alpha\\) confidence interval for some parameter \\(\\theta\\), and \\(g(\\theta)\\) is a monotone function, then \\((g(L_{n}),g(U_{n}))\\) is a \\(1-\\alpha\\) confidence interval for \\(g(\\theta)\\) if \\(g(\\theta)\\) is increasing and \\((g(U_{n}),g(L_{n}))\\) is a \\(1-\\alpha\\) confidence interval for \\(g(\\theta)\\) if \\(g(\\theta)\\) is decreasing. "],["section-extended-example-reasoning-about-goodness-of-fit.html", "Chapter 10 Extended Example: Reasoning About Goodness of Fit 10.1 Go and read the blog post 10.2 Distribution of last digits", " Chapter 10 Extended Example: Reasoning About Goodness of Fit This chapter is a bit different: we introduce the idea of “goodness of fit” through implementing the analysis in a blog post discussing a disputed paper in a Psychology journal. You can find the article here: http://datacolada.org/74 library(tidyverse) 10.1 Go and read the blog post To start, go read this blog post: http://datacolada.org/74. This should take you at least an hour or so to do in detail, if not longer. Exercise: summarize this study in your own words. Two or three sentences should suffice. What did they do, how did they do it, and what were they trying to find out? It is important that you understand this before you move on. 10.2 Distribution of last digits The distribution of digits in numeric data is of considerable interest in certain fields. In forensic accounting, where investigators try to identify fraudulent accounting practice by identifying systematic anomalies in financial records, the relative frequency with which different digits occur can indicate potential fraud if it differs from what you would expect. It is well-established that the last digit, in particular, of numeric data should be distributed pretty evenly between the numbers \\(0 - 9\\) in any given set of data. Remark: this isn’t trivial; for example, Benford’s Law says that this is not true about the first digit in numbers in certain types of financial data. So, let’s look at the last digit of the measurements from the hand sanitizer study. The data is available from http://datacolada.org/appendix/74/ and their R code can be found here. Here we use our own R code but I borrow parts from theirs. 10.2.1 Read in the data First, read in the data as usual: study1 &lt;- readr::read_csv( file = &quot;http://datacolada.org/appendix/74/Study%201%20-%20Decoy%20Effect.csv&quot;, col_names = TRUE, col_types = stringr::str_c(rep(&quot;n&quot;,42),collapse = &quot;&quot;) ) glimpse(study1) Observations: 40 Variables: 42 $ Subject &lt;dbl&gt; 1, 2, 3, 4,… $ `Group (1=experimental condition, 0 = control condition)` &lt;dbl&gt; 1, 1, 1, 1,… $ Day1 &lt;dbl&gt; 55, 60, 63,… $ Day2 &lt;dbl&gt; 45, 55, 40,… $ Day3 &lt;dbl&gt; 45, 55, 50,… $ Day4 &lt;dbl&gt; 40, 55, 60,… $ Day5 &lt;dbl&gt; 45, 50, 25,… $ Day6 &lt;dbl&gt; 45, 45, 35,… $ Day7 &lt;dbl&gt; 55, 45, 55,… $ Day8 &lt;dbl&gt; 60, 40, 40,… $ Day9 &lt;dbl&gt; 45, 40, 33,… $ Day10 &lt;dbl&gt; 67, 40, 50,… $ Day11 &lt;dbl&gt; 65, 56, 40,… $ Day12 &lt;dbl&gt; 70, 40, 50,… $ Day13 &lt;dbl&gt; 80, 40, 25,… $ Day14 &lt;dbl&gt; 75, 45, 45,… $ Day15 &lt;dbl&gt; 70, 50, 50,… $ Day16 &lt;dbl&gt; 80, 55, 50,… $ Day17 &lt;dbl&gt; 70, 55, 45,… $ Day18 &lt;dbl&gt; 75, 50, 45,… $ Day19 &lt;dbl&gt; 70, 45, 45,… $ Day20 &lt;dbl&gt; 65, 50, 40,… $ `Day21 (Beginning of intervention)` &lt;dbl&gt; 85, 55, 65,… $ Day22 &lt;dbl&gt; 85, 75, 65,… $ Day23 &lt;dbl&gt; 90, 45, 70,… $ Day24 &lt;dbl&gt; 85, 60, 55,… $ Day25 &lt;dbl&gt; 75, 50, 60,… $ Day26 &lt;dbl&gt; 75, 50, 70,… $ Day27 &lt;dbl&gt; 65, 35, 55,… $ Day28 &lt;dbl&gt; 75, 55, 40,… $ Day29 &lt;dbl&gt; 75, 60, 65,… $ Day30 &lt;dbl&gt; 80, 55, 65,… $ Day31 &lt;dbl&gt; 75, 65, 70,… $ Day32 &lt;dbl&gt; 50, 45, 75,… $ Day33 &lt;dbl&gt; 70, 40, 55,… $ Day34 &lt;dbl&gt; 75, 55, 50,… $ Day35 &lt;dbl&gt; 65, 55, 65,… $ Day36 &lt;dbl&gt; 75, 60, 45,… $ Day37 &lt;dbl&gt; 70, 50, 50,… $ Day38 &lt;dbl&gt; 75, 45, 55,… $ Day39 &lt;dbl&gt; 90, 65, 65,… $ Day40 &lt;dbl&gt; 85, 70, 65,… Exercise: download the data by pasting the link into a web browser. Print out the header on the command line or open the .csv file in Excel or otherwise. Verify that the col_names and col_types arguments I provided are correct. The data is in “wide” format– each day has its own column. We want one column for subject ID, one for Day, and one for measurement. Let’s reformat the data: study1_long &lt;- study1 %&gt;% pivot_longer(Day1:Day40,names_to = &quot;day&quot;,values_to = &quot;measurement&quot;) glimpse(study1_long) Observations: 1,600 Variables: 4 $ Subject &lt;dbl&gt; 1, 1, 1, 1,… $ `Group (1=experimental condition, 0 = control condition)` &lt;dbl&gt; 1, 1, 1, 1,… $ day &lt;chr&gt; &quot;Day1&quot;, &quot;Da… $ measurement &lt;dbl&gt; 55, 45, 45,… We previously had \\(40\\) subjects each with \\(40\\) columns of measurements; now we have \\(1,600\\) rows, which looks good to me. Let’s further clean up the data: we need to Rename the colums so they are pleasant (yes, this is important!), Extract the last digit of each measurement and save it in a new column. Let’s do this. We’ll use the substr function to choose the last digit of each number. Type ?substr to learn about this function. study1_clean &lt;- study1_long %&gt;% rename(subject = Subject, group = `Group (1=experimental condition, 0 = control condition)`) %&gt;% mutate(last_digit = as.numeric(substr(measurement,nchar(measurement),nchar(measurement)))) glimpse(study1_clean) Observations: 1,600 Variables: 5 $ subject &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ group &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ day &lt;chr&gt; &quot;Day1&quot;, &quot;Day2&quot;, &quot;Day3&quot;, &quot;Day4&quot;, &quot;Day5&quot;, &quot;Day6&quot;, &quot;Day7&quot;, &quot;… $ measurement &lt;dbl&gt; 55, 45, 45, 40, 45, 45, 55, 60, 45, 67, 65, 70, 80, 75, 7… $ last_digit &lt;dbl&gt; 5, 5, 5, 0, 5, 5, 5, 0, 5, 7, 5, 0, 0, 5, 0, 0, 0, 5, 0, … Looks clean to me! 10.2.2 Make the histogram Okay, now consider Figure 1 in the blog post. They filter out observations ending in \\(0\\) or \\(5\\) (remember this later), because the authors of the disputed paper claim to have occasionally used a scale with \\(5\\)-gram precision, and occasionally one with \\(1\\)-gram precision (this alone is suspect…). They then make a histogram of all the last digits. We can do this too: study1_clean %&gt;% filter(!(last_digit %in% c(0,5))) %&gt;% ggplot(aes(x = last_digit)) + theme_classic() + geom_bar(stat = &quot;count&quot;,colour = &quot;black&quot;,fill = &quot;lightblue&quot;) + scale_x_continuous(breaks = c(1,2,3,4,6,7,8,9),labels = c(1,2,3,4,6,7,8,9)) + labs(title = &quot;Study 1: Last digit strikingly not uniform&quot;, x = &quot;Last Digit (0&#39;s and 5&#39;s removed)&quot;, y = &quot;Frequency&quot;) I called this plot a “histogram” (which it is), but I used geom_bar(stat = \"count\") to create it. I did this because I wanted the bins to be at specific values. It’s still a histogram (why?). 10.2.3 Testing goodness of fit: simulation If you expect the last digits of a set of numbers to be evenly-distributed across the values \\(0 - 9\\), then this plot might look surprising. But can we conclude that something is wrong, just by looking at a plot? What if the real underlying distribution of digits is actually evenly-distributed, and we just got a weird sample– we’d be making an incorrect harsh judgement. We’re going to ask the question: if the underlying distribution of last digits really was evenly distributed, what is the probability of seeing the data that we saw, or a dataset that is even more extreme under this hypothesis?. That’s a mouthful! But it’s a good question to ask. If it’s really, really unlikely that we see what we saw (or something even further) if the claim of even distribution is true, then this provides evidence against the notion that the digits are actually evenly distributed. And this provides evidence that there is something funny in the data. We are going to do a simulation to investigate this claim. We are going to Generate a bunch of datasets the same size as ours but where the distribution of the last digit actually is evenly distributed across \\(0 - 9\\), and Record the proportion of digits in each that are \\(3\\) or \\(7\\), and Compute the proportion of our simulated datasets that have a proportion of 3’s or 7’s as high, or higher, as what we saw in our sample. There are a few details that we can’t get exactly right here: the real data was generated by sampling a bunch of values that ended in \\(0&#39;s\\) or \\(5&#39;s\\) and then filtering these out, which is behaviour that I don’t know how to replicate exactly. We also could consider the distribution of \\(3&#39;s\\) and \\(7&#39;s\\) separately, or jointly (using “and” instead of “or”). The first question is, how do we generate data that has last digits evenly distributed? Well, any random numbers should work, but to keep things consistent with the real data, let’s try generating from a normal distribution with mean and variance equal to the sample mean and variance of our data, rounded to the nearest integer: mn &lt;- mean(study1_clean$measurement) ss &lt;- sd(study1_clean$measurement) testnumbers &lt;- round(rnorm(10000,mn,ss)) # Plot a chart of the last digits tibble(x = testnumbers) %&gt;% mutate(last_digit = as.numeric(substr(x,nchar(x),nchar(x)))) %&gt;% ggplot(aes(x = last_digit)) + theme_classic() + geom_bar(stat = &quot;count&quot;,colour = &quot;black&quot;,fill = &quot;lightblue&quot;) + scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9),labels = c(0,1,2,3,4,5,6,7,8,9)) + labs(title = &quot;Last digits of normal random sample&quot;, x = &quot;Last Digit&quot;, y = &quot;Frequency&quot;) Looks pretty uniform to me! Let’s proceed with our simulation: set.seed(789685) # Create a function that simulates a dataset # and returns the proportion of last digits # that are either 3 or 7 N &lt;- nrow(study1_clean %&gt;% filter(!(last_digit %in% c(5,0)))) # Size of dataset to simulate B &lt;- 1e04 # Number of simulations to do mn &lt;- mean(study1_clean$measurement) ss &lt;- sd(study1_clean$measurement) simulate_proportion &lt;- function() { ds &lt;- round(rnorm(N,mn,ss)) last_digits &lt;- substr(ds,nchar(ds),nchar(ds)) mean(last_digits %in% c(&quot;3&quot;,&quot;7&quot;)) } # What is the proportion of 3&#39;s and 7&#39;s in our data, # after filtering out 5&#39;s and 0&#39;s? study_proportion &lt;- study1_clean %&gt;% filter(!(last_digit %in% c(5,0))) %&gt;% summarize(p = mean(last_digit %in% c(3,7))) %&gt;% pull(p) study_proportion [1] 0.88148 # 88.1%. Wow. # Perform the simulation: sim_results &lt;- numeric(B) for (b in 1:B) sim_results[b] &lt;- as.numeric(simulate_proportion() &gt;= study_proportion) # This is a vector of 0/1 which says whether each simulation&#39;s proportion of # 3&#39;s and 7&#39;s exceeded the study proportion. Its mean is the simulated probability # of seeing what we saw in the study, if the digits are actually evenly distributed: mean(sim_results) [1] 0 # Okay... how many? sum(sim_results) [1] 0 In \\(B = 10,000\\) simulations, I didn’t even get a single dataset that was as extreme as ours. This provides strong evidence against the notion that the digits are, in fact, evenly distributed. Exercise: how many simulations do you need before you get even one that is as extreme as our dataset? Exercise: investigate the manner in which we generated these random digits. Try the following: Simulate from a continuous uniform distribution with mean and variance equal to the sample mean and variance (you have to figure out how to do this), Simulate from a discrete uniform distribution with mean and variance equal to the sample mean and variance, (Advanced) Simulate from a discrete uniform distribution as above, but then, chop off the last digit and replace it with one simulated from a discrete uniform distribution on \\(\\left\\{0,\\ldots,9\\right\\}\\). Exercise: repeate the simulation, but filter out numbers ending in \\(0\\) or \\(5\\) from the simulated datasets. Do the results change at all? 10.2.4 Testing goodness of fit: math We may also use mathematics and statistical modelling to answer the question: if the underlying distribution of last digits really was evenly distributed, what is the probability of seeing the data that we saw, or a dataset that is even more extreme under this hypothesis?. We do this in a clever way: we construct a statistical model that represents the truth, if the truth is what we say it is. Namely, we will define a probability distribution that should represent the distribution of the last digits of our measurements, if the last digits are evenly distributed. We then see how probable our data is under this model. If, under this model, it is very unlikely to see a dataset like ours, then this provides evidence that the model isn’t representative of the truth. And since the model was built under the premise that the digits are evenly distributed, a lack of fit of the model to the observed data provides evidence against the notion that the digits are evenly distributed. Let’s develop a model. Define \\(y_{ij} = 1\\) if the last digit of the \\(i^{th}\\) measurement equals \\(j\\), for \\(j \\in J = \\left\\{1,2,3,4,6,7,8,9\\right\\}\\) (remember: they filtered out measurements ending in \\(0\\) or \\(5\\)), and equals \\(0\\) otherwise. So for example, if the \\(i^{th}\\) measurement is \\(42\\) then \\(y_{i1} = 0\\) and \\(y_{i2} = 1\\) and \\(y_{i3} = 0\\) and so on. Define \\(y_{i} = (y_{i1},\\ldots,y_{i9})\\), a vector containing all zeroes except for exactly one \\(1\\). Then the \\(y_{i}\\) are independent draws from a Multinomial distribution, \\(y_{i}\\overset{iid}{\\sim}\\text{Multinomial}(1,p_{1},\\ldots,p_{9})\\), with \\(\\mathbb{E}(y_{ij}) = \\mathbb{P}(y_{ij} = 1) = p_{j}\\) and \\(\\sum_{j\\in J}p_{j} = 1\\). The vectors \\(y_{i}\\) have the following (joint) density function: \\[\\begin{equation} \\mathbb{P}(y_{i} = (y_{i1},\\ldots,y_{i9})) = p_{1}^{y_{i1}}\\times\\cdots\\times p_{9}^{y_{i9}} \\end{equation}\\] The multinomial is the generalization of the binomial/bernoulli to multiple possible outcomes on each trial. If the bernoulli is thought of as flipping a coin (two possible outcomes), then the multinomial should be thought of as rolling a die (six possible outcomes). How does this help us answer the question? If the digits are actually evenly distributed, then this means \\(p_{1} = \\cdots = p_{9} = 1/8\\) (why?). However, the data might tell us something different. We estimate the \\(p_{j}\\) from out data \\(y_{1},\\ldots,y_{n}\\) by computing the maximum likelihood estimator: \\[\\begin{equation} \\hat{p}_{j} \\ \\frac{1}{n}\\sum_{i=1}^{n}y_{ij} \\end{equation}\\] which are simply the sample proportions of digits that equal each value of \\(j\\). We assess how close the MLEs are to what the true values ought to be using the likelihood ratio. For \\(p_{0} = (1/8,\\ldots,1/8)\\), \\(\\hat{p} = (\\hat{p}_{1},\\ldots,\\hat{p}_{9})\\), and \\(L(\\cdot)\\) the likelihood based off of the multinomial density, \\[\\begin{equation} \\Lambda = \\frac{L(p_{0})}{L(\\hat{p})} \\end{equation}\\] Remember the definition of the likelihood, for discrete data: for any \\(p\\), \\(L(p)\\) is the relative frequency with which the observed data would be seen in repeated sampling, if the true parameter value were \\(p\\). The likelihood ratio \\(\\Lambda\\) is the ratio of how often our data would be seen if \\(p = p_{0}\\), against how often it would be seen if \\(p = \\hat{p}\\). If \\(\\Lambda = 0.5\\), for example, that means that our data would occur half as often if \\(p = p_{0}\\), compared to if \\(p = \\hat{p}\\). Note that \\(0 &lt; \\Lambda \\leq 1\\) (why?). Lower values of \\(\\Lambda\\) mean that there is stronger evidence against the notion that \\(p_{0}\\) is a plausible value for \\(p\\)– that is, that the digits are evenly distributed. But how do we quantify how much less likely is less likely enough? Here is where our previous question comes back. We ask: if the digits were evenly distributed, what is the probability of seeing the data we saw, or something with an even more extreme distribution of digits? To compute this, we need to be able to compute probabilities involving \\(\\Lambda\\). It turns out that using \\(\\Lambda\\) is very clever, because well, we can do this. There is a Big Theorem which states that if \\(p\\in\\mathbb{R}^{d}\\) and \\(p = p_{0}\\), \\[\\begin{equation} -2\\log\\Lambda \\overset{\\cdot}{\\sim} \\chi_{d-1}^{2} \\end{equation}\\] So, if the digits are evenly distributed, then our value of \\(-2\\log\\Lambda\\) should be a realization of a \\(\\chi^{2}_{7}\\) random variable. We therefore compute \\[\\begin{equation} \\nu = \\mathbb{P}\\left(\\chi^{2}_{7} \\geq -2\\log\\Lambda\\right) \\end{equation}\\] The quantity \\(\\nu\\) is the probability of observing a distribution of digits as or more extreme than the one we observed in our data, if the distribution of digits truly is even. It’s called a p-value, and it’s one helpful summary statistic in problems where the research question concerns comparing observations to some sort of reference, like we’re doing here. Let’s compute the likelihood ratio for our data: # Compute the MLEs study1_filtered &lt;- study1_clean %&gt;% filter(!(last_digit %in% c(5,0))) # Should have done this before... pmledat &lt;- study1_filtered %&gt;% group_by(last_digit) %&gt;% summarize(cnt = n(), pp = n() / nrow(study1_filtered)) obsvec &lt;- pmledat$cnt names(obsvec) &lt;- pmledat$last_digit # Notice how there are no digits ending in 1 in the data # So the MLE for p1 is zero. # Need to account for this manually pmle &lt;- rep(0,8) names(pmle) &lt;- c(1,2,3,4,6,7,8,9) pmlevals &lt;- pmledat$pp names(pmlevals) &lt;- pmledat$last_digit pmle[names(pmlevals)] &lt;- pmlevals pmle 1 2 3 4 6 7 8 9 0.0000000 0.0074074 0.5259259 0.0148148 0.0222222 0.3555556 0.0518519 0.0222222 # Truth, if digits evenly distributed p0 &lt;- rep(1,length(pmle)) / length(pmle) names(p0) &lt;- names(pmle) p0 1 2 3 4 6 7 8 9 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 # Compute minus twice the likelihood ratio multinom_log_likelihood &lt;- function(x,p) { # x: named vector where the name is j and the value is the count of # times y_ij = 1 in the sample # p: named vector with names equal to the unique values of x, # containing probabilities of each sum(x * log(p[names(x)])) } lr &lt;- -2 * (multinom_log_likelihood(obsvec,p0) - multinom_log_likelihood(obsvec,pmle)) lr [1] 257.16 # Compute the probability that a chisquare random variable is greater than this # value: 1 - pchisq(lr,df = length(pmle) - 1) # zero. [1] 0 Exercise: write your own function to implement the multinomial loglikelihood, using the dmultinom function and a for loop. Compare your results to mine. We would interpret this result as: the observed data provides strong evidence against the notion that the digits are evenly distributed. The fact that there is essentially zero chance of seeing what we saw if our claim that the digits are evenly distributed agrees with our simulation. We just did some statistical forensics! Pretty cool. This type of reasoning is abundant in the scientific literature, as are these p-value things. They are useful, but we want you, as stats majors and specialists, to leave this course understanding than statistics is so very much more than just a ritual or toolbox! library(tidyverse) "],["section-bayesian-inference-estimation.html", "Chapter 11 Bayesian Inference: Estimation 11.1 Estimation in Bayesian Inference: general ideas 11.2 Estimation in Bayesian Inference: point and interval estimation 11.3 Choosing a Prior", " Chapter 11 Bayesian Inference: Estimation This chapter describes how to use Bayesian inference for estimation. Materials in this tutorial are taken from Alex’s comprehensive tutorial on Bayesian Inference, which is very long and outside the scope of this course. 11.1 Estimation in Bayesian Inference: general ideas Recall our basic setup: we flip a coin and define the random variable \\(X\\) which takes value \\(1\\) if the coin comes up heads and \\(0\\) if tails. We know our sample \\(X_{i}\\overset{iid}{\\sim}\\text{Bernoulli}(\\theta)\\) and we wish to use the observed flips to estimate \\(\\theta\\). We had previously looked at Bayesian inference for doing this when \\(\\theta\\) could only take the values \\(0.3\\) or \\(0.7\\). The case where there are only two possible parameter values is useful for examples, but not common in practice. More realistic is the case where \\(0 &lt; \\theta &lt; 1\\), and we need to use the observed data to estimate \\(\\theta\\). Intuition and frequentist results dictate that a “good” estimator of \\(\\theta\\) is the sample proportion number of heads, \\[ \\hat{\\theta}_{freq} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i} \\] This makes sense; of course the best guess of the value of the probability of heads based on one sample is just the relative frequency with which \\(X\\) is heads in that sample. It’s also the maximum likelihood estimator, and the unbiased estimator with minimum variance. Let’s see how this estimator, which is optimal from a frequentist perspective, behaves compared to what we come up with using Bayesian estimation. 11.1.1 The Prior The new parameter space is \\(\\Theta = (0,1)\\). Bayesian inference proceeds as above, with the modification that our prior must be continuous and defined on the unit interval \\((0,1)\\). This reflects the fact that our parameter can take any value on the interval \\((0,1)\\). Choosing the prior is a subjective decision, and is slightly more difficult in the continuous case because interpreting densities is harder than interpreting discrete probability mass functions. In a nutshell, we should choose our prior distribution such that values of \\(\\theta\\) that we think are reasonable have high probability under the prior, and values of \\(\\theta\\) that we think are not reasonable have low probability under the prior. There is a lot more that goes into the choice of prior in more complicated applied problems, but this is always the basic idea. A popular choice for a prior for a binomial likelihood like we have here is the beta distribution, \\[ \\begin{aligned} \\theta &amp;\\sim \\text{Beta}(a,b) \\\\ f(\\theta;a,b) &amp;= \\theta^{a-1}(1-\\theta)^{b-1}, 0 &lt; \\theta &lt; 1, a &gt; 0, b &gt; 0 \\\\ E(\\theta) &amp;= \\frac{a}{a+b} \\\\ Var(\\theta) &amp;= \\frac{ab}{(a+b)^2 (a+b+1)} \\end{aligned} \\] The Beta distribution is defined on \\(0,1\\) and itself has two parameters \\(a,b\\) which control the shape of the distribution, and its moments. If a parameter having a mean and variance makes you uncomfortable at first, try interpreting these quantities less literally; the mean is just the “centre” of the possible values of \\(\\theta\\) as weighted by their probabilities, and the variance (or more accurately, the standard deviation) roughly describes the size of the region around the mean in which \\(\\theta\\) is likely to fall. This just gives a measure of how “sure” we are, before seeing the results of any coin flips, that \\(\\theta\\) is near the centre of its possible values. Let’s visualize the prior distribution in order to help us specify the parameters \\(a,b\\) that will give us a reasonable prior: # Generate plot data: # dbeta evaluated at x and a,b for x on a grid between 0 and 1 and various values of a,b x &lt;- seq(0.01,0.99,by=0.01) ngrid &lt;- length(x) expand.grid(a = c(.5,5,15),b = c(.5,5,15)) %&gt;% as_tibble() %&gt;% # Add the grid values and the density values # Here&#39;s a cool trick: to replicate each row of a dataframe n times, # use slice(df,rep(1:n,each = n)) slice(rep(1:n(),each = ngrid)) %&gt;% mutate(x = rep(x,n() / ngrid), y = dbeta(x,a,b)) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + facet_wrap(a~b) + geom_line(colour = &quot;purple&quot;) + labs(title = &quot;Beta Distribution, various a and b&quot;, subtitle = &quot;Top value is a, bottom is b&quot;, x = &quot;Datapoint x&quot;, y = &quot;Density f(x;a,b)&quot;) + theme(text = element_text(size = 22)) The Beta distribution is very flexible; different values of a and b give very different shapes. If we thought extreme values (close to 0 or 1) of \\(\\theta\\) were likely, we could choose a prior with a and b both less than 1; if we think middle values are more likely, we can choose a and b to be greater than 1. For our example, we will choose a Beta(12,12) distribution, for reasons we will discuss below in the section on choosing prior distributions. This looks like this: data_frame(x = c(0.01,0.99)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = dbeta, args = list(shape1 = 12,shape2 = 12), colour = &quot;blue&quot;) + labs(title = &quot;Beta Prior for Theta&quot;, subtitle = &quot;Bayesian Coin Flipping Example&quot;, x = &quot;Theta&quot;, y = &quot;Prior Density, p(Theta)&quot;) + scale_x_continuous(breaks = seq(0,1,by=0.1)) Warning: `data_frame()` is deprecated, use `tibble()`. This warning is displayed once per session. This prior puts strong weight on the coin being close to fair; values below \\(\\theta = 0.3\\) and \\(\\theta = 0.7\\) have very little prior probability. This can be verified: # Prior probability of theta being between 0.3 and 0.7 pbeta(0.7,shape1=12,shape2=12) - pbeta(0.3,shape1=12,shape2=12) [1] 0.9571 Most of the mass of the distribution is between \\(0.3\\) and \\(0.7\\). 11.1.2 The Posterior Our prior has density \\[ p(\\theta;a,b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}(1-\\theta)^{b-1} \\] Our likelihood remains the same as before: \\[ p(X|\\theta) = \\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}} \\] Bayes’ Rule is still used to compute the posterior from these quantities; however, it looks slightly different now: \\[ \\begin{aligned} p(\\theta|X) &amp;= \\frac{p(X|\\theta)p(\\theta)}{p(X)} \\\\ &amp;= \\frac{p(X|\\theta)p(\\theta)}{\\int_{0}^{1}p(X|\\theta)p(\\theta)d\\theta} \\end{aligned} \\] Now, because \\(\\theta\\) is defined on a continuous interval, the marginal likelihood/model evidence/normalizing constant is computed via integrating the joint distributionn of \\(X\\) and \\(\\theta\\) over the range of \\(\\theta\\). In this example, the marginal likelihood and the posterior can be computed explicitly as follows: \\[ \\begin{aligned} p(X) &amp;= \\int_{0}^{1}p(X|\\theta)p(\\theta)d\\theta \\\\ &amp;= \\int_{0}^{1} \\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}} \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}(1-\\theta)^{b-1} d\\theta \\\\ &amp;= \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\times \\int_{0}^{1} \\theta^{\\sum_{i=1}^{n}x_{i} + a - 1}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i} + b - 1} d\\theta \\\\ &amp;= \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\times \\frac{\\Gamma(\\sum_{i=1}^{n}x_{i} + a)\\Gamma(n - \\sum_{i=1}^{n}x_{i} + b)}{\\Gamma(n + a + b)} \\end{aligned} \\] How did we evaluate that integral and get to the last line? We recognized the integrand as being the \\(\\theta\\)-dependent part of a \\(Beta(\\sum_{i=1}^{n}x_{i} + a,n - \\sum_{i=1}^{n}x_{i} + b)\\) density; hence it integrates to the reciprocal of the appropriate normalizing constant. This trick is commonly used in examples illustrating Bayesian inference; it shouldn’t be taken from this, though, that this integral is always easy to evaluate like this. It is almost never easy, or even possible, to evaluate this integral in anything beyond these simple examples- more on this later. Exercise: use this trick to “show” that \\[\\begin{equation} \\int_{-\\infty}^{\\infty}e^{-x^{2}}dx = \\sqrt{\\pi} \\end{equation}\\] and \\[\\begin{equation} \\int_{0}^{\\infty}x^{\\alpha}e^{-\\beta x} = \\frac{\\Gamma(\\alpha + 1)}{\\beta^{(\\alpha + 1)}} \\end{equation}\\] where \\(\\Gamma(\\cdot)\\) is the Gamma function. Hint: the wikipedia articles on the Normal and Gamma distributions will be helpful. With \\(p(X)\\) available, we can explicitly compute the posterior: \\[ \\begin{aligned} p(\\theta|X) &amp;= \\frac{p(X|\\theta)p(\\theta)}{p(X)} \\\\ &amp;= \\frac{\\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}} \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}(1-\\theta)^{b-1}}{\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\times \\frac{\\Gamma(\\sum_{i=1}^{n}x_{i} + a)\\Gamma(n - \\sum_{i=1}^{n}x_{i} + b)}{\\Gamma(n + a + b)}} \\\\ &amp;= \\frac{\\Gamma(n + a + b)}{\\Gamma(\\sum_{i=1}^{n}x_{i} + a)\\Gamma(n - \\sum_{i=1}^{n}x_{i} + b)} \\times \\theta^{\\sum_{i=1}^{n}x_{i} + a - 1}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i} + b - 1} \\end{aligned} \\] which we recognize as a \\(Beta(a + \\sum_{i=1}^{n}x_{i},b + n - \\sum_{i=1}^{n}x_{i})\\) distribution. Priors where the posterior belongs to the same family of distributions as the prior are called conjugate priors, and while they represent the minority of practical applications, they are very useful for examples. In this scenario, we can interpret the likelihood as directly updating the prior parameters, from \\((a,b)\\) to \\((a + \\sum_{i=1}^{n}x_{i},b + n - \\sum_{i=1}^{n}x_{i})\\). Let’s visualize this for a few different datasets and sample sizes, for our chosen prior: prior &lt;- function(theta) dbeta(theta,shape1 = 12,shape2 = 12) posterior &lt;- function(theta,sumx,n) dbeta(theta,shape1 = 12 + sumx,shape2 = 12 + n - sumx) data_frame(x = c(0.01,0.99)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = prior, colour = &quot;blue&quot;) + stat_function(fun = posterior, args = list(sumx = 5,n = 10), colour = &quot;purple&quot;) + stat_function(fun = posterior, args = list(sumx = 0,n = 10), colour = &quot;red&quot;) + stat_function(fun = posterior, args = list(sumx = 10,n = 10), colour = &quot;orange&quot;) + labs(title = &quot;Beta Prior vs Posterior for Theta, 10 coin flips&quot;, subtitle = &quot;Blue: Prior. Purple: 5 heads. Red: 0 heads. Orange: 10 heads&quot;, x = &quot;Theta&quot;, y = &quot;Density&quot;) + scale_x_continuous(breaks = seq(0,1,by=0.1)) Some interesting points can be seen from this plot: When the observed data “matches” the prior, in the sense that we observe a dataset for which the original frequentist estimate of \\(\\theta\\) is very probable under the prior, the posterior becomes more peaked around that value. When the observed data are extreme, as in the case of 0 or 10 heads in 10 flips, the frequestist inference would also be extreme. In these cases, we would have estimated \\(\\hat{\\theta}_{freq} = 0\\) or \\(1\\). Because our prior distribution is not extreme, though, the posterior is more moderate, and ends up being peaked at the low/high end of the range of values that were reasonable under the prior. Exercise: Now is a good time to go back to the Shiny App you saw a few weeks ago, and see if you can elucidate the behaviour I describe above using your own simulations. 11.2 Estimation in Bayesian Inference: point and interval estimation With the posterior in hand, what do we actually do? We’re used to immediately having a point estimate from frequentist inference, and there we typically proceed to derive a confidence interval for the parameter using the sampling distribution of the estimator. The situation isn’t so different here. The posterior is analagous to the sampling distribution in the frequentist case, although the interpretation is different. In frequentist inference, we make statements about probable values for estimates in repeated sampling at fixed parameter values, and use this to infer the value of the parameter under which our observed data was most likely. In Bayesian inference, the posterior distribution is intepreted literally as the conditional distribution of the parameter given the data. We can just directly say things like “there is a \\(95\\%\\) chance that \\(\\theta\\) is between \\(0.4\\) and \\(0.5\\)”. Specifically, to obtain point estimates of parameters, we may use either the posterior mean: \\[ \\hat{\\theta}_{post. mean} = E(\\theta|X) = \\int\\theta p(\\theta|X)d\\theta \\] interpreted as a weighted average of possible values of \\(\\theta\\), with weights corresponding to their posterior probabilities (densities); or, we may use the posterior mode: \\[ \\hat{\\theta}_{post. mode} = \\mbox{argmax}_{\\theta} p(\\theta|X) \\] which is the most probable value of \\(\\theta\\), given the observed data. In simple examples the posterior may be symmetric or nearly symmetric, so the two are nearly the same; in more complicated applications, either one is directly preferred, or both are computed and compared. For interval estimation, the frequentist notion of a confidence interval is replaced by a credible interval: an interval which has a specified probability of containing the parameter, given the observed data. Contrast this interpretation to that of the frequentist confidence interval, which states that a certain proportion of the intervals computed in the same manner from repeatedly sampled datasets would contain the parameter. The Bayesian credible interval interpretation is closer to how many people would interpret such an interval intuitively. Remark: technically, any interval \\(I\\) with \\(P(\\theta\\in I|X) = 1 - \\alpha\\) is a \\((1-\\alpha)\\times 100\\%\\) credible interval for \\(\\theta\\), having observed \\(X\\). In practical applications and certainly in this course, we choose the “Highest Posterior Density” credible interval– the shortest possible \\((1-\\alpha)\\times 100\\%\\) credible interval. This is the one that falls in the “centre” of the distribution, and is computed using quantiles, much in the same was as a confidence interval. It’s important that you know this, but going forward you can just assume we’re using credible intervals derived in this manner and don’t have to think about it each time. Computing such a credible interval is a matter of finding the corresponding quantiles of the posterior, which is either simple or complicated depending on what the posterior is. In our example the posterior is known completely, and we can get a \\(95\\%\\) credible interval using the qbeta function: # E.g. for n = 10 and sumx = 5 c(qbeta(0.025,shape1=12 + 5,shape2 = 12 + 10 - 5),qbeta(0.975,shape1=12 + 5,shape2 = 12 + 10 - 5)) [1] 0.33544 0.66456 The point estimate based off the posterior mode is the same as the frequentist estimate for these data, \\(\\hat{\\theta}_{freq} = \\hat{\\theta}_{post. mode} = 0.5\\), as can be seen from the plot above. The corresponding frequentist confidence interval is given by c(.5 - sqrt(.5*.5/10)*1.96,.5 + sqrt(.5*.5/10)*1.96) [1] 0.1901 0.8099 which is much wider. The Bayesian approach gives a more accurate estimate here, because we assumed strong prior information that ended up agreeing with the data. If the data had been more extreme, say \\(X = 1\\) heads in \\(n = 10\\) flips, then the situation is different. The frequentist point estimate would be \\(\\hat{\\theta}_{freq} = 0.1\\) with confidence interval: c(.1 - sqrt(.1*.9/10)*1.96,.1 + sqrt(.1*.9/10)*1.96) [1] -0.085942 0.285942 Observing a single head in \\(10\\) tosses leads us to believe strongly that \\(\\theta\\) must be small, and the corresponding confidence interval actually goes beyond the parameter space. It is true that if the coin were fair (\\(\\theta = 0.5\\)), the observed data had about a \\(1\\%\\) chance of occurring. But, if this did occur, we’d still want to make sensible inferences! If we had a prior belief that the coin had a probability of heads that is anywhere between \\(0.3\\) and \\(0.7\\), as above, we can compute the point and interval estimates obtained in a Bayesian setting: # Point estimate- find the posterior mode, which is a critical point # Use R&#39;s built in optimization tools, function nlminb() performs constrained minimization # Pass it a function that returns minus the posterior; minimizing -f(x) is the same as maximizing f(x) minus_posterior &lt;- function(theta,sumx,n) -1 * posterior(theta,sumx,n) opt &lt;- nlminb(objective = minus_posterior,start = 0.3,sumx = 1,n = 10,lower = 0.01,upper = 0.99) opt$par # Return the value at which the maximum occurs, i.e. the posterior mode [1] 0.375 # Credible interval c(qbeta(0.025,shape1=12 + 1,shape2 = 12 + 9),qbeta(0.975,shape1=12 + 1,shape2 = 12 + 9)) [1] 0.22907 0.54875 This interval is much more reasonable, stays within the parameter space, and still even includes the possibility that the coin is fair– good, since we only flipped the coin \\(n = 10\\) times! Had we increased the sample size and observed a similarly extreme result, the posterior would become more centered in that region of the parameter space- that is, observing an equally extreme result with more data would diminish the effect of the prior on the resulting inferences. You can play around once more in the Shiny App to get a feel for the comparison between frequentist confidence intervals and bayesian credible intervals works in this example. 11.3 Choosing a Prior The following section on choosing a prior distribution is more subjective, and doesn’t include any calculations. It is still part of the course material and important to understand. The choice of prior distribution is up to the analyst. There is no formula for doing this that will work in every problem; we can, though, discuss a few guidelines for doing so. When choosing a prior, you should consider at a minimum: Reasonability: does the chosen prior give reasonable prior estimates for parameters, having observed no data? Put another way, does it put mass in regions of the parameter space where the parameter is likely to be, and does it put mass in regions where it is not likely to be? Sensitivity: how much does the prior we choose actually affect the posterior, and in what ways? Does a given prior get “swamped” by the data, and how much data does it take for the prior to have negligible effect on the posterior? Does the prior affect the posterior differently for more “extreme” data than for less “extreme” data? Tractability: do the prior and likelihood combine to give a posterior for which we can compute point estimates and credible intervals (quantiles)? Can we evaluate the integral required to compute the normalization constant? Can the posterior density/distribution (with or without the constant) be evaluated with a reasonable computational complexity? These are just some of the questions to ask when choosing a prior. It may sound like more work that in the frequentist paradigm, but an advantage of the Bayesian approach is that it makes it relatively simple for us to ask these questions of our modelling procedure. How did we choose our prior for the coin-flipping example? To begin, we knew that the parameter of interest, \\(\\theta\\), was bounded on \\((0,1)\\) and could take any real value in that region, so we considered only distributions that were continuous and defined on \\((0,1)\\). That alone narrowed it down- then we thought about what shape we wanted the distribution to have. We didn’t really have any idea about this, so we picked a distribution with a very flexible shape. We then chose hyperparameters (the parameters of the prior distribution, that we specify in advance) that gave us a reasonable location and spread of this distribution (more on this below). We then did a sensitivity analysis, showing the prior/posterior for various potential observed datasets, and even used a simple Shiny App to get a feel for how different priors and datasets would combine in the posterior. All this was to ensure that our choice gives reasonable inferences for datasets that we could possibly/are likely to see. If this is sounding like it should be easy, it isn’t. I used a concept we haven’t learned yet that renders a Beta distribution an “obvious” choice for a prior on \\(\\theta\\) for a \\(Bern(\\theta)\\) distribution: the Beta is the conjugate prior for the Bernoulli. 11.3.1 Conjugate Priors A conjugate prior, in relation to a specific likelihood, is a prior that when combined with that likelihood gives a posterior with the same functional form as that prior. The Beta/Bernoulli we saw above is an example of this, because we found: \\[ \\begin{aligned} \\mbox{Prior: } &amp; p(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\\\ \\mbox{Likelihood: } &amp; \\ell(X|\\theta) \\propto \\theta^{\\sum_{i=1}^{n}x_{i}}(1-\\theta)^{n - \\sum_{i=1}^{n}x_{i}} \\\\ \\implies \\mbox{ Posterior: } &amp; p(\\theta | X) \\propto \\theta^{a + \\sum_{i=1}^{n}x_{i} - 1}(1-\\theta)^{b + n - \\sum_{i=1}^{n}x_{i} - 1} \\end{aligned} \\] The prior has the form \\(\\theta^{a-1}(1-\\theta)^{b - 1}\\), and the posterior has the form \\(\\theta^{c-1}(1-\\theta)^{d - 1}\\), with \\(c\\) and \\(d\\) depending on \\(a\\) and \\(b\\) as well as the data. The posterior has the same functional form as the prior, with parameters that are updated after the data is observed. Conjugate priors are great because they are mathematically tractible, and allow us to easily evaluate the impact of the prior distribution on the posterior under different datasets. It is often not possible, though, to find a conjugate prior for a given likelihood in anything but the most simple examples. Exercise: Here are some common likelihoods and their conjugate priors; as an exercise, verify that each posterior is in the same family as the prior, and find expressions for the updated parameters: Likelihood Prior Posterior Bernoulli or Binomial, \\(P(X = x) = \\theta^{x}(1-\\theta)^{1-x}\\) \\(\\theta \\sim Beta(a,b)\\) ??? Poisson, \\(P(X = x) = \\frac{\\lambda^{x} e^{-\\lambda}}{x!}\\) \\(\\lambda \\sim Gamma(a,b)\\) ??? Normal, \\(f(x|\\mu,\\tau) = \\sqrt{\\frac{\\tau}{2\\pi}}\\exp\\left( -\\frac{\\tau}{2} (x - \\mu)^{2} \\right)\\) (note \\(\\tau = 1/\\sigma^{2}\\) is called the precision, and is the inverse of the variance) \\(\\mu \\sim Normal(m,v)\\), \\(\\tau \\sim Gamma(a,b)\\) ??? Wikipedia has a great list containing many more examples. 11.3.2 Setting Hyperparameters by moment-matching When using a conjugate prior (or any prior), once a family of distributions like Beta, Normal, Gamma, etc is chosen, the analyst still needs to set hyperparameter values. We did this above- first we chose a \\(Beta(a,b)\\) family of distributions, then we went a step further and actually specified \\(a = 12\\) and \\(b = 12\\). How did we come up with such wonky looking values of \\(a\\) and \\(b\\)? We will discuss two ways here. A very direct way to encode your prior beliefs about the range of reasonable values of a parameter into a prior distribution is by setting hyperparameters via moment-matching. Analagous to the Method of Moments in frequentist estimation, we pick prior moments (mean, variance, etc) that give us a sensible range of values for the parameter, then find the prior hyperparameters that give us those moments. This is where we got the \\((12,12)\\) in the above example. Suppose we think that, prior to seeing any data, \\(\\theta\\) is most likely to be around \\(0.5\\), with values on in either direction away from this being equally likely, and that \\(\\theta\\) is most likely between \\(0.3\\) and \\(0.7\\). Translate this statement into mathematical terms: we think the prior should be peaked at \\(0.5\\) and be symmetric about that value, which implies that its mean is also \\(0.5\\). We think that “most” of the mass should be between \\(0.3\\) and \\(0.7\\), so let’s say that \\(0.3\\) and \\(0.7\\) should be two standard deviations away from \\(E(\\theta) = 0.5\\). This gives \\(SD(\\theta) = 0.1\\). A \\(Beta(a,b)\\) distribution has mean \\(E(\\theta) = \\frac{a}{a+b}\\) and \\(Var(\\theta) = \\frac{ab}{(a+b)^{2}(a+b+1)}\\). Moment-matching proceeds by setting these equal to the values we decided on above, and solving for \\(a\\) and \\(b\\): \\[ \\begin{aligned} E(\\theta) = \\frac{a}{a+b} &amp;= 0.5 \\\\ Var(\\theta) = \\frac{ab}{(a+b)^{2}(a+b+1)} &amp;= 0.1^{2} \\\\ \\implies (a,b) &amp;= (12,12) \\end{aligned} \\] As an exercise, verify the solutions to the above equations. We can verify that our answer is correct computationally by taking a sample from a Beta distribution with these parameters, and checking that the mean and standard deviation are close to what we want: x &lt;- rbeta(1000,12,12) c(mean(x),sd(x)) [1] 0.50084 0.10171 library(tidyverse) "],["section-predictive-modelling.html", "Chapter 12 Predictive Modelling 12.1 Overview 12.2 Plug-in prediction: coin flipping 12.3 Bayesian Prediction: coin flipping 12.4 Extended example: predicting call centre wait times", " Chapter 12 Predictive Modelling This chapter is a supplementary tutorial on predictive modelling for STA238. The readings on this topic are An Introduction to Statistical Learning with Applications in R (ISLR), sections 2.1, 2.1.1, 2.2, 2.2.1, and 2.2.2; and Probability and Statistics: the Science of Uncertainty, section 7.2.4. The former has an overview of predictive modelling; the latter treats the Bayesian perspective. The assigned exercises associated with this material are from ISLR Section 2.4. Do questions 1, 2, 3, 4, 5, 8, 9, 10. 12.1 Overview Up to this point in the course, we have been focussing on data analysis and inference. Analyzing the specific data that you have available helps answer the question “what happened?”. Combining the available data with computational and mathematical modelling helps answer the question “what can be learned about the world, based on what happened?”. Predictive modelling focusses on answering “what is going to happen next?”. While the former two questions are of primary scientific interest, modern data science in business and industrial practice is often concerned with making predictions about what kind of data will be seen in the future. Predictive modelling, which is itself a science, is applied to address this question. Often, the point estimates we get from our inferential procedures are themselves predictions. For example, we built a regression model of TTC ridership revenue over time back in chapter 4 of these supplementary notes. We inferred the slope of the regression line of revenue across years– but we could also use this line to make predictions for future years by simply extending the line to the year we want, and reading off its value. However, when measuring the uncertainty in our predictions, the situation is different. When doing inference, we have one source of uncertainty that we choose to address: the variability in the data. If we sampled the dataset again, we would get different inferences, and so on. In prediction, we have two sources of uncertainty: that from the model that we use for prediction, which we inferred from the data and hence is subject to variability; and that from the act of sampling a new datapoint from this model. ISLR refers to these two sources of error as reducible and irreducible. We can always make our inferences less uncertain in theory by getting more data (reducibile uncertainty), but even if we knew the underlying data-generating mechanism perfectly, its output is still random (irreducible uncertainty)! In fact: recall we measured the quality of an estimator \\(\\hat{\\theta}\\) of a parameter \\(\\theta\\) using the Mean Squared Error (MSE): \\(\\mathbb{E}(\\hat{\\theta} - \\theta)^{2}\\). The analagous concept in prediction is the Mean Square Prediction Error (MSPE). Suppose we use some function \\(\\hat{Y} \\equiv \\hat{Y}(Y_{1},\\ldots,Y_{n})\\) to predict a new \\(Y_{n+1}\\) using a random sample \\(Y_{1},\\ldots,Y_{n}\\), where \\(Y_{i}, i = 1\\ldots n+1\\) are drawn IID from the same (unknown) distribution. Then define: \\[\\begin{equation} MSPE(\\hat{Y}) = \\mathbb{E}(\\hat{Y}_{n+1} - Y_{n+1})^{2} \\end{equation}\\] ISLR (page 19) makes an argument that in the simple model \\(Y = \\theta + \\epsilon\\) where \\(\\epsilon\\) is a zero-mean “noise” random variable which is independent of both \\(Y_{n+1}\\) and \\(\\hat{Y}_{n+1}\\) and \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\), that \\[\\begin{equation}\\begin{aligned} MSPE(\\hat{Y}) &amp;= \\mathbb{E}(\\hat{\\theta} - \\theta)^{2} + \\mathbb{E}(\\epsilon^{2}) \\\\ &amp;= MSE(\\hat{\\theta}) + \\text{Var}(\\epsilon) \\end{aligned}\\end{equation}\\] Exercise: prove this formula. Hint: look at what ISLR does, figure out the differences in notation between us and them, and fill in the details. The key takeaway is that the prediction error comes from two sources: error in inferring the underlying data-generating mechanism, and error in drawing new data from this mechanism. 12.2 Plug-in prediction: coin flipping To illustrate the idea of multiple sources of error, let’s briefly revist our now-classic coin flipping example. We are going to Flip a bunch of coins, and use this sample to… …estimate the probability of heads, and then use this estimate to… …predict the results of the next toss. This is an example of a classification problem: we are predicting a binary event, something that either happens or it doesn’t. We’re either completely right, or completely wrong. Note that the MSPE here has a special form, because \\(|\\hat{Y} - Y| \\in \\left\\{ 0,1 \\right\\}\\). First let’s draw a sample. We’ll leave the parameters to be specified at the top of the program so you can play around with them: set.seed(87886) n &lt;- 10 # Number of flips p0 &lt;- 0.5 # True probability of heads # Draw the sample samp &lt;- rbinom(n,1,p0) samp [1] 1 0 0 0 1 0 1 0 0 1 We have drawn \\(n = 10\\) samples from a \\(\\text{Bernoulli}(p)\\) distribution with parameter \\(p = 0.5\\). If \\(Y_{i}\\) is a \\(0/1\\) indicator of the \\(i^{th}\\) toss being heads, the likelihood function for \\(p\\) is given by: \\[\\begin{equation} L(p) = p^{\\sum_{i=1}^{n}Y_{i}}(1-p)^{n - \\sum_{i=1}^{n}Y_{i}} \\end{equation}\\] Exercise (review): show that the maximum likelihood estimator is \\(\\hat{p} = \\frac{1}{n}\\sum_{i=1}^{n}Y_{i}\\), the sample proportion of heads. We can compute the MLE for our sample: pmle &lt;- mean(samp) pmle [1] 0.4 How to use this to make predictions? We need a rule—a function \\(\\hat{Y}\\)—that takes in our sample and predicts the next value. This construction is very similar in spirit to constructing an estimate of a parameter; we want something that makes intuitive sense and that has “desirable” statistical properties. It can be shown that the prediction rule that minimizes the MSPE for this simple case of binary prediction is to simply predict the outcome (heads or tails, 0 or 1) that is the most probable under our estimated model. Our inferred model ascribes a \\(40\\%\\) chance of heads and \\(60\\%\\) chance of tails to each flip, so our prediction rule (for this sample) is simply \\(\\hat{Y} = 0\\). Exercise: suppose that \\(p_{0} = \\hat{p} = 0.4\\), that is, suppose we were able to perfectly infer the value of \\(p\\) based on our sample and hence incur no model error. Show that the MSPE for our prediction rule is \\(\\mathbb{E}(\\hat{Y} - Y)^{2} = p_{0} = 0.4\\). Exercise (challenge): now suppose that the true value of \\(p\\) is \\(p_{0} = 0.5\\) (which it actually is, in our simulation). Show that the MSPE of our prediction rule equals \\(0.5\\) when you incorporate the model error into the calculation. Hint: recall the “tower property” of expectation: for any two random variables \\(X,Y\\), \\[\\begin{equation} \\mathbb{E}(X) = \\mathbb{E}_{Y}\\mathbb{E}_{X|Y}(X) \\end{equation}\\] That is, you can first take the expectation with respect to one random variable conditional on another, and then take the expectation of this expression with respect to the second random variable. Use this to separate the model variability from the sampling variability. Let’s assess the MSPE using a prediction. We’re going to repeat the following procedure a bunch of times: set.seed(874327086) n &lt;- 10 # Number of flips p0 &lt;- 0.5 # True probability of heads B &lt;- 1000 # Number of simulations to do do_simulation &lt;- function(n,p) { pmle &lt;- mean(rbinom(n,1,p)) # Compute the MLE newy &lt;- rbinom(1,1,p) if (pmle &gt;= 0.5) { predy &lt;- 1 # Predict 1 if pmle &gt;= 0.5 } else { predy &lt;- 0 } (predy - newy)^2 } # Do the simulation sims &lt;- numeric(B) for (b in 1:B) sims[b] &lt;- do_simulation(n,p0) sim_mspe &lt;- mean(sims) sim_mspe [1] 0.498 Notice that it is close to \\(0.5\\), not \\(0.4\\)– the model error matters, as you saw in the above theoretical exercise. Actually, to drive this point home… Exercise: what did I do differently in the below code, compared to the above? set.seed(874327086) n &lt;- 10 # Number of flips p0 &lt;- 0.5 # True probability of heads B &lt;- 1000 # Number of simulations to do do_simulation &lt;- function(n,p) { pmle &lt;- mean(rbinom(n,1,p)) # Compute the MLE newy &lt;- rbinom(1,1,pmle) if (pmle &gt;= 0.5) { predy &lt;- 1 # Predict 1 if pmle &gt;= 0.5 } else { predy &lt;- 0 } (predy - newy)^2 } # Do the simulation sim_mspe2 &lt;- 1:B %&gt;% map(~do_simulation(n = n,p = p0)) %&gt;% reduce(c) %&gt;% mean() sim_mspe2 [1] 0.389 Figure out why it’s lower, computationally (what did I do differently in the code), mathematically (relate it to the above exercises), and intuitively (explain in words what the difference is). 12.3 Bayesian Prediction: coin flipping We saw above that making predictions using our frequentist-inferred models for the observed data wasn’t that complicated (just “plug in” the inferred model parameters), but assessing the uncertainty in the predictions was challenging because the uncertainty comes from multiple different sources, and it is not always clear how to effectively combine these. One more systematic way to quantify the uncertainty in a prediction, conditional on the observed sample is, well, to literally do this, via the conditional distribution \\(\\pi(Y_{n+1}|Y_{1},\\ldots,Y_{n})\\). The mean of this distribution says “what will happen on average, given what we have observed?”, and the standard deviation of this distribution quantifies how close future values are to be concentrated around this mean, which seems like a perfectly reasonable measure of the uncertainty in a prediction. So what’s the problem? If we’re in the frequentist paradigm, there is a big one: the data are IID! So \\(Y_{n+1}\\) is statistically independent of \\(Y_{1},\\ldots,Y_{n}\\), and \\[\\begin{equation} \\pi(Y_{n+1}|Y_{1},\\ldots,Y_{n}) = \\pi(Y_{n+1}) \\end{equation}\\] Under this paradigm, the sample provides no direct information about the distribution of a new datapoint. In the Bayesian paradigm, things are a bit different. Following Evans and Rosenthal Section 7.2.4, we consider the posterior predictive distribution: \\[\\begin{equation} \\pi(Y_{n+1}|Y_{1},\\ldots,Y_{n}) = \\int_{0}^{1}\\pi(Y_{n+1}|p)\\pi(p|Y_{1},\\ldots,Y_{n})dp \\end{equation}\\] where \\(\\pi(p|Y_{1},\\ldots,Y_{n})\\) is the posterior distribution of \\(p\\) given the sample, and \\(\\pi(Y_{n+1}|p)\\) is simply the likelihood for each value of \\(p\\), and for a single datapoint. This has the following attractive properties: Let’s see what this looks like for the coin flipping example. Recall the setup: we use a \\(\\text{Beta}(\\alpha,\\beta)\\) prior for the parameter \\(p\\), which leads to the posterior: \\[\\begin{equation} \\pi(p|Y_{1},\\ldots,Y_{n}) = \\text{Beta}\\left(n\\bar{Y} + \\alpha,n(1 - \\bar{Y}) + \\beta\\right) \\end{equation}\\] Evans and Rosenthal derive the posterior predictive distribution using messier, but similar algebra to how we derived the posterior: \\[\\begin{equation} Y_{n+1} | Y_{1},\\ldots,Y_{n} \\sim \\text{Bernoulli}\\left(\\frac{n\\bar{Y} + \\alpha}{n + \\alpha + \\beta}\\right) \\end{equation}\\] Exercise: derive the formula for the posterior predictive as reported by Evans and Rosenthal. We use the posterior predictive mode for prediction. Since the posterior predictive distribution is only defined at two values, \\(0\\) and \\(1\\), the mode is simply whichever value has the higher posterior predictive density. Exercise: prove Evans and Rosenthal’s formula for the posterior predictive mode: \\[\\begin{equation} \\hat{Y}_{n+1} | Y_{1},\\ldots,Y_{n} = \\begin{cases} 1 \\ \\text{if} \\ \\frac{n\\bar{Y} + \\alpha}{n + \\alpha + \\beta} \\geq \\frac{n(1-\\bar{Y}) + \\beta}{n + \\alpha + \\beta} \\\\ 0 \\ \\text{else} \\end{cases} \\end{equation}\\] Hint: start by writing out the Bernoulli density. How does the use of the posterior predictive mode do in terms of MSPE? Consider the setup from chapter 11 of these supplementary notes, where we put a \\(\\text{Beta}(12,12)\\) prior on \\(p\\). Let’s simulate the MSPE again, but using this for prediction: set.seed(874327086) n &lt;- 10 # Number of flips p0 &lt;- 0.5 # True probability of heads alpha &lt;- 12 # Prior beta &lt;- 12 # Prior B &lt;- 1000 # Number of simulations to do do_simulation_bayes &lt;- function(n,p) { pmle &lt;- mean(rbinom(n,1,p)) # Compute the MLE newy &lt;- rbinom(1,1,p) # Draw a new value from the true distribution postparam &lt;- (n * pmle + alpha) / (n + alpha + beta) if (postparam &gt;= (1 - postparam)) { predy &lt;- 1 # Predict 1 if postparam &gt;= (1 - postparam) (i.e. postparam &gt;= 0.5) } else { predy &lt;- 0 } (predy - newy)^2 } # Do the simulation sim_bayes &lt;- numeric(B) for (b in 1:B) sim_bayes[b] &lt;- do_simulation_bayes(n,p0) sim_mspe_bayes &lt;- mean(sim_bayes) sim_mspe_bayes [1] 0.498 Notice anything odd about these results compared to what we got using the frequentist approach? It turns out that if the prior is symmetric (\\(\\alpha = \\beta\\)), then the posterior mode exactly equals the prediction rule we obtained by minimizing the MSPE. If the prior is not symmetric, they won’t be the same– so which do you expect to be better? Exercise: repeate the above simulation, but change the prior so that it is not symmetric (make \\(a\\neq b\\)). What is the \\(MSPE\\) of the Bayesian prediction? 12.4 Extended example: predicting call centre wait times The coin-flipping example is illustrative, but too simple to be interesting. For a given dataset, we either predict heads, or tails, the same for all future coins! For a more interesting example, we’ll fit a curve to Toronto 311 contact centre wait times. We’ll use a minor extension of the linear regression we saw in chapter 4 of these supplementary notes. It’s more important to focus on what we’re doing (prediction) than how we’re doing it (linear regression with polynomials). Materials in this tutorial are taken from the (much) more advanced tutorial by Alex on this stuff, here. 311 is a service operated by the City of Toronto in which residents can call (dial *311) and submit service requests for things like potholes, excess garbage pickup, downed trees, and so on. People call in to the service and speak to an actual human being in a contact centre. When you call, sometimes you have to wait. Data on the daily average wait time for such calls for the period from December 28th, 2009 to January 2nd, 2011 is available from Open Data Toronto here, and stored on github by us for your use. Let’s read in and visualize the data: # Read data contactdat &lt;- readr::read_csv( file = &quot;https://media.githubusercontent.com/media/awstringer1/leaf2018/gh-pages/datasets/contact-centre.csv&quot;, col_names = TRUE, col_types = &quot;cd&quot;) %&gt;% mutate_at(&quot;date&quot;,lubridate::mdy) glimpse(contactdat) Observations: 371 Variables: 2 $ date &lt;date&gt; 2009-12-28, 2009-12-29, 2009-12-30, 2009-12-31, 2010-01… $ answer_speed &lt;dbl&gt; 55, 67, 56, 15, 9, 62, 51, 20, 16, 14, 23, 33, 14, 23, 1… contactdat %&gt;% ggplot(aes(x = date,y = answer_speed)) + theme_classic() + geom_point(pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;) + labs(title = &quot;Average Wait Time by Day, Toronto 311 Contact Centre&quot;, x = &quot;Date&quot;, y = &quot;Wait Time&quot;) Woah! That spike in wait times in July is terrible. If we were analyzing these data with the intent of informing any actual policy, we would contact the 311 office and ask what the heck happened there before proceeding. Let’s proceed. To mitigate the drastic spike, consider a log transformation of the answer_speed: contactdat %&gt;% ggplot(aes(x = date,y = log(answer_speed))) + theme_classic() + geom_point(pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;) + labs(title = &quot;Average Wait Time by Day, Toronto 311 Contact Centre&quot;, x = &quot;Date&quot;, y = &quot;log(Wait Time)&quot;) There’s a lot of variance, but it looks like there might be a kind of quadratic trend. Recall the simple linear model from Chapter 4 of these supplementary notes: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\] where \\(y_{i}\\) is the average wait time on the \\(i^{th}\\) date, \\(x_{i}\\) is a suitable numerical value for the \\(i^{th}\\) date (e.g. number of days since some reference point), and \\(\\epsilon_{i} \\sim N(0,\\sigma^{2})\\) is the error of the \\(i^{th}\\) observation about its mean. Because it looks like the trend in the data might be quadratic, not linear, we can extend this slightly: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{i}^{2} + \\epsilon_{i} \\] In general, it’s not too much of an extension to consider models of the form \\[ y_{i} = \\beta_{0} + \\sum_{k=1}^{p}\\beta_{k}x_{i}^{k} + \\epsilon_{i} \\] The higher we choose \\(p\\), the degree of polynomial, the closer we will fit to the observed data, and the better we will be able to predict the datapoints we have already seen. However, paradoxically, models that are too complicated will actually predict new data worse! This is due in part to the variance-bias tradeoff discussed in this week’s readings in ISLR. Let’s fit the model(s) and assess the in-sample mean-square prediction error. # THIS CODE IS ADVANCED-- the idea is important, but you won&#39;t be # tested on the following CODE. contact_std &lt;- contactdat %&gt;% mutate_at(&quot;date&quot;,as.numeric) %&gt;% mutate_at(&quot;answer_speed&quot;,log) %&gt;% mutate_all(list(~(. - mean(.)) / sd(.))) # Fit a polynomial regression of degree p fit_poly_regression &lt;- function(p) { lm(answer_speed ~ poly(date,degree = p,raw = TRUE), data = contact_std) } contactmod_1 &lt;- fit_poly_regression(1) contactmod_2 &lt;- fit_poly_regression(2) contactmod_10 &lt;- fit_poly_regression(10) contactmod_100 &lt;- fit_poly_regression(100) # Plot of the predictions prediction_plot &lt;- function(mod) { contact_std %&gt;% mutate(pred = predict(mod)) %&gt;% ggplot(aes(x = date)) + theme_classic() + geom_point(aes(y = answer_speed),pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;) + geom_line(aes(y = pred),colour = &quot;purple&quot;) } library(patchwork) (prediction_plot(contactmod_1) | prediction_plot(contactmod_2)) / (prediction_plot(contactmod_10) | prediction_plot(contactmod_100)) Exercise: I deliberately didn’t label the plots. Can you guess which plot is from which degree of polynomial? Remember that higher degree polynomials should be more wiggly than lower degrees, and should give better in-sample predictions than lower degrees. Indeed, if we compute the MSPE for each of these models, we find the higher the degree, the better the fit to the data we have observed: compute_mspe_polymod &lt;- function(mod) { mean(( predict(mod) - contact_std$answer_speed )^2) } compute_mspe_polymod(contactmod_1) [1] 0.98471 compute_mspe_polymod(contactmod_2) [1] 0.74395 compute_mspe_polymod(contactmod_10) [1] 0.59074 compute_mspe_polymod(contactmod_100) [1] 0.46629 The problem with this is that the MSPE is supposed to be for predicting a new datapoint, not one that you’ve already seen! To get a better estimate of the real MSPE, and hence tell how the algorithm might do on new datapoints, we can bootstrap it. We’ll use a slightly different kind of bootstrap, called cross-validation, where we Split the data up randomly into, say, \\(K\\) chunks, where \\(K = 10\\) (say), and Fit the model of interest to data from each combination of \\(K-1\\) chunks, and Predict the values on the \\(K^{th}\\) chunk, and then average the \\(K\\) “out-of-sample” estimates of the MSPE we get from this. There are many types of cross-validation; this one is called “K-fold” cross validation. We can implement this as follows: set.seed(809796857) do_crossval &lt;- function(p,K = 10) { n &lt;- nrow(contact_std) mspevec &lt;- numeric(K) # Split the data into K &quot;folds&quot; # First randomly shuffle the rows: contact_std_shuffle &lt;- contact_std[sample.int(n), ] # Then split the data in order: datachunks &lt;- split(contact_std_shuffle,rep(1:K,each = round(n/K))[1:n]) # Now fit the model to each combination of K-1 chunks, and predict # on the Kth chunk: for (k in 1:K) { ds &lt;- reduce(datachunks[(1:K)[-k]],bind_rows) mod &lt;- lm(answer_speed ~ poly(date,degree = p,raw = TRUE), data = ds) mspevec[k] &lt;- mean( (predict(mod,newdata = datachunks[[k]]) - datachunks[[k]]$answer_speed)^2 ) } mean(mspevec) } do_crossval(1) [1] 0.99517 do_crossval(2) [1] 0.74894 do_crossval(10) [1] 0.60901 do_crossval(100) [1] 4.1858 Woah! The fit on new data–that is, the quality of the predictions– seems to improve as the degree of polynomial increases, but only to a certain point. For the \\(p = 100\\) degree polynomial, the out-of-sample MSPE is terrible. The reason is because of the bias-variance tradeoff. The lower-degree models have some bias–they don’t predict perfectly in-sample–but they have low variance, in that the inferred model parameters tend to be similar across datasets. Consider again the \\(p = 1\\) and \\(p = 100\\) models: cowplot::plot_grid( prediction_plot(contactmod_1), prediction_plot(contactmod_100), nrow = 1 ) The \\(p = 1\\) plot (exercise: make sure you can tell which is which) doesn’t predict very many datapoints correctly. The \\(p = 100\\) hugs the observed data much closer. But because of this, if we sample a new dataset… # Take a bootstrap sample contact_new &lt;- contact_std[sample.int(nrow(contact_std),replace = TRUE), ] contactmod_new_1 &lt;- lm(answer_speed ~ poly(date,degree = 1,raw = TRUE),data = contact_new) contactmod_new_100 &lt;- lm(answer_speed ~ poly(date,degree = 100,raw = TRUE),data = contact_new) prediction_plot_new &lt;- function(mod) { contact_new %&gt;% mutate(pred = predict(mod)) %&gt;% ggplot(aes(x = date)) + theme_classic() + geom_point(aes(y = answer_speed),pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;) + geom_line(aes(y = pred),colour = &quot;purple&quot;) } cowplot::plot_grid( prediction_plot_new(contactmod_new_1), prediction_plot_new(contactmod_new_100), nrow = 1 ) The \\(p = 1\\) model remains nearly identical, while the \\(p = 100\\) model changes! High variance refers to the fact that slight changes in the sample cause big changes in the model– and this leads to bad predictions on new data. "],["section-installing-r-and-rstudio.html", "Chapter 13 Installing R and RStudio 13.1 Installing R 13.2 Installing RStudio 13.3 Using RMarkdown", " Chapter 13 Installing R and RStudio This supplementary document contains a guided tutorial on how to install R and RStudio, and create a simple document with RMarkdown. It is your responsibility to have access to a computer on which you can use these tools. We want to help! However, us providing this help should in no way be interpreted as us taking on the responsibility of getting these tools working. In particular, you have an assignment due in the second week of classes that requires these tools, and it is 100% your responsibility to complete it on time. Most of these screenshots are taken from https://rstudio.com/products/rstudio/. 13.1 Installing R R is the scientific computing language that we will use in this course to perform statistical compuations– reading and manipulating data, performing scientific computations, and creating automated reports. Install R on your Windows, MacOS or Linux platform by going to https://cran.rstudio.com/. Choose the correct link based on your operating system: This tutorial proceeds with Windows and Mac; if you’re a Linux user (good for you!), I’m assuming you’re familiar with how to install software like this. Windows: click “base”: Mac: scroll down and click on the link for the “latest version”. At the time of this writing, it’s 3.6.1, but don’t worry if it’s increased when you’re doing this. Follow the instructions for your operating system to install the software. There should be a new icon for R, like this on a Mac: and similar on Windows. Click it to open the software. You should see something like: on a Mac, and again similar on Windows. Type print(\"Hello world!\") in the console and press Enter: If it runs without errors, good job, you’ve installed R! 13.2 Installing RStudio R is the underlying platform that executes code that you write and returns results. However, you won’t usually use R directly. You will use R through RStudio, the most popular Integrated Development Environment (IDE) for R. You can download RStudio for your platform here. Select RStudio Desktop, free version, and make sure to install the correct one for your platform. I think this is displayed automatically; when I go there on my Mac, I see: Download the installation file and follow the instructions for your system. An icon will appear: Click this icon to open RStudio. You should see something like this (note: I have set my favourite visual settings, so yours will look slightly different): The IDE contains four panels. The two important ones for now are: The console. You can type R code here, and press Enter to run it. It is the same as the console from R that you saw in the previous step. The editor. This is where you will write your programs which involve more than one command– so, all of your programs. This is where I am currently writing this tutorial! It is just a text editor. You can write code, highlight it, and then press Cmd+Enter or Shift+Enter to run it. Give it a try: click the + icon in the top right corner of RStudio, and click “R script”: A blank script will open. Type print(\"Hello World!\"), highlight the code, and press Cmd+Enter (mac) or Ctrl+Enter (windows): If this works, you are successfully using RStudio! 13.3 Using RMarkdown One of the real powers of using R and RStudio is the ability to automatically create typeset documents containing the results of running your code combined with text. First, make sure the rmarkdown package is installed. Open RStudio and in the Console, type install.packages(\"rmarkdown\"). The rmarkdown package will be installed. Now, click the + icon in the top-right corner, and select “R Markdown”: Leave all the default settings as they are. You should see a new script open up, prepopulated with some text and code: Click anywhere in this document (so the cursor is in the editor) and press Cmd+Shift+K or Ctrl+Shift+K. You’ll be prompted to save the script; do this, and wait a moment, and then a document will magically be created and appear in the Viewer pane: This document is itself a tutorial for using RMarkdown. Read this; it explains how to add text, and code, right in the editor, and then magically create a report. When you change the code or the text, you create the report again, and everything is typeset properly. Not only does this save you time, but it ensures that all your output matches the code that was used to create it, reducing human error. This notion is part of the broader concept of reproducibility, which is very important in modern scientific practice. As a test of your use of RMarkdown, try to add the following three items to this report: The title “My First RMarkdown Report”. Look up at the top where it says “title”. The first few rows of the cars dataset. You can get this with the head(cars) command. My output looks like: A pairs plot of the iris dataset. You can add this with pairs(iris). My output looks like: "],["section-assigned-exercises-from-mips.html", "Chapter 14 Assigned Exercises from MIPS 14.1 Chapters 15 and 16 14.2 Chapters 13 and 14 14.3 Chapters 17 and 19 14.4 Chapter 20 14.5 Evans and Rosenthal, Chapter 7.1 14.6 Chapter 18 14.7 Chapter 21 14.8 Chapter 23 14.9 Evans and Rosenthal, Chapter 7.2", " Chapter 14 Assigned Exercises from MIPS 14.1 Chapters 15 and 16 15.1, 15.2; 15.5; 15.6; 15.8; 15.11; 15.12 16.1, 16.2; 16.3; 16.4, 16.5; 16.6; 16.8; 16.15. 14.2 Chapters 13 and 14 13.1; 13.2; 13.3; 13.4; 13.5, 13.6; 13.9; 13.10, 13.12 14.1, 14.2; 14.3; 14.5; 14.6; 14.9 14.3 Chapters 17 and 19 17.1, 17.2; 17.3; 17.4; 17.7; 17.9 19.1; 19.2, 19.3; 19.5; 19.8; 19.9. 14.4 Chapter 20 20.1; 20.2; 20.3; 20.4; 20.5; 20.8; 20.9; 20.11. 14.5 Evans and Rosenthal, Chapter 7.1 7.1.1, 7.1.2, 7.1.3; 7.1.4; 7.1.8; 7.1.9; 7.1.13, 7.1.14, 7.1.15. 14.6 Chapter 18 18.1 18.2; 18.3, 18.4; 18.6; 18.7; 18.8; 18.9; 18.11. 14.7 Chapter 21 21.1; 21.2; 21.3; 21.4; 21.5; 21.6; 21.8; 21.9; 21.11; 21.14 14.8 Chapter 23 23.1, 23.2, 23.3; 23.5; 23.6; 23.7; 23.10; 23.11 24.1, 24.2; 24.3; 24.4; 24.6; 24.9; 24.10. 14.9 Evans and Rosenthal, Chapter 7.2 7.2.1, 7.2.2, 7.2.3, 7.2.4, 7.2.5, 7.2.6, 7.2.12a, 7.2.14 library(tidyverse) library(patchwork) "],["section-extended-example-world-population-data.html", "Chapter 15 Extended Example: World Population Data 15.1 Read in and prepare the data for analysis 15.2 Model world population over time 15.3 Bayesian model for world population 15.4 Quantifying uncertainty in estimates of world population: regression model 15.5 Bayesian estimate of world population 15.6 Predicting world population", " Chapter 15 Extended Example: World Population Data In this chapter, we analyze the United Nations’ World Population Prospects data. These data contain estimated population sizes for each country for single years from 1950–2020, and projections for five year blocks from 2025–2100. We will apply methods from each chapter of this course to answer questions about these data, the underlying true population of the world, and what the population will look like in the future. More specifically, we will: Read in and prepare the data for analysis (Chapter 2), Build a simple linear regression model for the total world population over time (Chapter 4), Develop a Bayesian model for total world population (Chapter 6), Quantify uncertainty in our estimate for total world population (Chapter 9), Develop a Bayesian estimator for total world population (Chapter 11), Predict the population to the year 2100 and compare our predictions and uncertainty quantification with those reported by the UN (Chapter 12). 15.1 Read in and prepare the data for analysis The World Population Prospects data contain estimated population sizes for each country for single years from 1950–2020, and projections for five year blocks from 2025–2100. They are available for free download from that link. They are not in any form suitable for analysis, however. The data are contained in a spreadsheet with variables in both rows and columns, and summary statistics mixed in with the raw data. I have done a bit of manual editing and posted the files to the book data folder at data/worldpop/*. Let’s read in the data corresponding to the world population estimates from 1950–2020. First, look at it, either in excel or on the command line: head data/worldpop/worldpop-estimates.csv country,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020 WORLD, 2 536 431, 2 584 034, 2 630 862, 2 677 609, 2 724 847, 2 773 020, 2 822 443, 2 873 306, 2 925 687, 2 979 576, 3 034 950, 3 091 844, 3 150 421, 3 211 001, 3 273 978, 3 339 584, 3 407 923, 3 478 770, 3 551 599, 3 625 681, 3 700 437, 3 775 760, 3 851 651, 3 927 781, 4 003 794, 4 079 480, 4 154 667, 4 229 506, 4 304 534, 4 380 506, 4 458 003, 4 536 997, 4 617 387, 4 699 569, 4 784 012, 4 870 922, 4 960 568, 5 052 522, 5 145 426, 5 237 441, 5 327 231, 5 414 289, 5 498 920, 5 581 598, 5 663 150, 5 744 213, 5 824 892, 5 905 046, 5 984 794, 6 064 239, 6 143 494, 6 222 627, 6 301 773, 6 381 185, 6 461 159, 6 541 907, 6 623 518, 6 705 947, 6 789 089, 6 872 767, 6 956 824, 7 041 194, 7 125 828, 7 210 582, 7 295 291, 7 379 797, 7 464 022, 7 547 859, 7 631 091, 7 713 468, 7 794 799 Burundi, 2 309, 2 360, 2 406, 2 449, 2 492, 2 537, 2 585, 2 636, 2 689, 2 743, 2 798, 2 852, 2 907, 2 964, 3 026, 3 094, 3 170, 3 253, 3 337, 3 414, 3 479, 3 530, 3 570, 3 605, 3 646, 3 701, 3 771, 3 854, 3 949, 4 051, 4 157, 4 267, 4 380, 4 498, 4 621, 4 751, 4 887, 5 027, 5 169, 5 307, 5 439, 5 565, 5 686, 5 798, 5 899, 5 987, 6 060, 6 122, 6 186, 6 267, 6 379, 6 526, 6 704, 6 909, 7 132, 7 365, 7 608, 7 862, 8 126, 8 398, 8 676, 8 958, 9 246, 9 540, 9 844, 10 160, 10 488, 10 827, 11 175, 11 531, 11 891 Comoros, 159, 163, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 204, 207, 211, 216, 221, 225, 230, 235, 239, 244, 250, 257, 266, 276, 287, 297, 308, 318, 327, 336, 345, 355, 366, 377, 388, 400, 412, 424, 436, 449, 462, 475, 489, 502, 515, 529, 542, 556, 569, 583, 597, 612, 626, 642, 657, 673, 690, 707, 724, 742, 759, 777, 796, 814, 832, 851, 870 Djibouti, 62, 63, 65, 66, 68, 70, 71, 74, 76, 80, 84, 89, 94, 101, 108, 115, 123, 131, 140, 150, 160, 169, 179, 191, 205, 224, 249, 277, 308, 336, 359, 375, 385, 394, 406, 426, 454, 490, 529, 564, 590, 607, 615, 619, 622, 630, 644, 661, 680, 700, 718, 733, 747, 760, 772, 783, 795, 805, 816, 828, 840, 854, 868, 883, 899, 914, 929, 944, 959, 974, 988 Eritrea, 822, 835, 849, 865, 882, 900, 919, 939, 961, 983, 1 008, 1 033, 1 060, 1 089, 1 118, 1 148, 1 179, 1 210, 1 243, 1 276, 1 311, 1 347, 1 385, 1 424, 1 464, 1 505, 1 548, 1 592, 1 637, 1 684, 1 733, 1 785, 1 837, 1 891, 1 946, 2 004, 2 065, 2 127, 2 186, 2 231, 2 259, 2 266, 2 258, 2 239, 2 218, 2 204, 2 196, 2 195, 2 206, 2 237, 2 292, 2 375, 2 481, 2 601, 2 720, 2 827, 2 918, 2 997, 3 063, 3 120, 3 170, 3 214, 3 250, 3 281, 3 311, 3 343, 3 377, 3 413, 3 453, 3 497, 3 546 Ethiopia, 18 128, 18 467, 18 820, 19 184, 19 560, 19 947, 20 348, 20 764, 21 201, 21 662, 22 151, 22 671, 23 221, 23 798, 24 397, 25 014, 25 641, 26 280, 26 944, 27 653, 28 415, 29 249, 30 141, 31 037, 31 861, 32 567, 33 128, 33 577, 33 993, 34 488, 35 142, 35 985, 36 995, 38 143, 39 374, 40 652, 41 966, 43 329, 44 757, 46 272, 47 888, 49 610, 51 424, 53 296, 55 181, 57 048, 58 884, 60 697, 62 508, 64 343, 66 225, 68 159, 70 142, 72 171, 74 240, 76 346, 78 489, 80 674, 82 916, 85 234, 87 640, 90 140, 92 727, 95 386, 98 094, 100 835, 103 603, 106 400, 109 224, 112 079, 114 964 Kenya, 6 077, 6 242, 6 416, 6 598, 6 789, 6 988, 7 195, 7 412, 7 638, 7 874, 8 120, 8 378, 8 647, 8 929, 9 223, 9 530, 9 851, 10 187, 10 540, 10 911, 11 301, 11 713, 12 146, 12 601, 13 077, 13 576, 14 096, 14 639, 15 205, 15 798, 16 417, 17 064, 17 736, 18 432, 19 146, 19 877, 20 623, 21 382, 22 154, 22 935, 23 725, 24 522, 25 326, 26 136, 26 951, 27 768, 28 589, 29 416, 30 250, 31 099, 31 965, 32 849, 33 752, 34 679, 35 635, 36 625, 37 649, 38 706, 39 792, 40 902, 42 031, 43 178, 44 343, 45 520, 46 700, 47 878, 49 052, 50 221, 51 393, 52 574, 53 771 Madagascar, 4 084, 4 168, 4 257, 4 349, 4 444, 4 544, 4 647, 4 754, 4 865, 4 980, 5 099, 5 224, 5 352, 5 486, 5 625, 5 769, 5 919, 6 074, 6 234, 6 402, 6 576, 6 758, 6 947, 7 143, 7 346, 7 556, 7 773, 7 998, 8 230, 8 470, 8 717, 8 971, 9 234, 9 504, 9 781, 10 063, 10 352, 10 648, 10 952, 11 269, 11 599, 11 943, 12 301, 12 675, 13 067, 13 475, 13 903, 14 348, 14 809, 15 283, 15 767, 16 261, 16 765, 17 279, 17 803, 18 337, 18 880, 19 434, 19 996, 20 569, 21 152, 21 744, 22 347, 22 961, 23 590, 24 234, 24 894, 25 571, 26 262, 26 969, 27 691 Malawi, 2 954, 3 012, 3 072, 3 136, 3 202, 3 271, 3 342, 3 417, 3 495, 3 576, 3 660, 3 748, 3 839, 3 934, 4 032, 4 134, 4 240, 4 350, 4 464, 4 582, 4 704, 4 829, 4 959, 5 093, 5 235, 5 385, 5 546, 5 718, 5 897, 6 075, 6 250, 6 412, 6 566, 6 738, 6 965, 7 268, 7 666, 8 141, 8 637, 9 076, 9 404, 9 600, 9 686, 9 710, 9 746, 9 844, 10 023, 10 265, 10 552, 10 854, 11 149, 11 432, 11 714, 12 000, 12 302, 12 626, 12 974, 13 342, 13 728, 14 128, 14 540, 14 962, 15 396, 15 839, 16 290, 16 745, 17 205, 17 670, 18 143, 18 629, 19 130 There is one character column containing the country and then \\((2020 - 1950 + 1) = 71\\) numeric columns containing population counts. Let’s read it in with those specs: worldpop &lt;- readr::read_csv( file = &quot;data/worldpop/worldpop-estimates.csv&quot;, col_names = TRUE, col_types = stringr::str_c(c(&quot;c&quot;,rep(&quot;n&quot;,71)),collapse = &quot;&quot;) ) glimpse(worldpop) Observations: 236 Variables: 72 $ country &lt;chr&gt; &quot;WORLD&quot;, &quot;Burundi&quot;, &quot;Comoros&quot;, &quot;Djibouti&quot;, &quot;Eritrea&quot;, &quot;Ethiop… $ `1950` &lt;dbl&gt; 2, 2, 159, 62, 822, 18, 6, 4, 2, 493, 15, 5, 248, 2, 36, 2, 2… $ `1951` &lt;dbl&gt; 2, 2, 163, 63, 835, 18, 6, 4, 3, 506, 16, 6, 259, 2, 37, 2, 2… $ `1952` &lt;dbl&gt; 2, 2, 167, 65, 849, 18, 6, 4, 3, 521, 16, 6, 268, 2, 37, 2, 2… $ `1953` &lt;dbl&gt; 2, 2, 170, 66, 865, 19, 6, 4, 3, 537, 17, 6, 277, 2, 38, 2, 2… $ `1954` &lt;dbl&gt; 2, 2, 173, 68, 882, 19, 6, 4, 3, 554, 18, 6, 284, 2, 39, 2, 2… $ `1955` &lt;dbl&gt; 2, 2, 176, 70, 900, 19, 6, 4, 3, 571, 19, 6, 292, 2, 39, 2, 2… $ `1956` &lt;dbl&gt; 2, 2, 179, 71, 919, 20, 7, 4, 3, 588, 20, 6, 300, 2, 39, 2, 2… $ `1957` &lt;dbl&gt; 2, 2, 182, 74, 939, 20, 7, 4, 3, 605, 21, 6, 308, 2, 40, 2, 2… $ `1958` &lt;dbl&gt; 2, 2, 185, 76, 961, 21, 7, 4, 3, 623, 22, 6, 317, 2, 40, 2, 2… $ `1959` &lt;dbl&gt; 2, 2, 188, 80, 983, 21, 7, 4, 3, 641, 23, 7, 326, 2, 41, 2, 2… $ `1960` &lt;dbl&gt; 3, 2, 191, 84, 1, 22, 8, 5, 3, 660, 24, 7, 336, 2, 42, 2, 2, … $ `1961` &lt;dbl&gt; 3, 2, 194, 89, 1, 22, 8, 5, 3, 679, 25, 7, 345, 2, 42, 2, 2, … $ `1962` &lt;dbl&gt; 3, 2, 197, 94, 1, 23, 8, 5, 3, 698, 27, 7, 355, 3, 43, 2, 2, … $ `1963` &lt;dbl&gt; 3, 2, 200, 101, 1, 23, 8, 5, 3, 717, 28, 7, 366, 3, 45, 2, 3,… $ `1964` &lt;dbl&gt; 3, 3, 204, 108, 1, 24, 9, 5, 4, 736, 29, 7, 378, 3, 46, 3, 3,… $ `1965` &lt;dbl&gt; 3, 3, 207, 115, 1, 25, 9, 5, 4, 753, 31, 8, 391, 3, 47, 3, 3,… $ `1966` &lt;dbl&gt; 3, 3, 211, 123, 1, 25, 9, 5, 4, 770, 32, 8, 405, 3, 48, 3, 3,… $ `1967` &lt;dbl&gt; 3, 3, 216, 131, 1, 26, 10, 6, 4, 785, 33, 8, 421, 3, 49, 3, 3… $ `1968` &lt;dbl&gt; 3, 3, 221, 140, 1, 26, 10, 6, 4, 799, 34, 8, 437, 3, 50, 3, 3… $ `1969` &lt;dbl&gt; 3, 3, 225, 150, 1, 27, 10, 6, 4, 813, 36, 8, 451, 3, 51, 3, 3… $ `1970` &lt;dbl&gt; 3, 3, 230, 160, 1, 28, 11, 6, 4, 826, 37, 9, 462, 3, 52, 3, 3… $ `1971` &lt;dbl&gt; 3, 3, 235, 169, 1, 29, 11, 6, 4, 840, 39, 9, 470, 3, 54, 3, 3… $ `1972` &lt;dbl&gt; 3, 3, 239, 179, 1, 30, 12, 6, 4, 852, 40, 9, 475, 3, 55, 3, 3… $ `1973` &lt;dbl&gt; 3, 3, 244, 191, 1, 31, 12, 7, 5, 865, 42, 9, 479, 4, 57, 3, 3… $ `1974` &lt;dbl&gt; 4, 3, 250, 205, 1, 31, 13, 7, 5, 878, 44, 9, 482, 4, 58, 3, 3… $ `1975` &lt;dbl&gt; 4, 3, 257, 224, 1, 32, 13, 7, 5, 892, 45, 10, 485, 4, 60, 3, … $ `1976` &lt;dbl&gt; 4, 3, 266, 249, 1, 33, 14, 7, 5, 907, 47, 10, 488, 4, 61, 4, … $ `1977` &lt;dbl&gt; 4, 3, 276, 277, 1, 33, 14, 7, 5, 922, 49, 10, 492, 4, 62, 4, … $ `1978` &lt;dbl&gt; 4, 3, 287, 308, 1, 33, 15, 8, 5, 938, 51, 11, 497, 4, 64, 5, … $ `1979` &lt;dbl&gt; 4, 4, 297, 336, 1, 34, 15, 8, 6, 953, 53, 11, 502, 4, 65, 5, … $ `1980` &lt;dbl&gt; 4, 4, 308, 359, 1, 35, 16, 8, 6, 966, 55, 11, 509, 5, 66, 6, … $ `1981` &lt;dbl&gt; 4, 4, 318, 375, 1, 35, 17, 8, 6, 978, 58, 11, 517, 5, 67, 6, … $ `1982` &lt;dbl&gt; 4, 4, 327, 385, 1, 36, 17, 9, 6, 989, 61, 12, 527, 5, 68, 6, … $ `1983` &lt;dbl&gt; 4, 4, 336, 394, 1, 38, 18, 9, 6, 999, 64, 12, 537, 5, 69, 6, … $ `1984` &lt;dbl&gt; 4, 4, 345, 406, 1, 39, 19, 9, 6, 1, 68, 12, 548, 5, 69, 6, 5,… $ `1985` &lt;dbl&gt; 4, 4, 355, 426, 2, 40, 19, 10, 7, 1, 72, 12, 559, 6, 70, 6, 5… $ `1986` &lt;dbl&gt; 4, 4, 366, 454, 2, 41, 20, 10, 7, 1, 76, 12, 569, 6, 70, 6, 5… $ `1987` &lt;dbl&gt; 5, 5, 377, 490, 2, 43, 21, 10, 8, 1, 80, 12, 579, 6, 70, 6, 5… $ `1988` &lt;dbl&gt; 5, 5, 388, 529, 2, 44, 22, 10, 8, 1, 85, 12, 589, 7, 70, 7, 5… $ `1989` &lt;dbl&gt; 5, 5, 400, 564, 2, 46, 22, 11, 9, 1, 90, 12, 599, 7, 70, 7, 5… $ `1990` &lt;dbl&gt; 5, 5, 412, 590, 2, 47, 23, 11, 9, 1, 95, 12, 611, 7, 71, 7, 5… $ `1991` &lt;dbl&gt; 5, 5, 424, 607, 2, 49, 24, 11, 9, 1, 100, 13, 622, 7, 71, 7, … $ `1992` &lt;dbl&gt; 5, 5, 436, 615, 2, 51, 25, 12, 9, 1, 106, 13, 635, 6, 73, 7, … $ `1993` &lt;dbl&gt; 5, 5, 449, 619, 2, 53, 26, 12, 9, 1, 112, 14, 648, 6, 74, 7, … $ `1994` &lt;dbl&gt; 5, 5, 462, 622, 2, 55, 26, 13, 9, 1, 117, 14, 661, 5, 75, 7, … $ `1995` &lt;dbl&gt; 5, 5, 475, 630, 2, 57, 27, 13, 9, 1, 123, 15, 674, 5, 77, 7, … $ `1996` &lt;dbl&gt; 5, 6, 489, 644, 2, 58, 28, 13, 10, 1, 129, 15, 686, 6, 78, 7,… $ `1997` &lt;dbl&gt; 5, 6, 502, 661, 2, 60, 29, 14, 10, 1, 134, 16, 699, 6, 78, 7,… $ `1998` &lt;dbl&gt; 5, 6, 515, 680, 2, 62, 30, 14, 10, 1, 140, 16, 712, 6, 79, 8,… $ `1999` &lt;dbl&gt; 6, 6, 529, 700, 2, 64, 31, 15, 10, 1, 145, 17, 724, 7, 80, 8,… $ `2000` &lt;dbl&gt; 6, 6, 542, 718, 2, 66, 31, 15, 11, 1, 150, 17, 737, 7, 81, 8,… $ `2001` &lt;dbl&gt; 6, 6, 556, 733, 2, 68, 32, 16, 11, 1, 156, 18, 749, 8, 82, 9,… $ `2002` &lt;dbl&gt; 6, 6, 569, 747, 2, 70, 33, 16, 11, 1, 161, 18, 760, 8, 84, 9,… $ `2003` &lt;dbl&gt; 6, 6, 583, 760, 2, 72, 34, 17, 12, 1, 167, 19, 771, 8, 86, 9,… $ `2004` &lt;dbl&gt; 6, 7, 597, 772, 2, 74, 35, 17, 12, 1, 172, 19, 782, 8, 87, 10… $ `2005` &lt;dbl&gt; 6, 7, 612, 783, 2, 76, 36, 18, 12, 1, 178, 20, 792, 8, 89, 10… $ `2006` &lt;dbl&gt; 6, 7, 626, 795, 2, 78, 37, 18, 12, 1, 184, 21, 801, 9, 90, 10… $ `2007` &lt;dbl&gt; 6, 7, 642, 805, 2, 80, 38, 19, 13, 1, 190, 21, 809, 9, 90, 11… $ `2008` &lt;dbl&gt; 6, 8, 657, 816, 3, 82, 39, 19, 13, 1, 196, 22, 816, 9, 90, 11… $ `2009` &lt;dbl&gt; 6, 8, 673, 828, 3, 85, 40, 20, 14, 1, 202, 22, 824, 9, 91, 11… $ `2010` &lt;dbl&gt; 6, 8, 690, 840, 3, 87, 42, 21, 14, 1, 209, 23, 831, 10, 91, 1… $ `2011` &lt;dbl&gt; 7, 8, 707, 854, 3, 90, 43, 21, 14, 1, 215, 24, 837, 10, 92, 1… $ `2012` &lt;dbl&gt; 7, 9, 724, 868, 3, 92, 44, 22, 15, 1, 221, 24, 844, 10, 93, 1… $ `2013` &lt;dbl&gt; 7, 9, 742, 883, 3, 95, 45, 22, 15, 1, 227, 25, 851, 10, 93, 1… $ `2014` &lt;dbl&gt; 7, 9, 759, 899, 3, 98, 46, 23, 16, 1, 234, 26, 857, 11, 94, 1… $ `2015` &lt;dbl&gt; 7, 10, 777, 914, 3, 100, 47, 24, 16, 1, 240, 27, 863, 11, 95,… $ `2016` &lt;dbl&gt; 7, 10, 796, 929, 3, 103, 49, 24, 17, 1, 246, 27, 870, 11, 96,… $ `2017` &lt;dbl&gt; 7, 10, 814, 944, 3, 106, 50, 25, 17, 1, 253, 28, 876, 11, 96,… $ `2018` &lt;dbl&gt; 7, 11, 832, 959, 3, 109, 51, 26, 18, 1, 260, 29, 883, 12, 97,… $ `2019` &lt;dbl&gt; 7, 11, 851, 974, 3, 112, 52, 26, 18, 1, 266, 30, 889, 12, 98,… $ `2020` &lt;dbl&gt; 7, 11, 870, 988, 3, 114, 53, 27, 19, 1, 273, 31, 895, 12, 98,… Does that look correct to you? No. Why is the world population only 2 for 1950? If you read the documentation for the data, you may notice that population is recorded in thousands, but I still think that there were more than \\(2,000\\) people in the world in 1950. Also, I’m pretty sure the single country of Comoros shouldn’t have more people than the entire world. Always look at the data when you read it in. The problem is debugged by printing the data out on the command line like we did above, and noticing that in the original file, numbers are stored with spaces in them. We have to remove these spaces for R to read in the data correctly. This kind of simple but annoying thing happens all the time when analyzing data “in the wild”. To remove the spaces (this works for any annoying character like a period, or a dollar sign, or whatever), read the data in with all columns as character, process the data in R, and then convert to numeric. Check it out: remove_space &lt;- function(x) stringr::str_remove_all(x,&quot; &quot;) worldpop &lt;- readr::read_csv( file = &quot;data/worldpop/worldpop-estimates.csv&quot;, col_names = TRUE, col_types = stringr::str_c(rep(&quot;c&quot;,72),collapse = &quot;&quot;) ) %&gt;% # Remove the space mutate_at(vars(`1950`:`2020`),remove_space) %&gt;% # Convert to numeric mutate_at(vars(`1950`:`2020`),as.numeric) glimpse(worldpop) Observations: 236 Variables: 72 $ country &lt;chr&gt; &quot;WORLD&quot;, &quot;Burundi&quot;, &quot;Comoros&quot;, &quot;Djibouti&quot;, &quot;Eritrea&quot;, &quot;Ethiop… $ `1950` &lt;dbl&gt; 2536431, 2309, 159, 62, 822, 18128, 6077, 4084, 2954, 493, 15… $ `1951` &lt;dbl&gt; 2584034, 2360, 163, 63, 835, 18467, 6242, 4168, 3012, 506, 16… $ `1952` &lt;dbl&gt; 2630862, 2406, 167, 65, 849, 18820, 6416, 4257, 3072, 521, 16… $ `1953` &lt;dbl&gt; 2677609, 2449, 170, 66, 865, 19184, 6598, 4349, 3136, 537, 17… $ `1954` &lt;dbl&gt; 2724847, 2492, 173, 68, 882, 19560, 6789, 4444, 3202, 554, 18… $ `1955` &lt;dbl&gt; 2773020, 2537, 176, 70, 900, 19947, 6988, 4544, 3271, 571, 19… $ `1956` &lt;dbl&gt; 2822443, 2585, 179, 71, 919, 20348, 7195, 4647, 3342, 588, 20… $ `1957` &lt;dbl&gt; 2873306, 2636, 182, 74, 939, 20764, 7412, 4754, 3417, 605, 21… $ `1958` &lt;dbl&gt; 2925687, 2689, 185, 76, 961, 21201, 7638, 4865, 3495, 623, 22… $ `1959` &lt;dbl&gt; 2979576, 2743, 188, 80, 983, 21662, 7874, 4980, 3576, 641, 23… $ `1960` &lt;dbl&gt; 3034950, 2798, 191, 84, 1008, 22151, 8120, 5099, 3660, 660, 2… $ `1961` &lt;dbl&gt; 3091844, 2852, 194, 89, 1033, 22671, 8378, 5224, 3748, 679, 2… $ `1962` &lt;dbl&gt; 3150421, 2907, 197, 94, 1060, 23221, 8647, 5352, 3839, 698, 2… $ `1963` &lt;dbl&gt; 3211001, 2964, 200, 101, 1089, 23798, 8929, 5486, 3934, 717, … $ `1964` &lt;dbl&gt; 3273978, 3026, 204, 108, 1118, 24397, 9223, 5625, 4032, 736, … $ `1965` &lt;dbl&gt; 3339584, 3094, 207, 115, 1148, 25014, 9530, 5769, 4134, 753, … $ `1966` &lt;dbl&gt; 3407923, 3170, 211, 123, 1179, 25641, 9851, 5919, 4240, 770, … $ `1967` &lt;dbl&gt; 3478770, 3253, 216, 131, 1210, 26280, 10187, 6074, 4350, 785,… $ `1968` &lt;dbl&gt; 3551599, 3337, 221, 140, 1243, 26944, 10540, 6234, 4464, 799,… $ `1969` &lt;dbl&gt; 3625681, 3414, 225, 150, 1276, 27653, 10911, 6402, 4582, 813,… $ `1970` &lt;dbl&gt; 3700437, 3479, 230, 160, 1311, 28415, 11301, 6576, 4704, 826,… $ `1971` &lt;dbl&gt; 3775760, 3530, 235, 169, 1347, 29249, 11713, 6758, 4829, 840,… $ `1972` &lt;dbl&gt; 3851651, 3570, 239, 179, 1385, 30141, 12146, 6947, 4959, 852,… $ `1973` &lt;dbl&gt; 3927781, 3605, 244, 191, 1424, 31037, 12601, 7143, 5093, 865,… $ `1974` &lt;dbl&gt; 4003794, 3646, 250, 205, 1464, 31861, 13077, 7346, 5235, 878,… $ `1975` &lt;dbl&gt; 4079480, 3701, 257, 224, 1505, 32567, 13576, 7556, 5385, 892,… $ `1976` &lt;dbl&gt; 4154667, 3771, 266, 249, 1548, 33128, 14096, 7773, 5546, 907,… $ `1977` &lt;dbl&gt; 4229506, 3854, 276, 277, 1592, 33577, 14639, 7998, 5718, 922,… $ `1978` &lt;dbl&gt; 4304534, 3949, 287, 308, 1637, 33993, 15205, 8230, 5897, 938,… $ `1979` &lt;dbl&gt; 4380506, 4051, 297, 336, 1684, 34488, 15798, 8470, 6075, 953,… $ `1980` &lt;dbl&gt; 4458003, 4157, 308, 359, 1733, 35142, 16417, 8717, 6250, 966,… $ `1981` &lt;dbl&gt; 4536997, 4267, 318, 375, 1785, 35985, 17064, 8971, 6412, 978,… $ `1982` &lt;dbl&gt; 4617387, 4380, 327, 385, 1837, 36995, 17736, 9234, 6566, 989,… $ `1983` &lt;dbl&gt; 4699569, 4498, 336, 394, 1891, 38143, 18432, 9504, 6738, 999,… $ `1984` &lt;dbl&gt; 4784012, 4621, 345, 406, 1946, 39374, 19146, 9781, 6965, 1008… $ `1985` &lt;dbl&gt; 4870922, 4751, 355, 426, 2004, 40652, 19877, 10063, 7268, 101… $ `1986` &lt;dbl&gt; 4960568, 4887, 366, 454, 2065, 41966, 20623, 10352, 7666, 102… $ `1987` &lt;dbl&gt; 5052522, 5027, 377, 490, 2127, 43329, 21382, 10648, 8141, 103… $ `1988` &lt;dbl&gt; 5145426, 5169, 388, 529, 2186, 44757, 22154, 10952, 8637, 103… $ `1989` &lt;dbl&gt; 5237441, 5307, 400, 564, 2231, 46272, 22935, 11269, 9076, 104… $ `1990` &lt;dbl&gt; 5327231, 5439, 412, 590, 2259, 47888, 23725, 11599, 9404, 105… $ `1991` &lt;dbl&gt; 5414289, 5565, 424, 607, 2266, 49610, 24522, 11943, 9600, 106… $ `1992` &lt;dbl&gt; 5498920, 5686, 436, 615, 2258, 51424, 25326, 12301, 9686, 108… $ `1993` &lt;dbl&gt; 5581598, 5798, 449, 619, 2239, 53296, 26136, 12675, 9710, 109… $ `1994` &lt;dbl&gt; 5663150, 5899, 462, 622, 2218, 55181, 26951, 13067, 9746, 111… $ `1995` &lt;dbl&gt; 5744213, 5987, 475, 630, 2204, 57048, 27768, 13475, 9844, 112… $ `1996` &lt;dbl&gt; 5824892, 6060, 489, 644, 2196, 58884, 28589, 13903, 10023, 11… $ `1997` &lt;dbl&gt; 5905046, 6122, 502, 661, 2195, 60697, 29416, 14348, 10265, 11… $ `1998` &lt;dbl&gt; 5984794, 6186, 515, 680, 2206, 62508, 30250, 14809, 10552, 11… $ `1999` &lt;dbl&gt; 6064239, 6267, 529, 700, 2237, 64343, 31099, 15283, 10854, 11… $ `2000` &lt;dbl&gt; 6143494, 6379, 542, 718, 2292, 66225, 31965, 15767, 11149, 11… $ `2001` &lt;dbl&gt; 6222627, 6526, 556, 733, 2375, 68159, 32849, 16261, 11432, 11… $ `2002` &lt;dbl&gt; 6301773, 6704, 569, 747, 2481, 70142, 33752, 16765, 11714, 12… $ `2003` &lt;dbl&gt; 6381185, 6909, 583, 760, 2601, 72171, 34679, 17279, 12000, 12… $ `2004` &lt;dbl&gt; 6461159, 7132, 597, 772, 2720, 74240, 35635, 17803, 12302, 12… $ `2005` &lt;dbl&gt; 6541907, 7365, 612, 783, 2827, 76346, 36625, 18337, 12626, 12… $ `2006` &lt;dbl&gt; 6623518, 7608, 626, 795, 2918, 78489, 37649, 18880, 12974, 12… $ `2007` &lt;dbl&gt; 6705947, 7862, 642, 805, 2997, 80674, 38706, 19434, 13342, 12… $ `2008` &lt;dbl&gt; 6789089, 8126, 657, 816, 3063, 82916, 39792, 19996, 13728, 12… $ `2009` &lt;dbl&gt; 6872767, 8398, 673, 828, 3120, 85234, 40902, 20569, 14128, 12… $ `2010` &lt;dbl&gt; 6956824, 8676, 690, 840, 3170, 87640, 42031, 21152, 14540, 12… $ `2011` &lt;dbl&gt; 7041194, 8958, 707, 854, 3214, 90140, 43178, 21744, 14962, 12… $ `2012` &lt;dbl&gt; 7125828, 9246, 724, 868, 3250, 92727, 44343, 22347, 15396, 12… $ `2013` &lt;dbl&gt; 7210582, 9540, 742, 883, 3281, 95386, 45520, 22961, 15839, 12… $ `2014` &lt;dbl&gt; 7295291, 9844, 759, 899, 3311, 98094, 46700, 23590, 16290, 12… $ `2015` &lt;dbl&gt; 7379797, 10160, 777, 914, 3343, 100835, 47878, 24234, 16745, … $ `2016` &lt;dbl&gt; 7464022, 10488, 796, 929, 3377, 103603, 49052, 24894, 17205, … $ `2017` &lt;dbl&gt; 7547859, 10827, 814, 944, 3413, 106400, 50221, 25571, 17670, … $ `2018` &lt;dbl&gt; 7631091, 11175, 832, 959, 3453, 109224, 51393, 26262, 18143, … $ `2019` &lt;dbl&gt; 7713468, 11531, 851, 974, 3497, 112079, 52574, 26969, 18629, … $ `2020` &lt;dbl&gt; 7794799, 11891, 870, 988, 3546, 114964, 53771, 27691, 19130, … Good. Verify that a few values of your choosing match their entries in the original text data. The data are in wide format, with the “year” variable contained in the columns. We want the data in long format for analysis, with two variables, country and year, and a variable containing the population count. We can do that: worldpop &lt;- worldpop %&gt;% pivot_longer( `1950`:`2020`, names_to = &quot;year&quot;, values_to = &quot;population&quot; ) %&gt;% mutate(year = as.numeric(year)) glimpse(worldpop) Observations: 16,756 Variables: 3 $ country &lt;chr&gt; &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WOR… $ year &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959… $ population &lt;dbl&gt; 2536431, 2584034, 2630862, 2677609, 2724847, 2773020, 2822… That looks better! Do the following exercises: Exercises: What is the estimated world population in 2020 (remember, the recorded values are in thousands of people. Answer this question in terms of number of people)? What is the estimated world population in Canada in 1975? Use the filter function. The total world population should equal the sum of the population in each country. Check this. Do the following: Compute the world population by summing the population of each country. Use the filter function to remove the WORLD row from each year. Then use group_by and summarize to sum population over country. Save the result in a dataframe called worldpop_summed. Pull the UN’s estimated world population by using the filter function to keep only the WORLD row from each year. Save this in a dataframe called worldpop_un. Join them, and make a plot of the difference between the sum of the countries’ populations and the UN’s estimate, for each year. Here’s what I got for both the data and the plot: Observations: 71 Variables: 4 $ year &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959… $ worldpop &lt;dbl&gt; 2536428, 2584029, 2630861, 2677604, 2724850, 2773023, 2822… $ country &lt;chr&gt; &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WOR… $ population &lt;dbl&gt; 2536431, 2584034, 2630862, 2677609, 2724847, 2773020, 2822… Hints: use scale_x_discrete(breaks = as.character(seq(1950,2020,by=5))) to get the five-year axis, and use theme(axis.text.x = element_text(angle = 90)) to make the axis text sideways. Use geom_bar(stat = \"identity\") to get the bar plot. Or, make another type of plot of your choosing! Don’t be afraid to have some fun. Now, for later in this chapter, we’re going to need data on the world population projections from 2020–2100. The median and “95% intervals” (you’ll learn what this means later) are all stored in seperate files. I’ll read in the “median” one and then you’ll read in the “interval” ones and then join them all together. The prediction data is mostly in the same format as the population estimates. The numbers are now stored with quotes and with thousands separated by commas, however. Like when it was stored with thousands separated by spaces, we have to read it in as a character, and then process the data in R and convert to numeric. head data/worldpop/worldpop-pred-median.csv country,2020,2025,2030,2035,2040,2045,2050,2055,2060,2065,2070,2075,2080,2085,2090,2095,2100 WORLD,&quot;7,794,799&quot;,&quot;8,184,437&quot;,&quot;8,548,487&quot;,&quot;8,887,524&quot;,&quot;9,198,847&quot;,&quot;9,481,803&quot;,&quot;9,735,034&quot;,&quot;9,958,099&quot;,&quot;10,151,470&quot;,&quot;10,317,879&quot;,&quot;10,459,240&quot;,&quot;10,577,288&quot;,&quot;10,673,904&quot;,&quot;10,750,662&quot;,&quot;10,809,892&quot;,&quot;10,851,860&quot;,&quot;10,875,394&quot; Eastern Africa,&quot;445,406&quot;,&quot;505,292&quot;,&quot;569,705&quot;,&quot;637,437&quot;,&quot;707,393&quot;,&quot;778,916&quot;,&quot;851,218&quot;,&quot;923,483&quot;,&quot;994,888&quot;,&quot;1,064,642&quot;,&quot;1,131,895&quot;,&quot;1,195,953&quot;,&quot;1,256,219&quot;,&quot;1,312,230&quot;,&quot;1,363,577&quot;,&quot;1,410,118&quot;,&quot;1,451,842&quot; Burundi,&quot;11,891&quot;,&quot;13,764&quot;,&quot;15,773&quot;,&quot;17,932&quot;,&quot;20,253&quot;,&quot;22,728&quot;,&quot;25,325&quot;,&quot;27,995&quot;,&quot;30,701&quot;,&quot;33,408&quot;,&quot;36,107&quot;,&quot;38,790&quot;,&quot;41,427&quot;,&quot;43,993&quot;,&quot;46,451&quot;,&quot;48,761&quot;,&quot;50,904&quot; Comoros,870,965,&quot;1,063&quot;,&quot;1,164&quot;,&quot;1,266&quot;,&quot;1,370&quot;,&quot;1,472&quot;,&quot;1,571&quot;,&quot;1,666&quot;,&quot;1,756&quot;,&quot;1,841&quot;,&quot;1,919&quot;,&quot;1,990&quot;,&quot;2,052&quot;,&quot;2,106&quot;,&quot;2,151&quot;,&quot;2,187&quot; Djibouti,988,&quot;1,056&quot;,&quot;1,117&quot;,&quot;1,170&quot;,&quot;1,217&quot;,&quot;1,259&quot;,&quot;1,295&quot;,&quot;1,325&quot;,&quot;1,346&quot;,&quot;1,358&quot;,&quot;1,364&quot;,&quot;1,365&quot;,&quot;1,363&quot;,&quot;1,359&quot;,&quot;1,353&quot;,&quot;1,343&quot;,&quot;1,332&quot; Eritrea,&quot;3,546&quot;,&quot;3,866&quot;,&quot;4,240&quot;,&quot;4,664&quot;,&quot;5,114&quot;,&quot;5,567&quot;,&quot;6,005&quot;,&quot;6,427&quot;,&quot;6,836&quot;,&quot;7,231&quot;,&quot;7,605&quot;,&quot;7,946&quot;,&quot;8,248&quot;,&quot;8,510&quot;,&quot;8,731&quot;,&quot;8,915&quot;,&quot;9,062&quot; Ethiopia,&quot;114,964&quot;,&quot;129,749&quot;,&quot;144,944&quot;,&quot;160,231&quot;,&quot;175,466&quot;,&quot;190,611&quot;,&quot;205,411&quot;,&quot;219,639&quot;,&quot;232,994&quot;,&quot;245,316&quot;,&quot;256,441&quot;,&quot;266,190&quot;,&quot;274,558&quot;,&quot;281,512&quot;,&quot;287,056&quot;,&quot;291,317&quot;,&quot;294,393&quot; Kenya,&quot;53,771&quot;,&quot;59,981&quot;,&quot;66,450&quot;,&quot;73,026&quot;,&quot;79,470&quot;,&quot;85,669&quot;,&quot;91,575&quot;,&quot;97,175&quot;,&quot;102,398&quot;,&quot;107,170&quot;,&quot;111,411&quot;,&quot;115,093&quot;,&quot;118,214&quot;,&quot;120,777&quot;,&quot;122,807&quot;,&quot;124,341&quot;,&quot;125,424&quot; Madagascar,&quot;27,691&quot;,&quot;31,510&quot;,&quot;35,622&quot;,&quot;39,949&quot;,&quot;44,471&quot;,&quot;49,175&quot;,&quot;54,048&quot;,&quot;59,033&quot;,&quot;64,059&quot;,&quot;69,074&quot;,&quot;74,035&quot;,&quot;78,897&quot;,&quot;83,598&quot;,&quot;88,090&quot;,&quot;92,343&quot;,&quot;96,310&quot;,&quot;99,957&quot; remove_comma &lt;- function(x) stringr::str_remove_all(x,&quot;,&quot;) worldpop_pred_median &lt;- readr::read_csv( file = &quot;data/worldpop/worldpop-pred-median.csv&quot;, col_names = TRUE, col_types = stringr::str_c(rep(&quot;c&quot;,18),collapse = &quot;&quot;) ) %&gt;% # Remove the commas. R already removed the quotes. mutate_at(vars(`2020`:`2100`),remove_comma) %&gt;% # Convert to numeric mutate_at(vars(`2020`:`2100`),as.numeric) %&gt;% # Pivot to long format pivot_longer( `2020`:`2100`, names_to = &quot;year&quot;, values_to = &quot;population&quot; ) %&gt;% mutate(year = as.numeric(year)) glimpse(worldpop_pred_median) Observations: 4,029 Variables: 3 $ country &lt;chr&gt; &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WOR… $ year &lt;dbl&gt; 2020, 2025, 2030, 2035, 2040, 2045, 2050, 2055, 2060, 2065… $ population &lt;dbl&gt; 7794799, 8184437, 8548487, 8887524, 9198847, 9481803, 9735… Exercise: read in the worldpop-pred-lower95.csv and worldpop-pred-upper95.csv datasets into dataframes called worldpop_pred_lower95 and worldpop_pred_upper95, with the population variable named population_lower95 and population_upper95. Join the three dataframes into a dataframe worldpop_pred which looks like this: Observations: 4,012 Variables: 5 $ country &lt;chr&gt; &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORL… $ year &lt;dbl&gt; 2020, 2025, 2030, 2035, 2040, 2045, 2050, 2055, 20… $ population &lt;dbl&gt; 7794799, 8184437, 8548487, 8887524, 9198847, 94818… $ population_lower95 &lt;dbl&gt; 7794799, 8144343, 8460182, 8746077, 8996324, 92131… $ population_upper95 &lt;dbl&gt; 7794799, 8223574, 8636041, 9029502, 9396987, 97448… We’ll use these data later in this chapter. 15.2 Model world population over time Look at the world population across years: worldpopplot &lt;- worldpop %&gt;% filter(country==&#39;WORLD&#39;) %&gt;% ggplot(aes(x = year,y = population)) + theme_classic() + geom_point() + theme(axis.text.x = element_text(angle = 90)) + labs(title = &quot;World population over time&quot;, x = &quot;Year&quot;, y = &quot;World Population (Billions)&quot;) + scale_x_continuous(breaks = seq(1950,2020,by=5)) + scale_y_continuous(labels = function(x) x*1e-06) worldpopplot It looks like population may be growing at a pretty constant rate, or equivalently (sort of- bear with me), may be increasing by a constant amount each year. What is this rate/increase? This is an inference problem. We have data (population counts for each year) and a model (population grows at a constant rate from year to year) and we need to infer the value of an unknown parameter (the rate at which population grows). In order to do this, we need to write down our model more formally. Let \\(Y_{i}\\) be the random variable representing the world population in year \\(i\\) with \\(i = 1950,\\ldots,2020\\). We think the population increases by the same amount per year on average, but want to allow for a bit of variability. We can model: \\[ Y_{i+1} - Y_{i} \\overset{iid}{\\sim}\\text{Normal}\\left(\\Delta,\\sigma^{2}\\right) \\] and then infer \\(\\Delta\\), the average increase in population. Exercise: estimate \\(\\Delta\\) using the sample mean of the differences in population. You can use the diff function, or the lag function to compute the differences– look up their documentation for help. I got the following: Mean: 75120 So it looks like the population increases by about 7.5 million on average (remember, population here is in thousands). It turns out that what we just did is similar to the following linear regression model: \\[ Y_{i} = \\beta_{0} + \\Delta i + \\epsilon_{i}, \\ \\epsilon_{i} \\overset{iid}{\\sim}\\text{Normal}\\left(0,\\sigma^{2}/2\\right) \\] This is because: \\[\\begin{equation}\\begin{aligned} Y_{i} &amp;= \\beta_{0} + \\Delta i + \\epsilon_{i} \\\\ Y_{i+1} &amp;= \\beta_{0} + \\Delta (i+1) + \\epsilon_{i+1} \\\\ \\implies Y_{i+1} - Y_{i} &amp;= \\Delta + \\left(\\epsilon_{i+1} - \\epsilon_{i}\\right) \\end{aligned}\\end{equation}\\] and \\(\\left(\\epsilon_{i+1} - \\epsilon_{i}\\right)\\overset{iid}{\\sim}\\text{Normal}\\left(0,\\sigma^{2}\\right)\\). diffmod &lt;- lm(population ~ as.numeric(year),data = filter(worldpop,country == &#39;WORLD&#39;)) summary(diffmod) Call: lm(formula = population ~ as.numeric(year), data = filter(worldpop, country == &quot;WORLD&quot;)) Residuals: Min 1Q Median 3Q Max -119897 -89256 964 52493 294834 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.50e+08 1.12e+06 -134 &lt;2e-16 *** as.numeric(year) 7.79e+04 5.63e+02 138 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 97200 on 69 degrees of freedom Multiple R-squared: 0.996, Adjusted R-squared: 0.996 F-statistic: 1.91e+04 on 1 and 69 DF, p-value: &lt;2e-16 unname(coef(diffmod)[2]) # Should be close to the mean difference [1] 77877 The estimated slope of the regression line is \\(\\hat{\\Delta} = 77877\\), close to the mean difference (estimating the standard deviation is a bit trickier). Plot the model: worldpopplot + geom_abline(slope = coef(diffmod)[2],intercept = coef(diffmod)[1]) It’s ok. It looks like maybe the growth isn’t by some constant value each year, but rather maybe the rate of growth is constant. We can build a linear regression model for that, too. Consider the following growth model: \\[ Y_{i+1} = Y_{i}(1+\\Delta) \\] where the parameter \\(\\Delta\\) is now the rate of population growth. We can model this approximately as a linear regression model: \\[ Y_{i} = \\exp\\left( \\beta_{0} + \\Delta i + \\epsilon_{i}\\right) \\] This gives \\[ Y_{i+1}/Y_{i} = \\exp\\left(\\Delta + \\epsilon_{i+1} - \\epsilon_{i}\\right) \\] which, ignoring the errors, gives \\(\\exp(\\Delta) \\approx 1 + \\Delta\\). You may or may not recall that the approximation \\(e^{x} \\approx 1 + x\\) is a first-order Taylor expansion of \\(e^{x}\\). Wait, how is this even a linear regression model? That’s obtained by taking logs: \\[ \\log Y_{i} = \\beta_{0} + \\Delta i + \\epsilon_{i} \\] so we fit this model by computing a new variable \\(\\log Y_{i}\\) in the data, and then doing a linear regression model for that. Exercise: fit this model: Create a new variable logpopulation using mutate(), Do a linear regression as above, like lm(logpopulation ~ ...). Here’s what I got, calling my model object logmodel: summary(logmodel) Call: lm(formula = logpopulation ~ year, data = logpopdat) Residuals: Min 1Q Median 3Q Max -0.07032 -0.02804 0.00389 0.02891 0.04295 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.73e+01 3.66e-01 -47.3 &lt;2e-16 *** year 1.65e-02 1.85e-04 89.2 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.0319 on 69 degrees of freedom Multiple R-squared: 0.991, Adjusted R-squared: 0.991 F-statistic: 7.96e+03 on 1 and 69 DF, p-value: &lt;2e-16 exp(unname(coef(logmodel)[2])) - 1 # Delta [1] 0.016599 So it looks like population increases by about \\(1.66\\%\\) each year on average. Check out how it looks (I called my new data logpopdat): logpopdat %&gt;% ggplot(aes(x = year,y = logpopulation)) + theme_classic() + geom_point() + geom_abline(slope = coef(logmodel)[2],intercept = coef(logmodel)[1]) + coord_trans(y = &quot;exp&quot;) + theme(axis.text.x = element_text(angle = 90)) + labs(title = &quot;World population over time&quot;, x = &quot;Year&quot;, y = &quot;World Population (Billions)&quot;) + scale_x_continuous(breaks = seq(1950,2020,by=5)) + scale_y_continuous(breaks = log(3:8 * 1e06),labels = function(x) exp(x)*1e-06) We don’t yet have the tools to tell if one model is “better” than the other! 15.3 Bayesian model for world population The reported world populations are just estimates. We saw that when you sum up the populations for each country, the answer does not exactly equal the reported world population. Here we will describe a Bayesian method for estimating the total world population in a given year, based on the reported value for that year, attempting to account for error. For any chosen year, let \\(Y\\) represent the reported world population. Let \\(\\lambda\\) be the true world population. We might have \\(Y &gt; \\lambda\\) or \\(Y &lt; \\lambda\\) due to reporting error– \\(Y\\) is random, \\(\\lambda\\) is fixed and unknown. We want to write down a statistical model for \\(Y\\) which depends on \\(\\lambda\\) and then use \\(Y\\) to infer \\(\\lambda\\). One such model is \\[ Y \\sim \\text{Poisson}(\\lambda) \\] To do Bayesian inference, we require a prior distribution on \\(\\lambda\\). How do we choose this? This is completely subjective, and it might seem like we have absolutely no information (without looking at the data, of course). But this isn’t true: we know \\(\\lambda &gt; 0\\) (there can’t, on average, be less than zero people on earth) and we know that \\(\\lambda\\) can’t be something absurd like \\(10^{100}\\). We want to choose what is called a weakly informative prior: one that restricts \\(\\lambda\\) to be in a not-absurd range. Let’s try a Gamma distribution with \\(95^{th}\\) percentile equal to \\(10\\)billion and \\(5^{th}\\) percentile equal to \\(1\\) billion. Measuring \\(\\lambda\\) in billions, this corresponds to parameters of about \\(\\alpha = 3.358\\) and \\(\\beta = 0.778\\): \\[ \\lambda \\sim\\text{Gamma}(3.358,0.778) \\] Let’s plot this prior to see what it looks like: alpha &lt;- 3.358 beta &lt;- 0.778 priorplot &lt;- tibble(xx = seq(0,15,length.out = 10000)) %&gt;% ggplot(aes(x = xx)) + theme_classic() + stat_function(fun = dgamma,args = list(shape = alpha,rate = beta)) + labs(title = &quot;Prior on total world population&quot;, x = &quot;Total world population (billions)&quot;, y = &quot;Prior density&quot;) + scale_x_continuous(breaks = seq(0,15,by = 1),labels = function(x) scales::comma(x)) priorplot Now, you have to get the posterior for \\(\\lambda|Y\\). The likelihood is \\[ P(Y = y|\\lambda) = \\frac{\\lambda^{y}e^{-\\lambda}}{y!} \\] The prior is \\[ \\pi(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta\\lambda} \\] Exercise: show that the posterior is \\[ \\lambda|Y \\sim\\Gamma\\left( \\alpha + y,\\beta + 1\\right) \\] Now, with the posterior determined, we can infer the world population for any given year. For \\(2010\\), say, we can plot the posterior (note that \\(Y\\) is now converted to billions, from thousands): observedpop &lt;- worldpop %&gt;% filter(country == &#39;WORLD&#39;,year == 2010) %&gt;% pull(population) observedpop &lt;- observedpop * 1e-06 # Convert to billions, from thousands priorplot + stat_function(fun = dgamma,args = list(shape = alpha + observedpop,rate = beta+1),colour = &quot;orange&quot;) + geom_vline(xintercept = observedpop,linetype = &#39;dashed&#39;,colour=&#39;red&#39;) + labs(title = &quot;Prior (black) and posterior (orange) for total world population&quot;, subtitle = &quot;Red line: observed world population&quot;, y = &quot;Density&quot;) We will see later how to use this to estimate the world population in a given year. Exercise: the prior pulls the posterior to the left of the observed values, favouring the notion that the reported world population is an overestimate of the true value. Is this reasonable? Try it out with some different priors. I wrote you a helper function which lets you put in the \\(2.5\\%\\) and \\(97.5\\%\\) quantiles you want, and gives you the \\(\\alpha,\\beta\\) that give you these quantiles: getgammaparams(1,10) alpha beta 3.3582 0.7787 For example, if you wanted to use a Gamma distribution with lower quantile \\(0.1\\) billion and upper quantile \\(15\\) billion, you would use getgammaparams(.1,15) alpha beta 0.99196 0.24480 and so on. Try it with any/all unique combinations of lower quantile \\(.01,.1,1,2,5\\) billion and upper quantile \\(6,10,15,20,50\\) billion. Is the posterior very sensitive to the choice of prior? 15.4 Quantifying uncertainty in estimates of world population: regression model We have done the following linear regression for world population: worldpopplot + geom_abline(slope = coef(diffmod)[2],intercept = coef(diffmod)[1]) The value of the regression line at any year gives an estimate of the population in that year. However, statistics is the study of uncertainty. We don’t just want to give a single estimate of population, we want to quantify uncertainty in this estimate. We do so using the probability distribution of the predicted value, and constructing an interval in which the predicted value has a high probability of falling, under the model. The observed world population in year \\(i\\) is: \\[ Y_{i} = \\beta_{0} + \\Delta i + \\epsilon_{i} \\] This can be written as \\[ Y_{i} = \\mu_{i} + \\epsilon_{i} \\] where \\(\\mu_{i} = \\beta_{0} + \\Delta_{i} i\\) is the actual world population, and \\(\\epsilon_{i}\\) is some random error. In the linear regression output, we obtain estimates \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\Delta}\\). The regression line that is actually plotted is \\[ \\hat{\\mu}_{i} = \\hat{\\beta}_{0} + \\hat{\\Delta}i \\] The predicted value of the world population in year \\(i\\) is simply \\(\\hat{\\mu}_{i}\\). Its value along with its standard deviation, \\(\\text{SD}(\\hat{\\mu}_{i})\\), can be obtained from the regression output using the predict function. To get the estimated world population and \\(95\\%\\) confidence interval for 2010, for example, you could do predict(diffmod,newdata = data.frame(year = 2010),se.fit = TRUE,interval = &quot;confidence&quot;)$fit fit lwr upr 1 6914203 6877901 6950504 There is a problem, though. A subtle problem. A confidence interval is interpreted as “if we repeated the experiment over and over again and calculated this interval, \\(95\\%\\) of the intervals we calculated would contain the true value”. This means that for the interval we just calculated, if we were to go into some parallel universe over and over again and measure the population of the world from 1950 – 2020 and build this regression model and calculate this interval, the intervals would contain the true population of the world \\(95\\%\\) of the time. That’s really confusing. What may instead calculate an interval which has a \\(95\\%\\) chance of containing the measured world population value for 2010 (say), accounting for both uncertainty in the estimated world population and the variability in the measurement of world population on any given year. If we went back and measured the population for 2010 over and over again, we think that \\(95\\%\\) of such measurements would fall within this interval. We call this a prediction interval instead of a confidence interval. It is based off both the variability in \\(\\hat{\\mu}_{i}\\) and the variability in \\(\\epsilon_{i}\\). It is obtained as predict(diffmod,newdata = data.frame(year = 2010),se.fit = TRUE,interval = &quot;prediction&quot;)$fit fit lwr upr 1 6914203 6716913 7111492 Notice how it is wider than the confidence interval, because it accounts for more types of uncertainty. Let’s plot the confidence intervals on our plot: # Note: removing the &quot;newdata&quot; argument gives # predictions on the original values worldpoponlyworld &lt;- filter(worldpop,country==&#39;WORLD&#39;) preddat &lt;- predict(diffmod,se.fit = TRUE,interval = &quot;confidence&quot;)$fit %&gt;% as_tibble() preddat$year &lt;- worldpoponlyworld$year worldpoppred &lt;- inner_join(worldpoponlyworld,preddat,by = &quot;year&quot;) glimpse(worldpoppred) Observations: 71 Variables: 6 $ country &lt;chr&gt; &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WOR… $ year &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959… $ population &lt;dbl&gt; 2536431, 2584034, 2630862, 2677609, 2724847, 2773020, 2822… $ fit &lt;dbl&gt; 2241597, 2319473, 2397350, 2475227, 2553104, 2630980, 2708… $ lwr &lt;dbl&gt; 2196050, 2274892, 2353727, 2432554, 2511372, 2590181, 2668… $ upr &lt;dbl&gt; 2287143, 2364054, 2440973, 2517900, 2594836, 2671780, 2748… predplotconfint &lt;- worldpoppred %&gt;% ggplot(aes(x = year,y = fit)) + theme_classic() + # Shaded region for the interval geom_ribbon(aes(ymin = lwr,ymax = upr),fill = &quot;darkgrey&quot;) + geom_line() + geom_point(aes(y = population)) + theme(axis.text.x = element_text(angle = 90)) + labs(title = &quot;World population with estimate and 95% CI&quot;, x = &quot;Year&quot;, y = &quot;World Population (Billions)&quot;) + scale_x_continuous(breaks = seq(1950,2020,by=5)) + scale_y_continuous(labels = function(x) x*1e-06) predplotconfint Notice how the interval is really narrow, and doesn’t contain most of the actual measured values. This is because it is an interval for the true, unknown value of the world population, not the actual measured values themselves. Exercise: create the same plot but with a prediction interval. I got the following: # Note: removing the &quot;newdata&quot; argument gives # predictions on the original values worldpoponlyworld &lt;- filter(worldpop,country==&#39;WORLD&#39;) preddat &lt;- predict(diffmod,se.fit = TRUE,interval = &quot;prediction&quot;)$fit %&gt;% as_tibble() Warning in predict.lm(diffmod, se.fit = TRUE, interval = &quot;prediction&quot;): predictions on current data refer to _future_ responses preddat$year &lt;- worldpoponlyworld$year worldpoppred &lt;- inner_join(worldpoponlyworld,preddat,by = &quot;year&quot;) glimpse(worldpoppred) Observations: 71 Variables: 6 $ country &lt;chr&gt; &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WOR… $ year &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959… $ population &lt;dbl&gt; 2536431, 2584034, 2630862, 2677609, 2724847, 2773020, 2822… $ fit &lt;dbl&gt; 2241597, 2319473, 2397350, 2475227, 2553104, 2630980, 2708… $ lwr &lt;dbl&gt; 2042399, 2120494, 2198583, 2276666, 2354743, 2432814, 2510… $ upr &lt;dbl&gt; 2440795, 2518453, 2596117, 2673788, 2751464, 2829147, 2906… predplotpredint &lt;- worldpoppred %&gt;% ggplot(aes(x = year,y = fit)) + theme_classic() + # Shaded region for the interval geom_ribbon(aes(ymin = lwr,ymax = upr),fill = &quot;darkgrey&quot;) + geom_line() + geom_point(aes(y = population)) + theme(axis.text.x = element_text(angle = 90)) + labs(title = &quot;World population with estimate and 95% PI&quot;, x = &quot;Year&quot;, y = &quot;World Population (Billions)&quot;) + scale_x_continuous(breaks = seq(1950,2020,by=5)) + scale_y_continuous(labels = function(x) x*1e-06) predplotpredint Note how the interval is much wider, and contains about 19/20 \\(95\\%\\) of the measured values. Exercise: create the same two plots but for the rate model (the other regression model we did). I got: Warning in predict.lm(logmodel, se.fit = TRUE, interval = &quot;prediction&quot;): predictions on current data refer to _future_ responses Comment on the difference in width of the two intervals. 15.5 Bayesian estimate of world population We previously looked at the model \\[\\begin{equation}\\begin{aligned} Y &amp;\\sim \\text{Poisson}(\\lambda) \\lambda &amp;\\sim\\text{Gamma}(3.358,0.778) \\implies \\lambda | Y &amp;\\sim \\Gamma(3.358 + Y,0.788 + 1) \\end{aligned}\\end{equation}\\] where \\(Y\\) is the measured value of world population in Billions, \\(\\lambda\\) is the real value of world population in Billions, and the prior on \\(\\lambda\\) was chosen so that \\(P(1 \\leq \\lambda \\leq 10) = 95\\%\\). Previously we derived the posterior distribution, but we didn’t actually use it to estimate or quantify uncertainty in \\(\\lambda\\). We’ll do that now. 15.5.1 Estimation and uncertainty quantification The posterior distribution gives us both estimation and uncertainty quantification, but it’s up to us to decide how to use it. There are three common ways to use a posterior for point estimation: the posterior mode, \\[ \\hat{\\lambda}_{\\texttt{mode}} = \\text{argmax}_{\\lambda}\\pi(\\lambda | Y), \\] the posterior mean, \\[ \\hat{\\lambda}_{\\texttt{mean}} = E(\\lambda|Y), \\] and the posterior median, which is the value \\(\\hat{\\lambda}_{\\texttt{med}}\\) which satisfies \\[ P(\\lambda &lt; \\hat{\\lambda}_{\\texttt{med}}|Y) = 0.5 \\] You can determine an expression for each of these posterior summaries using your usual probability techniques. Exercise: show that \\[\\hat{\\lambda}_{\\texttt{mode}} = \\frac{\\alpha + Y - 1}{\\beta + 1}\\] where \\((\\alpha,\\beta)\\) are the prior parameters (i.e. \\(\\lambda\\) has a \\(\\text{Gamma}(\\alpha,\\beta)\\) prior). Exercise: show that \\[\\hat{\\lambda}_{\\texttt{mean}} = \\frac{\\alpha + Y}{\\beta + 1}.\\] The posterior median does not have a closed-form expression in this example, although we can still compute it using R. Let’s compute these three statistics: # Prior params alpha &lt;- 3.358 beta &lt;- 0.778 # Y: observed world population observedpop &lt;- worldpop %&gt;% filter(country == &#39;WORLD&#39;,year == 2010) %&gt;% pull(population) observedpop &lt;- observedpop * 1e-06 # Convert to billions, from thousands # Posterior mode lambda_mode &lt;- (alpha + observedpop - 1) / (beta + 1) # Posterior mean lambda_mean &lt;- (alpha + observedpop) / (beta + 1) # Posterior median lambda_med &lt;- qgamma(.5,alpha + observedpop,beta+1) # Look at them all c( &#39;observed&#39; = observedpop, &#39;post_mode&#39; = lambda_mode, &#39;post_mean&#39; = lambda_mean, &#39;post_med&#39; = lambda_med ) observed post_mode post_mean post_med 6.9568 5.2389 5.8014 5.6150 Compare these to their corresponding prior values: # Prior params alpha &lt;- 3.358 beta &lt;- 0.778 # Prior mode prior_mode &lt;- (alpha - 1) / (beta) # Prior mean prior_mean &lt;- (alpha) / (beta) # Prior median prior_med &lt;- qgamma(.5,alpha,beta) # Look at them all c( &#39;observed&#39; = observedpop, &#39;prior_mode&#39; = prior_mode, &#39;prior_mean&#39; = prior_mean, &#39;prior_med&#39; = prior_med ) observed prior_mode prior_mean prior_med 6.9568 3.0308 4.3162 3.8961 Each statistic is pulled towards its corresponding prior value. The skewness in the Gamma distribution means that the mode and median are less than the mean. The observed data affects not only the location but also the shape of the posterior relative to the prior. The observed value being higher than the prior mean and mode causes them to be pulled up, but the median is actually pulled down. Since computing the posterior seems harder than working with a likelihood, and since coming up with a point estimator based on the posterior seems harder than maximizing a likelihood, you might be wondering why we’re bothering with Bayesian inference at all. One compelling answer comes from uncertainty quantification. Using the non-Bayesian methods, it was difficult to come up with a way to quantify uncertainty. We did a lot of work and came up with an interval that “if you were to repeat the experiment over and over and calculate many such intervals, \\(95\\%\\) of them would cover the true value of the parameter”. This is difficult to explain and interpret, and also, when you move into more complicated statistical procedures, it becomes difficult or impossible to derive such a confidence interval. Because of this, some of the most popular modern statistical procedures use Bayesian inference to compute uncertainty estimates (even nominally frequentist techniques), or don’t compute them at all. Since Statistics is the science of uncertainty, this means Bayesian inference is a lot more mainstream than it might appear to be when taught in introductory courses such as this. The reason is that computing uncertainty intervals in Bayesian stats is, for the most part, pretty straightforward. Uncertainty for \\(\\lambda|Y\\) is quantified by means of a credible interval. A \\(95\\%\\) (or some other level) credible interval is any pair of numbers \\((L,U)\\) which satisfy \\[ P(L \\leq \\lambda \\leq U | Y) = 95\\%. \\] The interpretation is straightforward: given the observed data, \\(\\lambda\\) has a \\(95\\%\\) posterior probability of being between \\(L\\) and \\(U\\). You can use any \\(L\\) and \\(U\\) that satisfy this, but the common choice is to just use the \\(2.5\\%\\) and \\(97.5\\%\\) posterior quantiles, in analogy to the construction of frequentist confidence intervals. Less common but still sometimes done is to compute one sided intervals, taking \\(L\\) or \\(U\\) to be the endpoint of the parameter space (which might be \\(\\pm\\infty\\)). Here are three calculations and visualizations of posterior credible intervals: # Lower interval lowerint &lt;- c(0,qgamma(.95,alpha+observedpop,beta+1)) # Upper interval # Top point should be Inf, had to use big finite value for plotting upperint &lt;- c(qgamma(.05,alpha+observedpop,beta+1),15) # Should be Inf # Middle interval, most common middleint &lt;- qgamma(c(.025,.975),alpha+observedpop,beta+1) # Plot them baseplot &lt;- tibble(x = c(0,15)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = dgamma,args = list(shape = alpha+observedpop,rate = beta+1)) lowerplot &lt;- baseplot + stat_function(fun = dgamma, args = list(shape = alpha+observedpop,rate = beta+1), xlim = lowerint, geom = &quot;area&quot;, alpha = 0.3) + labs(title = &quot;Lower one-sided 95% posterior credible interval&quot;, x = expression(lambda),y = &quot;Posterior Density&quot;) upperplot &lt;- baseplot + stat_function(fun = dgamma, args = list(shape = alpha+observedpop,rate = beta+1), xlim = upperint, geom = &quot;area&quot;, alpha = 0.3) + labs(title = &quot;Upper one-sided 95% posterior credible interval&quot;, x = expression(lambda),y = &quot;Posterior Density&quot;) middleplot &lt;- baseplot + stat_function(fun = dgamma, args = list(shape = alpha+observedpop,rate = beta+1), xlim = middleint, geom = &quot;area&quot;, alpha = 0.3) + labs(title = &quot;Two-sided 95% posterior credible interval&quot;, x = expression(lambda),y = &quot;Posterior Density&quot;) lowerplot / middleplot / upperplot 15.5.2 Estimation by sampling Usually in practice you won’t know the posterior distribution exactly, but you might have a sample from it. Alternatively, you might just want to calculate any estimates you want without having to do tedious math. Bayesian inference can be done using only a sample from the posterior. In fact, this is very desirable: all of your data analytic skills can be carried over and used for inference. We will briefly touch on this here. Suppose we have a sample from the posterior: sample_from_the_posterior &lt;- rgamma(1000,alpha+observedpop,beta+1) You can compute an estimate of any quantity you like from this sample, without having to do math. Check it out and compare to what we did before: # Mean mean(sample_from_the_posterior) [1] 5.8378 lambda_mean [1] 5.8014 # Ok, the mode is a bit trickier (actually in practice this is the one # that I would always calculate using math or brute-force optimization): ss &lt;- round(sample_from_the_posterior,3) sst &lt;- table(ss) names(sst[sst == max(sst)]) [1] &quot;4.589&quot; &quot;4.671&quot; &quot;5.706&quot; &quot;6.593&quot; lambda_mode [1] 5.2389 # Difficult to calculate and not that accurate # Everything else is pretty good though: # Quantiles and median mm &lt;- median(sample_from_the_posterior) q2.5 &lt;- quantile(sample_from_the_posterior,.025) q97.5 &lt;- quantile(sample_from_the_posterior,.975) c(q2.5,mm,q97.5) 2.5% 97.5% 2.8977 5.7171 9.9938 c(middleint[1],lambda_med,middleint[2]) [1] 2.8194 5.6150 9.8411 Sample-based Bayesian inference is a subject of entire upper year courses! Exercise: these estimates and intervals are for the entire world population, and for 2010. Do the following: Pick any country, and year (perhaps the current year, or the year you were born or first attended university), Use Bayesian inference to estimate the population of that country in that year. Come up with a suitable prior (still use the Gamma, but change the parameters), and give a point and interval estimate based upon the posterior distribution. 15.6 Predicting world population Finally, let’s predict world population through 2100 and see how our results compare to the predictions given by the WHO. In section 1 of this chapter we read in the files containing the WHO’s predictions for world population until 2100: glimpse(worldpop_pred) Observations: 4,012 Variables: 5 $ country &lt;chr&gt; &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORLD&quot;, &quot;WORL… $ year &lt;dbl&gt; 2020, 2025, 2030, 2035, 2040, 2045, 2050, 2055, 20… $ population &lt;dbl&gt; 7794799, 8184437, 8548487, 8887524, 9198847, 94818… $ population_lower95 &lt;dbl&gt; 7794799, 8144343, 8460182, 8746077, 8996324, 92131… $ population_upper95 &lt;dbl&gt; 7794799, 8223574, 8636041, 9029502, 9396987, 97448… The population variable contains their predicted world population for each year and the population_lower95 and population_upper95 contain the endpoints of a \\(95\\%\\) prediction interval for population. Let’s plot their predictions: whopredplot &lt;- worldpop_pred %&gt;% filter(country == &#39;WORLD&#39;) %&gt;% ggplot(aes(x = year)) + theme_classic() + geom_point(aes(y = population*1e-06)) + geom_ribbon(aes(ymin = population_lower95*1e-06,ymax = population_upper95*1e-06),alpha = .1) + labs(title = &quot;WHO predicted world population with 95% prediction interval&quot;, x = &quot;Year&quot;, y = &quot;World population (Billions)&quot;) whopredplot It doesn’t look like the WHO used a simple linear model like we did. We can plot our predictions with their associated prediction intervals like this: # Predict for the same years as the WHO datatopredict &lt;- data.frame(year = sort(unique(worldpop_pred$year))) preddat &lt;- predict(diffmod,newdata = datatopredict,se.fit = TRUE,interval = &quot;prediction&quot;)$fit %&gt;% as_tibble() preddat$year &lt;- datatopredict$year worldpoppred &lt;- inner_join(datatopredict,preddat,by = &quot;year&quot;) glimpse(worldpoppred) Observations: 17 Variables: 4 $ year &lt;dbl&gt; 2020, 2025, 2030, 2035, 2040, 2045, 2050, 2055, 2060, 2065, 2070… $ fit &lt;dbl&gt; 7692970, 8082354, 8471738, 8861122, 9250505, 9639889, 10029273, … $ lwr &lt;dbl&gt; 7493772, 7881972, 8270023, 8657928, 9045689, 9433310, 9820795, 1… $ upr &lt;dbl&gt; 7892168, 8282736, 8673452, 9064315, 9455322, 9846468, 10237751, … predplotpredint &lt;- worldpoppred %&gt;% ggplot(aes(x = year,y = fit)) + theme_classic() + # Shaded region for the interval geom_ribbon(aes(ymin = lwr,ymax = upr),alpha = .1) + geom_line() + theme(axis.text.x = element_text(angle = 90)) + labs(title = &quot;World population with estimate and 95% PI&quot;, x = &quot;Year&quot;, y = &quot;World Population (Billions)&quot;) + scale_x_continuous(breaks = seq(2020,2100,by=10)) + scale_y_continuous(labels = function(x) x*1e-06,breaks = (8:14)*1e06) predplotpredint | whopredplot Our model is simpler than whatever the WHO did: we predict that population is going to go up by the same amount each year for the next 80 years. Does that seem reasonable? Because we impose such a stringent assumption on the phenomenon we’re trying to model, we get really confident predictions, as measured by the width of the prediction interval. But, this is not necessarily good: we might be confidently incorrect! We probably are; the WHO knows what they are doing, and they built a model which allows for a curvy relationship between time and population, but with much wider uncertainty bands. Predictive modelling is an art as well as a science. Exercise: try it yourself. Use our previous “log” model which assumed a constant rate of population growth over time. I got the following: Observations: 17 Variables: 4 $ year &lt;dbl&gt; 2020, 2025, 2030, 2035, 2040, 2045, 2050, 2055, 2060, 2065, 2070… $ fit &lt;dbl&gt; 15.939, 16.022, 16.104, 16.186, 16.269, 16.351, 16.433, 16.515, … $ lwr &lt;dbl&gt; 15.874, 15.956, 16.038, 16.120, 16.201, 16.283, 16.365, 16.446, … $ upr &lt;dbl&gt; 16.005, 16.087, 16.170, 16.253, 16.336, 16.419, 16.502, 16.584, … Woah! This model looks even worse. However, it is a bit more flexible (just a bit) than the simple linear model, and the prediction interval is even wider. Exercise: pick your favourite country and year, and predict its population to 2100 and compare with the WHO. Are your predictions more or less reasonable than what we did for the total world population? 15.6.1 Why aren’t our predictions accurate? The WHO uses decades (centuries?) of demographic theory and research, combined with data on mortality, fertility, and other demographic factors to make its predictions. They are constantly changing as new information and theories become available. In contrast, we just took some numbers and plugged them into a textbook statistical procedure. It would be troubling if we got anything remotely comparable to what the WHO got using this approach. Keep this in mind as you approach your future projects and industry or academic work. This example was useful for teaching the mechanics of prediction, but there is an upper limit to how good you can get without domain-specific knowledge and additional data sources. In upper year courses you’ll learn about new types of fancy predictive (and other) models, which is great, but it always always always pays to think about the problem harder too! Better statistical techniques are not a replacement for better science. With that, you’re done STA238. Hope you enjoyed the course, and maybe see you in future courses too :). "]]
