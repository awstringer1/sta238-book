<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Introduction to Bayesian Inference | Probability, Statistics, and Data Analysis</title>
  <meta name="description" content="This book represents part of the course materials for STA238 at the University of Toronto" />
  <meta name="generator" content="bookdown 0.21.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Introduction to Bayesian Inference | Probability, Statistics, and Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book represents part of the course materials for STA238 at the University of Toronto" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Introduction to Bayesian Inference | Probability, Statistics, and Data Analysis" />
  
  <meta name="twitter:description" content="This book represents part of the course materials for STA238 at the University of Toronto" />
  

<meta name="author" content="Alison Gibbs and Alex Stringer" />


<meta name="date" content="2020-11-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="section-evaluating-estimators-efficiency-and-mean-squared-error.html"/>
<link rel="next" href="section-the-bootstrap.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STA238 University of Toronto</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><i class="fa fa-check"></i><b>2</b> Introduction to Data Analysis: Data Input and Basic Summaries</a><ul>
<li class="chapter" data-level="2.1" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-old-faithful"><i class="fa fa-check"></i><b>2.1</b> Old Faithful</a><ul>
<li class="chapter" data-level="2.1.1" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-read-in-the-data"><i class="fa fa-check"></i><b>2.1.1</b> Read in the data</a></li>
<li class="chapter" data-level="2.1.2" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-graphical-summaries"><i class="fa fa-check"></i><b>2.1.2</b> Graphical Summaries</a></li>
<li class="chapter" data-level="2.1.3" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-numerical-summaries"><i class="fa fa-check"></i><b>2.1.3</b> Numerical Summaries</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-drilling"><i class="fa fa-check"></i><b>2.2</b> Drilling</a><ul>
<li class="chapter" data-level="2.2.1" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-read-in-data"><i class="fa fa-check"></i><b>2.2.1</b> Read in data</a></li>
<li class="chapter" data-level="2.2.2" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-graphical-summaries-1"><i class="fa fa-check"></i><b>2.2.2</b> Graphical summaries</a></li>
<li class="chapter" data-level="2.2.3" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-numerical-summaries-1"><i class="fa fa-check"></i><b>2.2.3</b> Numerical summaries</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.4" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-extended-example-smoking-and-age-and-mortality"><i class="fa fa-check"></i><b>2.4</b> Extended example: smoking and age and mortality</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-exercises-1"><i class="fa fa-check"></i><b>2.4.1</b> Exercises</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-association-between-smoking-and-mortality"><i class="fa fa-check"></i><b>2.4.2</b> Association between smoking and mortality</a></li>
<li class="chapter" data-level="2.4.3" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-exercises-2"><i class="fa fa-check"></i><b>2.4.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-case-study-rental-housing-in-toronto"><i class="fa fa-check"></i><b>2.5</b> Case study: rental housing in Toronto</a><ul>
<li class="chapter" data-level="2.5.1" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-load-the-data"><i class="fa fa-check"></i><b>2.5.1</b> Load the data</a></li>
<li class="chapter" data-level="2.5.2" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-analysis-i-what-does-the-data-look-like"><i class="fa fa-check"></i><b>2.5.2</b> Analysis I: what does the data look like?</a></li>
<li class="chapter" data-level="2.5.3" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-analysis-ii-do-different-wards-have-different-quality-housing"><i class="fa fa-check"></i><b>2.5.3</b> Analysis II: Do different wards have different quality housing?</a></li>
<li class="chapter" data-level="2.5.4" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-analysis-iii-trends-in-quality-over-time"><i class="fa fa-check"></i><b>2.5.4</b> Analysis III: trends in quality over time</a></li>
<li class="chapter" data-level="2.5.5" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-summary"><i class="fa fa-check"></i><b>2.5.5</b> Summary</a></li>
<li class="chapter" data-level="2.5.6" data-path="section-introduction-to-data-analysis-data-input-and-basic-summaries.html"><a href="section-introduction-to-data-analysis-data-input-and-basic-summaries.html#section-exercises-3"><i class="fa fa-check"></i><b>2.5.6</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html"><a href="section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html"><i class="fa fa-check"></i><b>3</b> Introduction to Statistics: Law of Large Numbers and Central Limit Theorem</a><ul>
<li class="chapter" data-level="3.1" data-path="section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html"><a href="section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html#section-law-of-large-numbers-chapter-13"><i class="fa fa-check"></i><b>3.1</b> Law of Large Numbers (Chapter 13)</a><ul>
<li class="chapter" data-level="3.1.1" data-path="section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html"><a href="section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html#section-extended-example-the-probability-of-heads"><i class="fa fa-check"></i><b>3.1.1</b> Extended example: the probability of heads</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html"><a href="section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html#section-central-limit-theorem-chapter-14"><i class="fa fa-check"></i><b>3.2</b> Central Limit Theorem (Chapter 14)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html"><a href="section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html#section-extended-example-the-probability-of-heads-1"><i class="fa fa-check"></i><b>3.2.1</b> Extended example: the probability of heads</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-statistical-models.html"><a href="section-statistical-models.html"><i class="fa fa-check"></i><b>4</b> Statistical Models</a><ul>
<li class="chapter" data-level="4.1" data-path="section-statistical-models.html"><a href="section-statistical-models.html#section-statistical-models-chapter-17"><i class="fa fa-check"></i><b>4.1</b> Statistical models (Chapter 17)</a><ul>
<li class="chapter" data-level="4.1.1" data-path="section-statistical-models.html"><a href="section-statistical-models.html#section-linear-regression"><i class="fa fa-check"></i><b>4.1.1</b> Linear Regression</a></li>
<li class="chapter" data-level="4.1.2" data-path="section-statistical-models.html"><a href="section-statistical-models.html#section-extended-example-ttc-ridership-revenues"><i class="fa fa-check"></i><b>4.1.2</b> Extended example: TTC ridership revenues</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="section-statistical-models.html"><a href="section-statistical-models.html#section-unbiased-estimators-chapter-19"><i class="fa fa-check"></i><b>4.2</b> Unbiased Estimators (Chapter 19)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="section-statistical-models.html"><a href="section-statistical-models.html#section-simulated-data"><i class="fa fa-check"></i><b>4.2.1</b> Simulated data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-evaluating-estimators-efficiency-and-mean-squared-error.html"><a href="section-evaluating-estimators-efficiency-and-mean-squared-error.html"><i class="fa fa-check"></i><b>5</b> Evaluating Estimators: Efficiency and Mean Squared Error</a><ul>
<li class="chapter" data-level="5.1" data-path="section-evaluating-estimators-efficiency-and-mean-squared-error.html"><a href="section-evaluating-estimators-efficiency-and-mean-squared-error.html#section-estimating-a-uniform-maximum"><i class="fa fa-check"></i><b>5.1</b> Estimating a Uniform Maximum</a></li>
<li class="chapter" data-level="5.2" data-path="section-evaluating-estimators-efficiency-and-mean-squared-error.html"><a href="section-evaluating-estimators-efficiency-and-mean-squared-error.html#section-efficiency"><i class="fa fa-check"></i><b>5.2</b> Efficiency</a></li>
<li class="chapter" data-level="5.3" data-path="section-evaluating-estimators-efficiency-and-mean-squared-error.html"><a href="section-evaluating-estimators-efficiency-and-mean-squared-error.html#section-mean-squared-error"><i class="fa fa-check"></i><b>5.3</b> Mean Squared Error</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-introduction-to-bayesian-inference.html"><a href="section-introduction-to-bayesian-inference.html"><i class="fa fa-check"></i><b>6</b> Introduction to Bayesian Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="section-introduction-to-bayesian-inference.html"><a href="section-introduction-to-bayesian-inference.html#section-tutorial"><i class="fa fa-check"></i><b>6.1</b> Tutorial</a><ul>
<li class="chapter" data-level="6.1.1" data-path="section-introduction-to-bayesian-inference.html"><a href="section-introduction-to-bayesian-inference.html#section-frequentistlikelihood-perspective"><i class="fa fa-check"></i><b>6.1.1</b> Frequentist/Likelihood Perspective</a></li>
<li class="chapter" data-level="6.1.2" data-path="section-introduction-to-bayesian-inference.html"><a href="section-introduction-to-bayesian-inference.html#section-bayesian-inference-introduction"><i class="fa fa-check"></i><b>6.1.2</b> Bayesian Inference: introduction</a></li>
<li class="chapter" data-level="6.1.3" data-path="section-introduction-to-bayesian-inference.html"><a href="section-introduction-to-bayesian-inference.html#section-flipping-more-coins"><i class="fa fa-check"></i><b>6.1.3</b> Flipping More Coins</a></li>
<li class="chapter" data-level="6.1.4" data-path="section-introduction-to-bayesian-inference.html"><a href="section-introduction-to-bayesian-inference.html#section-visualization"><i class="fa fa-check"></i><b>6.1.4</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="section-introduction-to-bayesian-inference.html"><a href="section-introduction-to-bayesian-inference.html#section-interactive-app"><i class="fa fa-check"></i><b>6.2</b> Interactive App</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-the-bootstrap.html"><a href="section-the-bootstrap.html"><i class="fa fa-check"></i><b>7</b> The Bootstrap</a><ul>
<li class="chapter" data-level="7.1" data-path="section-the-bootstrap.html"><a href="section-the-bootstrap.html#section-the-bootstrap-chapter-18"><i class="fa fa-check"></i><b>7.1</b> The Bootstrap (Chapter 18)</a><ul>
<li class="chapter" data-level="7.1.1" data-path="section-the-bootstrap.html"><a href="section-the-bootstrap.html#section-empirical-bootstrap-old-faithful-data"><i class="fa fa-check"></i><b>7.1.1</b> Empirical bootstrap: Old Faithful data</a></li>
<li class="chapter" data-level="7.1.2" data-path="section-the-bootstrap.html"><a href="section-the-bootstrap.html#section-parametric-bootstrap-software-data"><i class="fa fa-check"></i><b>7.1.2</b> Parametric Bootstrap: software data</a></li>
<li class="chapter" data-level="7.1.3" data-path="section-the-bootstrap.html"><a href="section-the-bootstrap.html#section-extended-example-the-standard-error-of-a-proportion"><i class="fa fa-check"></i><b>7.1.3</b> Extended example: the standard error of a proportion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-maximum-likelihood.html"><a href="section-maximum-likelihood.html"><i class="fa fa-check"></i><b>8</b> Maximum Likelihood</a><ul>
<li class="chapter" data-level="8.1" data-path="section-maximum-likelihood.html"><a href="section-maximum-likelihood.html#section-maximum-likelihood-chapter-21"><i class="fa fa-check"></i><b>8.1</b> Maximum Likelihood (Chapter 21)</a><ul>
<li class="chapter" data-level="8.1.1" data-path="section-maximum-likelihood.html"><a href="section-maximum-likelihood.html#section-example-two-coins"><i class="fa fa-check"></i><b>8.1.1</b> Example: two coins</a></li>
<li class="chapter" data-level="8.1.2" data-path="section-maximum-likelihood.html"><a href="section-maximum-likelihood.html#section-example-unknown-coins-n-2"><i class="fa fa-check"></i><b>8.1.2</b> Example: unknown coins, <span class="math inline">\(n = 2\)</span></a></li>
<li class="chapter" data-level="8.1.3" data-path="section-maximum-likelihood.html"><a href="section-maximum-likelihood.html#section-example-unknown-coins-n-bigger-than-2"><i class="fa fa-check"></i><b>8.1.3</b> Example: unknown coins, <span class="math inline">\(n\)</span> bigger than <span class="math inline">\(2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="section-maximum-likelihood.html"><a href="section-maximum-likelihood.html#section-extended-example-rental-housing-in-toronto"><i class="fa fa-check"></i><b>8.2</b> Extended example: rental housing in Toronto</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-confidence-intervals-and-quantifying-uncertainty.html"><a href="section-confidence-intervals-and-quantifying-uncertainty.html"><i class="fa fa-check"></i><b>9</b> Confidence Intervals and Quantifying Uncertainty</a><ul>
<li class="chapter" data-level="9.1" data-path="section-confidence-intervals-and-quantifying-uncertainty.html"><a href="section-confidence-intervals-and-quantifying-uncertainty.html#section-confidence-intervals-for-the-mean-chapter-23"><i class="fa fa-check"></i><b>9.1</b> Confidence Intervals for the Mean (Chapter 23)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="section-confidence-intervals-and-quantifying-uncertainty.html"><a href="section-confidence-intervals-and-quantifying-uncertainty.html#section-simulation-an-example"><i class="fa fa-check"></i><b>9.1.1</b> Simulation: an example</a></li>
<li class="chapter" data-level="9.1.2" data-path="section-confidence-intervals-and-quantifying-uncertainty.html"><a href="section-confidence-intervals-and-quantifying-uncertainty.html#section-gross-calorific-value-measurements-for-osterfeld-262de27"><i class="fa fa-check"></i><b>9.1.2</b> Gross calorific value measurements for Osterfeld 262DE27</a></li>
<li class="chapter" data-level="9.1.3" data-path="section-confidence-intervals-and-quantifying-uncertainty.html"><a href="section-confidence-intervals-and-quantifying-uncertainty.html#section-when-you-dont-know-sigma"><i class="fa fa-check"></i><b>9.1.3</b> When you don’t know <span class="math inline">\(\sigma\)</span></a></li>
<li class="chapter" data-level="9.1.4" data-path="section-confidence-intervals-and-quantifying-uncertainty.html"><a href="section-confidence-intervals-and-quantifying-uncertainty.html#section-gross-calorific-value-measurements-for-daw-mill-258gb41"><i class="fa fa-check"></i><b>9.1.4</b> Gross calorific value measurements for Daw Mill 258GB41</a></li>
<li class="chapter" data-level="9.1.5" data-path="section-confidence-intervals-and-quantifying-uncertainty.html"><a href="section-confidence-intervals-and-quantifying-uncertainty.html#section-bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>9.1.5</b> Bootstrap Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html"><i class="fa fa-check"></i><b>10</b> Extended Example: Reasoning About Goodness of Fit</a><ul>
<li class="chapter" data-level="10.1" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-go-and-read-the-blog-post"><i class="fa fa-check"></i><b>10.1</b> Go and read the blog post</a></li>
<li class="chapter" data-level="10.2" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-distribution-of-last-digits"><i class="fa fa-check"></i><b>10.2</b> Distribution of last digits</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-read-in-the-data-1"><i class="fa fa-check"></i><b>10.2.1</b> Read in the data</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-make-the-histogram"><i class="fa fa-check"></i><b>10.2.2</b> Make the histogram</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-testing-goodness-of-fit-simulation"><i class="fa fa-check"></i><b>10.2.3</b> Testing goodness of fit: simulation</a></li>
<li class="chapter" data-level="10.2.4" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-testing-goodness-of-fit-math"><i class="fa fa-check"></i><b>10.2.4</b> Testing goodness of fit: math</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-bayesian-inference-estimation.html"><a href="section-bayesian-inference-estimation.html"><i class="fa fa-check"></i><b>11</b> Bayesian Inference: Estimation</a><ul>
<li class="chapter" data-level="11.1" data-path="section-bayesian-inference-estimation.html"><a href="section-bayesian-inference-estimation.html#section-estimation-in-bayesian-inference-general-ideas"><i class="fa fa-check"></i><b>11.1</b> Estimation in Bayesian Inference: general ideas</a><ul>
<li class="chapter" data-level="11.1.1" data-path="section-bayesian-inference-estimation.html"><a href="section-bayesian-inference-estimation.html#section-the-prior"><i class="fa fa-check"></i><b>11.1.1</b> The Prior</a></li>
<li class="chapter" data-level="11.1.2" data-path="section-bayesian-inference-estimation.html"><a href="section-bayesian-inference-estimation.html#section-the-posterior"><i class="fa fa-check"></i><b>11.1.2</b> The Posterior</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="section-bayesian-inference-estimation.html"><a href="section-bayesian-inference-estimation.html#section-estimation-in-bayesian-inference-point-and-interval-estimation"><i class="fa fa-check"></i><b>11.2</b> Estimation in Bayesian Inference: point and interval estimation</a></li>
<li class="chapter" data-level="11.3" data-path="section-bayesian-inference-estimation.html"><a href="section-bayesian-inference-estimation.html#section-choosing-a-prior"><i class="fa fa-check"></i><b>11.3</b> Choosing a Prior</a><ul>
<li class="chapter" data-level="11.3.1" data-path="section-bayesian-inference-estimation.html"><a href="section-bayesian-inference-estimation.html#section-conjugate-priors"><i class="fa fa-check"></i><b>11.3.1</b> Conjugate Priors</a></li>
<li class="chapter" data-level="11.3.2" data-path="section-bayesian-inference-estimation.html"><a href="section-bayesian-inference-estimation.html#section-setting-hyperparameters-by-moment-matching"><i class="fa fa-check"></i><b>11.3.2</b> Setting Hyperparameters by moment-matching</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html"><i class="fa fa-check"></i><b>12</b> Predictive Modelling</a><ul>
<li class="chapter" data-level="12.1" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html#section-overview"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html#section-plug-in-prediction-coin-flipping"><i class="fa fa-check"></i><b>12.2</b> Plug-in prediction: coin flipping</a></li>
<li class="chapter" data-level="12.3" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html#section-bayesian-prediction-coin-flipping"><i class="fa fa-check"></i><b>12.3</b> Bayesian Prediction: coin flipping</a></li>
<li class="chapter" data-level="12.4" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html#section-extended-example-predicting-call-centre-wait-times"><i class="fa fa-check"></i><b>12.4</b> Extended example: predicting call centre wait times</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-installing-r-and-rstudio.html"><a href="section-installing-r-and-rstudio.html"><i class="fa fa-check"></i><b>13</b> Installing R and RStudio</a><ul>
<li class="chapter" data-level="13.1" data-path="section-installing-r-and-rstudio.html"><a href="section-installing-r-and-rstudio.html#section-installing-r"><i class="fa fa-check"></i><b>13.1</b> Installing R</a></li>
<li class="chapter" data-level="13.2" data-path="section-installing-r-and-rstudio.html"><a href="section-installing-r-and-rstudio.html#section-installing-rstudio"><i class="fa fa-check"></i><b>13.2</b> Installing RStudio</a></li>
<li class="chapter" data-level="13.3" data-path="section-installing-r-and-rstudio.html"><a href="section-installing-r-and-rstudio.html#section-using-rmarkdown"><i class="fa fa-check"></i><b>13.3</b> Using RMarkdown</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="section-assigned-exercises-from-mips.html"><a href="section-assigned-exercises-from-mips.html"><i class="fa fa-check"></i><b>14</b> Assigned Exercises from MIPS</a><ul>
<li class="chapter" data-level="14.1" data-path="section-assigned-exercises-from-mips.html"><a href="section-assigned-exercises-from-mips.html#section-chapters-15-and-16"><i class="fa fa-check"></i><b>14.1</b> Chapters 15 and 16</a></li>
<li class="chapter" data-level="14.2" data-path="section-assigned-exercises-from-mips.html"><a href="section-assigned-exercises-from-mips.html#section-chapters-13-and-14"><i class="fa fa-check"></i><b>14.2</b> Chapters 13 and 14</a></li>
<li class="chapter" data-level="14.3" data-path="section-assigned-exercises-from-mips.html"><a href="section-assigned-exercises-from-mips.html#section-chapters-17-and-19"><i class="fa fa-check"></i><b>14.3</b> Chapters 17 and 19</a></li>
<li class="chapter" data-level="14.4" data-path="section-assigned-exercises-from-mips.html"><a href="section-assigned-exercises-from-mips.html#section-chapter-20"><i class="fa fa-check"></i><b>14.4</b> Chapter 20</a></li>
<li class="chapter" data-level="14.5" data-path="section-assigned-exercises-from-mips.html"><a href="section-assigned-exercises-from-mips.html#section-evans-and-rosenthal-chapter-7.1"><i class="fa fa-check"></i><b>14.5</b> Evans and Rosenthal, Chapter 7.1</a></li>
<li class="chapter" data-level="14.6" data-path="section-assigned-exercises-from-mips.html"><a href="section-assigned-exercises-from-mips.html#section-chapter-18"><i class="fa fa-check"></i><b>14.6</b> Chapter 18</a></li>
<li class="chapter" data-level="14.7" data-path="section-assigned-exercises-from-mips.html"><a href="section-assigned-exercises-from-mips.html#section-chapter-21"><i class="fa fa-check"></i><b>14.7</b> Chapter 21</a></li>
<li class="chapter" data-level="14.8" data-path="section-assigned-exercises-from-mips.html"><a href="section-assigned-exercises-from-mips.html#section-chapter-23"><i class="fa fa-check"></i><b>14.8</b> Chapter 23</a></li>
<li class="chapter" data-level="14.9" data-path="section-assigned-exercises-from-mips.html"><a href="section-assigned-exercises-from-mips.html#section-evans-and-rosenthal-chapter-7.2"><i class="fa fa-check"></i><b>14.9</b> Evans and Rosenthal, Chapter 7.2</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="section-extended-example-world-population-data.html"><a href="section-extended-example-world-population-data.html"><i class="fa fa-check"></i><b>15</b> Extended Example: World Population Data</a><ul>
<li class="chapter" data-level="15.1" data-path="section-extended-example-world-population-data.html"><a href="section-extended-example-world-population-data.html#section-read-in-and-prepare-the-data-for-analysis"><i class="fa fa-check"></i><b>15.1</b> Read in and prepare the data for analysis</a></li>
<li class="chapter" data-level="15.2" data-path="section-extended-example-world-population-data.html"><a href="section-extended-example-world-population-data.html#section-model-world-population-over-time"><i class="fa fa-check"></i><b>15.2</b> Model world population over time</a></li>
<li class="chapter" data-level="15.3" data-path="section-extended-example-world-population-data.html"><a href="section-extended-example-world-population-data.html#section-bayesian-model-for-world-population"><i class="fa fa-check"></i><b>15.3</b> Bayesian model for world population</a></li>
<li class="chapter" data-level="15.4" data-path="section-extended-example-world-population-data.html"><a href="section-extended-example-world-population-data.html#section-quantifying-uncertainty-in-estimates-of-world-population-regression-model"><i class="fa fa-check"></i><b>15.4</b> Quantifying uncertainty in estimates of world population: regression model</a></li>
<li class="chapter" data-level="15.5" data-path="section-extended-example-world-population-data.html"><a href="section-extended-example-world-population-data.html#section-bayesian-estimate-of-world-population"><i class="fa fa-check"></i><b>15.5</b> Bayesian estimate of world population</a><ul>
<li class="chapter" data-level="15.5.1" data-path="section-extended-example-world-population-data.html"><a href="section-extended-example-world-population-data.html#section-estimation-and-uncertainty-quantification"><i class="fa fa-check"></i><b>15.5.1</b> Estimation and uncertainty quantification</a></li>
<li class="chapter" data-level="15.5.2" data-path="section-extended-example-world-population-data.html"><a href="section-extended-example-world-population-data.html#section-estimation-by-sampling"><i class="fa fa-check"></i><b>15.5.2</b> Estimation by sampling</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="section-extended-example-world-population-data.html"><a href="section-extended-example-world-population-data.html#section-predicting-world-population"><i class="fa fa-check"></i><b>15.6</b> Predicting world population</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Probability, Statistics, and Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-introduction-to-bayesian-inference" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Introduction to Bayesian Inference</h1>
<p>This chapter introduces the foundations of Bayesian inference. Materials in this tutorial are taken from Alex Stringer’s <a href="https://awstringer1.github.io/leaf2018/intro-to-bayesian.html">comprehensive tutorial on Bayesian Inference</a>, which is very long and outside the scope of this course.</p>
<div id="section-tutorial" class="section level2">
<h2><span class="header-section-number">6.1</span> Tutorial</h2>
<p>In this tutorial we will discuss at length the Beta-Bernoulli example
from section 7.1. First follow along with this tutorial, then check out the <a href="http://shiny.sta220.utstat.utoronto.ca:88/BayesianApp/">interactive app</a>.</p>
<p><strong>Review</strong>: given data generated from some family of probability distributions indexed by an unknown parameter, statistical inference is concerned with <strong>estimating</strong> these parameters- finding reasonable values for them, given the observed data, and <strong>quantifying the uncertainty</strong> in these estimates. The central notion is that of <em>uncertainty</em>: we simply don’t know the values of the parameters that generated the data we observed, and we do know that several different values could reasonably have generated these data. <strong>Probability</strong> is the mathematical construct used to represent uncertainty.</p>
<div id="section-frequentistlikelihood-perspective" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Frequentist/Likelihood Perspective</h3>
<p>Classically, the approach to this problem is taught from the <strong>frequentist perspective</strong>. Uncertainty in the values of the parameters that generated the data is represented by probability via the notion of <em>repeated sampling</em>:</p>
<blockquote>
<p>under the given probability model with the given parameter values, what is the relative frequency with which these same data would be observed, if the experiment that generated the data were repeated again and again?</p>
</blockquote>
<p>Values of the parameters that have a higher probability of having generated the observed data are thought to be more <strong>likely</strong> than other values. “Likelihood” is NOT a synonym of “probability”: probability refers to the realization of a random variable (what is the probability we see 4 heads in 10 flips of a coin?), while likelihood refers to the value of a parameter (what is the likelihood of having seen 4 heads in 10 flips of a coin <em>which has a .4 chance of coming up heads</em>?).</p>
<p>As an example, consider a coin with unknown <strong>parameter</strong> <span class="math inline">\(\theta\)</span> representing the probability of heads. We toss the coin once, and observe random outcome (data) <span class="math inline">\(X = 1\)</span> if the toss is heads and <span class="math inline">\(X = 0\)</span> if not. For simplicity, suppose we know we have chosen one of two possible coins, either having <span class="math inline">\(\theta = 0.7\)</span> or <span class="math inline">\(\theta = 0.3\)</span>. How do we use the observed data to infer which of these two coins we threw?</p>
<p>For any <span class="math inline">\(0 &lt; \theta &lt; 1\)</span>, the probability distribution of the single coin toss is given by
<span class="math display">\[
P(X = x) = \theta^{x}(1-\theta)^{1-x}
\]</span>
This just says that <span class="math inline">\(P(X = 1) = \theta\)</span> and <span class="math inline">\(P(X = 0) = 1-\theta\)</span>.</p>
<p>Let’s say we throw the coin once, and observe <span class="math inline">\(X = 1\)</span>. If <span class="math inline">\(\theta = 0.7\)</span> the probabilty of observing this result is <span class="math inline">\(P(X = 1|\theta = 0.7) = 0.7\)</span>. That is if <span class="math inline">\(\theta = 0.7\)</span>, we would expect roughly <span class="math inline">\(70\%\)</span> of repetitions of this experiment to yield the same results as we observed in our data. If <span class="math inline">\(\theta = 0.3\)</span> on the other hand, <span class="math inline">\(P(X = 1|\theta = 0.3) = 0.3\)</span>; only <span class="math inline">\(30\%\)</span> of the repetitions of this experiment would yield the observed data if <span class="math inline">\(\theta = 0.3\)</span>. Because <span class="math inline">\(\theta = 0.7\)</span> would yield the observed data more frequently than <span class="math inline">\(\theta = 0.3\)</span>, we say that <span class="math inline">\(\theta = 0.7\)</span> is <em>more likely</em> to have generated the observed data than <span class="math inline">\(\theta=0.3\)</span>, and our inference favours <span class="math inline">\(\theta =0.7\)</span>.</p>
<p><strong>Exercise</strong>:</p>
<ol style="list-style-type: decimal">
<li><p>What is the <strong>probability</strong> of observing two heads in two flips if <span class="math inline">\(\theta = 0.7\)</span>?</p></li>
<li><p>Suppose you observe two heads and one tails in three flips. Which is more <strong>likely</strong> to have generated
these data, <span class="math inline">\(\theta = 0.3\)</span> or <span class="math inline">\(\theta = 0.7\)</span>?</p></li>
</ol>
</div>
<div id="section-bayesian-inference-introduction" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Bayesian Inference: introduction</h3>
<p>One criticism of the above approach is that is depends not only on the observed data, but also on infinitely many other possible datasets that are not observed. This is an artifact of the manner in which probability is used to represent uncertainty. In contrast, Bayesian statistics represents uncertainty about the value of a parameter <strong>directly</strong> using probability distributions.</p>
<p>In Bayesian inference, a <strong>prior distribution</strong> is placed on the parameter, representing the probable values of that parameter before data is observed. Having observed the data, the prior is updated via <em>Bayes’ Rule</em>, yielding the <strong>posterior distribution</strong> of the parameter, given the data.</p>
<p>The choice of prior distribution is based either on subject-matter knowledge or mathematical convenience, and is a subjective choice on the part of the analyst. Don’t worry too much about it in this course– you should get comfortable with the idea and the math. We’d talk more about it in a more advanced applied statistics course. But be warned: you may hear criticisms like “there is no way to pick a good prior”. While rooted in valid concern, such criticisms are a bit outdated. Bayesian inference is very useful in modern practice.</p>
<p>To see how this works, suppose that we have a pocket full of coins (woohoo!) and we think about <span class="math inline">\(4/5\)</span> coins in our pocket are the <span class="math inline">\(\theta = 0.3\)</span> coins, and only <span class="math inline">\(1/5\)</span> are the <span class="math inline">\(\theta = 0.7\)</span> coins. The parameter space here is <span class="math inline">\(\Theta = \left\{ 0.3, 0.7\right\}\)</span>. Our prior distribution on <span class="math inline">\(\theta\)</span> is then
<span class="math display">\[
P(\theta = q) = 0.2^{I(q = 0.7)}0.8^{I(q = 0.3)}, \ q\in\Theta
\]</span>
where <span class="math inline">\(I(q = 0.7) = 1\)</span> if <span class="math inline">\(q = 0.7\)</span> and <span class="math inline">\(0\)</span> otherwise. This, like the probability distribution of the actual result of the coin toss, just encodes our notion that <span class="math inline">\(P(\theta = 0.3) = 0.8\)</span> and <span class="math inline">\(P(\theta = 0.7) = 0.2\)</span>.</p>
<p>So without knowing the result of the coin toss, we think there is a <span class="math inline">\(20\%\)</span> chance that <span class="math inline">\(\theta = 0.7\)</span>. We know from above that if we observe heads on the coin toss, we have observed a result that would occur about <span class="math inline">\(70\%\)</span> of the time if <span class="math inline">\(\theta = 0.7\)</span>. In probability terms, we have a <em>marginal</em> distribution for <span class="math inline">\(\theta\)</span>, and a <em>conditional</em> distribution for <span class="math inline">\(X|\theta\)</span>.</p>
<p>These two ideas are combined by computing the conditional distribution of <span class="math inline">\(\theta|X\)</span>, known as the <strong>posterior distribution</strong> for <span class="math inline">\(\theta\)</span> having observed <span class="math inline">\(X\)</span>. This is obtained (explaining the name) via <em>Bayes’ Rule</em>:
<span class="math display">\[
p(\theta|X) = \frac{p(X|\theta)\times p(\theta)}{p(X)}
\]</span>
where the marginal distribution of <span class="math inline">\(X\)</span>, or the <em>normalizing constant</em> or <em>marginal likelihood</em> or <em>model evidence</em> (this thing has a lot of names) is given by
<span class="math display">\[
p(X) = \sum_{\theta\in\Theta}p(X|\theta)\times p(\theta)
\]</span>
and ensures <span class="math inline">\(p(\theta|X)\)</span> is a proper probability distribution.</p>
<p><strong>Exercise</strong>: verify that <span class="math inline">\(p(\theta|X)\)</span> is a valid probability density on <span class="math inline">\(\Theta\)</span> by verifying that <span class="math inline">\(\sum_{\theta\in\Theta}p(\theta|X) = 1\)</span>.</p>
<p>In our example, the prior probability of <span class="math inline">\(\theta = 0.7\)</span> is only <span class="math inline">\(20\%\)</span>. But we flip the coin and observe <span class="math inline">\(X = 1\)</span>. We can see how this observation updates our belief about the likely values of <span class="math inline">\(\theta\)</span> by computing the posterior distribution of <span class="math inline">\(\theta\)</span> given the observed data:
<span class="math display">\[
\begin{aligned}
&amp;p(\theta|X) = \frac{\theta^{x}(1-\theta)^{1-x}\times 0.2^{I(\theta = 0.7)}0.8^{I(\theta = 0.3)}}{\sum_{\theta = 0.3,0.7}\theta^{x}(1-\theta)^{1-x}\times 0.2^{I(\theta = 0.7)}0.8^{I(\theta = 0.3)}} \\
\implies&amp; P(\theta = 0.7 | X = 1) = \frac{0.7 \times 0.2}{0.7\times0.2 + 0.3\times0.8} \\
&amp;= 0.368
\end{aligned}
\]</span>
Before observing heads, we would have thought the <span class="math inline">\(\theta = 0.7\)</span> coin to be very unlikely, but because the observed data favours <span class="math inline">\(\theta = 0.7\)</span> more strongly than <span class="math inline">\(\theta = 0.3\)</span>, after observing these data we feel that <span class="math inline">\(\theta = 0.7\)</span> is more likely than before.</p>
</div>
<div id="section-flipping-more-coins" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Flipping More Coins</h3>
<p>Suppose now that we flip <span class="math inline">\(n\)</span> coins, obtaining a dataset <span class="math inline">\(X = (X_{1},\ldots,X_{n})\)</span> of heads or tails, represented by 0’s and 1’s. If we’re still considering only two candidate values <span class="math inline">\(\theta = 0.7\)</span> or <span class="math inline">\(\theta = 0.3\)</span>, we may still ask the question “which value is more likely to have generated the observed data?”. We again form the <em>likelihood function</em> for each value of <span class="math inline">\(\theta\)</span>, the relative frequency with which each value of <span class="math inline">\(\theta\)</span> would have generated the observed sample.</p>
<p><strong>Exercise</strong>: assuming the tosses are statistically independent:</p>
<ol style="list-style-type: decimal">
<li><p>Write down the complete <strong>statistical model</strong>: identify all random quantities,
their probability distributions, and the unknown parameters,</p></li>
<li><p>Show that the <strong>likelihood</strong> for the data is:
<span class="math display">\[
p(X|\theta) = \theta^{\sum_{i=1}^{n}X_{i}} \times (1 - \theta)^{n - \sum_{i=1}^{n}X_{i}}
\]</span>
where <span class="math inline">\(\sum_{i=1}^{n}X_{i}\)</span> is just the number of heads observed in the sample. We see that any two samples that have the same number of heads will lead to the same inferences about <span class="math inline">\(\theta\)</span> in this manner.</p></li>
</ol>
<p>Suppose we throw the coin <span class="math inline">\(10\)</span> times and observe <span class="math inline">\(6\)</span> heads. The likelihood function for each candidate value of <span class="math inline">\(\theta\)</span> is
<span class="math display">\[
\begin{aligned}
p(X|\theta = 0.7) &amp;= 0.7^{6} \times 0.3^{4} = 0.000953 \\
p(X|\theta = 0.3) &amp;= 0.3^{6} \times 0.7^{4} = 0.000175 \\
\end{aligned}
\]</span>
It is much more likely to observe <span class="math inline">\(6\)</span> heads when <span class="math inline">\(\theta = 0.7\)</span> than when <span class="math inline">\(\theta = 0.3\)</span>.</p>
<p><strong>Remark</strong>: it doesn’t look very “likely” to observe these data using either value of <span class="math inline">\(\theta\)</span>!
It turns out the actual value of the likelihood for a particular <span class="math inline">\(\theta\)</span> holds little meaning:
what is relevant is the <em>relative</em> values for different <span class="math inline">\(\theta\)</span>’s. In this example, <span class="math inline">\(\theta = 0.7\)</span> is
<span class="math inline">\(0.000953 / 0.000175 = 5.45\)</span> times <em>more likely</em> to have generated the observed data than <span class="math inline">\(\theta = 0.3\)</span>.</p>
<p><strong>Exercise</strong>: calculate the likelihood function for <span class="math inline">\(\theta\)</span> when <span class="math inline">\(n = 100\)</span> and <span class="math inline">\(\sum_{i=1}^{n}X_{i} = 60\)</span>. How much more likely is <span class="math inline">\(\theta = 0.7\)</span> than <span class="math inline">\(\theta = 0.3\)</span> now?</p>
<p>In the Bayesian setting, with our prior distribution on <span class="math inline">\(\theta\)</span> from above, we would form the posterior distribution as follows:
<span class="math display">\[
p(\theta|X) = \frac{\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{n - \sum_{i=1}^{n}x_{i}}\times 0.2^{I(\theta = 0.7)}0.8^{I(\theta = 0.3)}}{\sum_{\theta = 0.3,0.7}\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{n - \sum_{i=1}^{n}x_{i}}\times 0.2^{I(\theta = 0.7)}0.8^{I(\theta = 0.3)}}
\]</span>
Computing this for our observed data of <span class="math inline">\(\sum_{i=1}^{10}x_{i} = 6\)</span> yields
<span class="math display">\[
\begin{aligned}
p(\theta = 0.7 |X) = \frac{0.7^{6}0.3^{4}\times 0.2}{0.7^{6}0.3^{4}\times 0.2 + 0.3^{6}0.7^4\times0.8} = 0.576 \\
p(\theta = 0.3 |X) = \frac{0.3^{6}0.7^{4}\times 0.2}{0.3^{6}0.7^{4}\times 0.8 + 0.7^{6}0.3^4\times0.2} = 0.424 \\
\end{aligned}
\]</span></p>
<p>We can see that the data “updates” our prior belief that <span class="math inline">\(\theta = 0.3\)</span> was more probable than <span class="math inline">\(\theta = 0.7\)</span>, because the observed data was more likely to have occurred if <span class="math inline">\(\theta = 0.7\)</span> than if <span class="math inline">\(\theta = 0.3\)</span>.</p>
<p><strong>Exercise</strong>: compute the posterior distribution with <span class="math inline">\(n = 100\)</span> and <span class="math inline">\(\sum_{i=1}^{100}x_{i} = 60\)</span>.</p>
</div>
<div id="section-visualization" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Visualization</h3>
<p>It is helpful to visualize the prior and posterior, for the observed data. Because both prior and posterior only allow two values, we can do this using a simple bar chart:</p>
<div class="sourceCode" id="section-cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="section-introduction-to-bayesian-inference.html#cb217-1"></a>visualize_binomial_priorposterior &lt;-<span class="st"> </span><span class="cf">function</span>(sumx,n) {</span>
<span id="cb217-2"><a href="section-introduction-to-bayesian-inference.html#cb217-2"></a>  prior &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {</span>
<span id="cb217-3"><a href="section-introduction-to-bayesian-inference.html#cb217-3"></a>    <span class="cf">if</span> (theta <span class="op">==</span><span class="st"> </span><span class="fl">.3</span>) {</span>
<span id="cb217-4"><a href="section-introduction-to-bayesian-inference.html#cb217-4"></a>      <span class="kw">return</span>(.<span class="dv">8</span>)</span>
<span id="cb217-5"><a href="section-introduction-to-bayesian-inference.html#cb217-5"></a>    }</span>
<span id="cb217-6"><a href="section-introduction-to-bayesian-inference.html#cb217-6"></a>    <span class="cf">else</span> <span class="cf">if</span> (theta <span class="op">==</span><span class="st"> </span><span class="fl">.7</span>) {</span>
<span id="cb217-7"><a href="section-introduction-to-bayesian-inference.html#cb217-7"></a>      <span class="kw">return</span>(.<span class="dv">2</span>)</span>
<span id="cb217-8"><a href="section-introduction-to-bayesian-inference.html#cb217-8"></a>    }</span>
<span id="cb217-9"><a href="section-introduction-to-bayesian-inference.html#cb217-9"></a>    <span class="dv">0</span></span>
<span id="cb217-10"><a href="section-introduction-to-bayesian-inference.html#cb217-10"></a>  }</span>
<span id="cb217-11"><a href="section-introduction-to-bayesian-inference.html#cb217-11"></a>  likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(theta) theta<span class="op">^</span>sumx <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>theta)<span class="op">^</span>(n <span class="op">-</span><span class="st"> </span>sumx)</span>
<span id="cb217-12"><a href="section-introduction-to-bayesian-inference.html#cb217-12"></a>  marginal_likelihood &lt;-<span class="st"> </span><span class="kw">prior</span>(.<span class="dv">7</span>) <span class="op">*</span><span class="st"> </span><span class="kw">likelihood</span>(.<span class="dv">7</span>) <span class="op">+</span><span class="st"> </span><span class="kw">prior</span>(.<span class="dv">3</span>) <span class="op">*</span><span class="st"> </span><span class="kw">likelihood</span>(.<span class="dv">3</span>)</span>
<span id="cb217-13"><a href="section-introduction-to-bayesian-inference.html#cb217-13"></a>  posterior &lt;-<span class="st"> </span><span class="cf">function</span>(theta) <span class="kw">likelihood</span>(theta) <span class="op">*</span><span class="st"> </span><span class="kw">prior</span>(theta) <span class="op">/</span><span class="st"> </span>marginal_likelihood</span>
<span id="cb217-14"><a href="section-introduction-to-bayesian-inference.html#cb217-14"></a>  </span>
<span id="cb217-15"><a href="section-introduction-to-bayesian-inference.html#cb217-15"></a>  <span class="co"># Plot of the prior and posterior distributions for these observed data</span></span>
<span id="cb217-16"><a href="section-introduction-to-bayesian-inference.html#cb217-16"></a>  <span class="kw">tibble</span>(</span>
<span id="cb217-17"><a href="section-introduction-to-bayesian-inference.html#cb217-17"></a>    <span class="dt">theta =</span> <span class="kw">c</span>(.<span class="dv">3</span>,.<span class="dv">7</span>,.<span class="dv">3</span>,.<span class="dv">7</span>),</span>
<span id="cb217-18"><a href="section-introduction-to-bayesian-inference.html#cb217-18"></a>    <span class="dt">value =</span> <span class="kw">c</span>(<span class="kw">prior</span>(.<span class="dv">3</span>),<span class="kw">prior</span>(.<span class="dv">7</span>),<span class="kw">posterior</span>(.<span class="dv">3</span>),<span class="kw">posterior</span>(.<span class="dv">7</span>)),</span>
<span id="cb217-19"><a href="section-introduction-to-bayesian-inference.html#cb217-19"></a>    <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;Prior&quot;</span>,<span class="st">&quot;Prior&quot;</span>,<span class="st">&quot;Posterior&quot;</span>,<span class="st">&quot;Posterior&quot;</span>)</span>
<span id="cb217-20"><a href="section-introduction-to-bayesian-inference.html#cb217-20"></a>  ) <span class="op">%&gt;%</span></span>
<span id="cb217-21"><a href="section-introduction-to-bayesian-inference.html#cb217-21"></a><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta,<span class="dt">y =</span> value,<span class="dt">fill =</span> type)) <span class="op">+</span></span>
<span id="cb217-22"><a href="section-introduction-to-bayesian-inference.html#cb217-22"></a><span class="st">    </span><span class="kw">theme_classic</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb217-23"><a href="section-introduction-to-bayesian-inference.html#cb217-23"></a><span class="st">    </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>,<span class="dt">position =</span> <span class="st">&quot;dodge&quot;</span>,<span class="dt">colour =</span> <span class="st">&quot;black&quot;</span>) <span class="op">+</span></span>
<span id="cb217-24"><a href="section-introduction-to-bayesian-inference.html#cb217-24"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Prior and Posterior for theta&quot;</span>,</span>
<span id="cb217-25"><a href="section-introduction-to-bayesian-inference.html#cb217-25"></a>         <span class="dt">subtitle =</span> <span class="kw">str_c</span>(<span class="st">&quot;Observed data: &quot;</span>,sumx,<span class="st">&quot; flips in &quot;</span>,n,<span class="st">&quot; throws&quot;</span>),</span>
<span id="cb217-26"><a href="section-introduction-to-bayesian-inference.html#cb217-26"></a>         <span class="dt">x =</span> <span class="st">&quot;Theta, probability of heads&quot;</span>,</span>
<span id="cb217-27"><a href="section-introduction-to-bayesian-inference.html#cb217-27"></a>         <span class="dt">y =</span> <span class="st">&quot;Prior/Posterior Probability&quot;</span>,</span>
<span id="cb217-28"><a href="section-introduction-to-bayesian-inference.html#cb217-28"></a>         <span class="dt">fill =</span> <span class="st">&quot;&quot;</span>) <span class="op">+</span></span>
<span id="cb217-29"><a href="section-introduction-to-bayesian-inference.html#cb217-29"></a><span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="fl">0.30</span>,<span class="fl">0.70</span>),<span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;0.30&quot;</span>,<span class="st">&quot;0.70&quot;</span>)) <span class="op">+</span></span>
<span id="cb217-30"><a href="section-introduction-to-bayesian-inference.html#cb217-30"></a><span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span><span class="kw">percent_format</span>()) <span class="op">+</span></span>
<span id="cb217-31"><a href="section-introduction-to-bayesian-inference.html#cb217-31"></a><span class="st">    </span><span class="kw">scale_fill_brewer</span>(<span class="dt">palette =</span> <span class="st">&quot;Reds&quot;</span>)</span>
<span id="cb217-32"><a href="section-introduction-to-bayesian-inference.html#cb217-32"></a>  </span>
<span id="cb217-33"><a href="section-introduction-to-bayesian-inference.html#cb217-33"></a>}</span></code></pre></div>
<p>Plotting is nice as it lets us compare how different observed data, and different experiments (number of throws) affect the prior/posterior balance of belief:</p>
<div class="sourceCode" id="section-cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="section-introduction-to-bayesian-inference.html#cb218-1"></a><span class="co"># library(patchwork)</span></span>
<span id="cb218-2"><a href="section-introduction-to-bayesian-inference.html#cb218-2"></a>(<span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">6</span>,<span class="dv">6</span>) <span class="op">|</span><span class="st"> </span><span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">6</span>,<span class="dv">10</span>)) <span class="op">/</span></span>
<span id="cb218-3"><a href="section-introduction-to-bayesian-inference.html#cb218-3"></a>(<span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">6</span>,<span class="dv">20</span>) <span class="op">|</span><span class="st"> </span><span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">6</span>,<span class="dv">50</span>)) <span class="op">/</span></span>
<span id="cb218-4"><a href="section-introduction-to-bayesian-inference.html#cb218-4"></a>(<span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">0</span>,<span class="dv">10</span>) <span class="op">|</span><span class="st"> </span><span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">1</span>,<span class="dv">10</span>)) <span class="op">/</span></span>
<span id="cb218-5"><a href="section-introduction-to-bayesian-inference.html#cb218-5"></a>(<span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">7</span>,<span class="dv">10</span>) <span class="op">|</span><span class="st"> </span><span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">10</span>,<span class="dv">10</span>))</span></code></pre></div>
<p><img src="book_files/figure-html/binomial-vis2-1.png" width="672" /></p>
</div>
</div>
<div id="section-interactive-app" class="section level2">
<h2><span class="header-section-number">6.2</span> Interactive App</h2>
<p>Go to the app: <a href="http://shiny.sta220.utstat.utoronto.ca:88/BayesianApp/" class="uri">http://shiny.sta220.utstat.utoronto.ca:88/BayesianApp/</a></p>
<p>The app lets you flip coins and estimate the probability of heads using Frequentist
and Bayesian methods. We haven’t covered estimation yet, but we have covered the
model for coin flipping in both contexts now, so you should be able to tell what’s
happening. Also shown are <em>interval estimates</em>, which measure the strength of
the conclusions about <span class="math inline">\(p\)</span> that are made based on the data and model. Narrower
interval estimates mean we’re more sure about the value of <span class="math inline">\(p\)</span>, after seeing the
data.</p>
<p>The app lets you change the following:</p>
<ul>
<li>The number of times you flip the coin,</li>
<li>The true probability of heads, <span class="math inline">\(p\)</span>,</li>
<li>Your prior belief about the probability of heads, the “prior mean”, and</li>
<li>The <em>strength</em> of your prior beliefs, as measured by the prior standard deviation.
Lower standard deviation means you’re <em>more sure</em> about the value of <span class="math inline">\(p\)</span>, before
seeing any flips.</li>
</ul>
<p>You should answer the following questions:</p>
<ol style="list-style-type: decimal">
<li><p>How many flips do you need before the Bayesian and frequentist inferences
agree closely? Does this depend on the true value of <span class="math inline">\(p\)</span>, your prior belief,
and the strength of your prior belief?</p></li>
<li><p>Intuitively: why are the Bayesian interval estimates narrower than the
frequentist ones? Is this always the case?</p></li>
<li><p>Can you “break” the Bayesian answer by expressing really strong and wrong
prior beliefs? Can you “fix” it by flipping the coin more times?</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-evaluating-estimators-efficiency-and-mean-squared-error.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-the-bootstrap.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
