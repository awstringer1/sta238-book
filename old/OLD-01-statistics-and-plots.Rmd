# Statistics and Plots

## Introduction

How do we turn **data** into **information**?

This question is highly *contextual*. The meaning of *information* will change depending on the task. And *data* isn't a well-defined concept on its own. Even [https://www.merriam-webster.com/dictionary/data](Webster's dictionary) defines data *implicitly*, as "a basis for...".

In this chapter we will discuss the use of data in the pursuit of three types of information: **descriptive**, **exploratory**, and **prescriptive**.

## Descriptive analysis

The most simple type of information available from a set of data is to  *describe* what you have in front of you. The decisions you make about *what* to describe and *how* to describe it make a big difference.

There is no one correct way to analyze any given dataset. You will make judgements,
and assumptions, and you will have to justify and explain them. These judgements
and assumptions can have a profound impact on the *story* you tell using your data.

For example, consider a famous dataset containing information on smoking and mortality.
The data is available in the `R` package `faraway`. We may load the package and data
and retrieve information on it as follows:
  
```{r load-smoke}
# install.packages("faraway") # Run this to install the faraway package, which has useful datasets
library(faraway) # Attach the faraway package
data("femsmoke") # Load the "femsmoke" data
# ?femsmoke # Run this to open the help page for the dataset.
```

Lines in `R` that begin with a pound sign (or "hashtag" for the younger, more hip reader), $\#$,
are comments and are not run. Remove the $\#$ to run the code.

We see from the help page, and associated reference to the paper in the *American Statistician*,
that the data comes from asking women in Whickham, England, whether they smoke or not, 
and then following up in 20 years to see if they died.

In *describing* the context surrouning the data---a quite natural step to take---we have achieved our first example of a descriptive analysis. We didn't even use and numbers or code. Just describing what the data *is* counts. And it's important, because as we learned in the introduction, information can't exist without *context*.

To go further in our descriptive analysis, we need some more *quantitative* pieces of descriptive information. What might we want to know about our data? Some ideas:

- How many **observations** are there in the data, and what does an observation represent in the context of how the data was collected?
- How many **variables** are present in the data, and what does each variable represent in the context of how the data was collected?
- How might we summarize each variable? We might compute a **mean** and a **five-number summary** for "continuous" variables, and a **table of counts** for "categorical" variables (more on this later...).

Let's see how we can obtain these descriptive measures in `R`:

```{r descriptive-1}
# install.packages("tidyverse")
# install.packages("SMPracticals") # For datasets
library(tidyverse) # We use a LOT of functions from this package.
# Get the number of observations (rows), variables, and an idea
# of what the data looks like:
glimpse(femsmoke)

# One observation represents a count of people in each category.
# How many people?
femsmoke %>%
  summarize(num_people = sum(y)) # The summarize() function lets you compute summaries of variables in your dataframe

# How many smokers?
femsmoke %>%
  filter(smoker == "yes") %>% # filter() lets you choose which rows to keep
  summarize(num_smokers = sum(y))

# How many non-smokers?
femsmoke %>%
  filter(smoker == "no") %>%
  summarize(num_non_smokers = sum(y))

# We can get both those numbers at the same time:
femsmoke %>%
  group_by(smoker) %>% # group_by() makes summarize() compute summaries within levels of a variable
  summarize(num_people = sum(y))
```

There are lots of other descriptive statistics you could calculate.

*Summary*: descriptive analysis involves communicating properties of a dataset, and the context behind the dataset.

### Exercises

1. How many non-smoking 18-24 year olds are there in the `femsmoke` data? Answer using `filter()`.

1. How many smokers died? Answer using `filter()`.

1. How many 45-55 year olds did not die?

1. Compute the following table using `group_by()` and `summarize()`:

```{r descriptive-ex-1,echo = FALSE}
femsmoke %>%
  group_by(age) %>%
  summarize(num_people = sum(y))
```

## Exploratory analysis

Descriptive analyses are an important task any time you are working with data. However, they don't accomplish anything *new*; they just, well, *describe* what you have.

An **exploratory analysis** involves looking for patterns or structure in the available data. We can use what we find to make decisions or come to conclusions.

One thing we can *choose* to explore about these data is whether we
observe any apparent *association* between smoking and mortality. To investigate any such associations, we can look at *the observed mortality rates*
for smokers and non-smokers. This is *exploratory*---not *descriptive*---because we are going beyond simply saying what the data looks like. We are looking for a pattern, namely, "do smokers die more or less frequently than non-smokers?"

Exploratory analyses involve decisions made by us, the investigator. They are naturally *subjective*. Different decisions can lead to uncovering different patterns. For example:

```{r mort-rate-1,warning=FALSE,message=FALSE}
# Compute the mortality rate for smokers and non-smokers.
# To do this, create a dataframe containing the numbers of smokers
# and non-smokers
smoker_numbers <- femsmoke %>% # The %>% operator lets you form sequences of operations
  group_by(smoker) %>% # group_by() makes all the following operations happen within groups
  summarize(num_people = sum(y)) # Count the number of people who are smokers and not smokers 
smoker_numbers

# Now, compute the number of people who died out of the smokers and non-smokers
# This looks the same as above, except we now filter() only the people who died.
smoker_numbers_dead <- femsmoke %>%
  filter(dead == "yes") %>% # Retains rows where dead == "yes" only
  group_by(smoker) %>%
  summarize(num_dead = sum(y))
smoker_numbers_dead

# Now, we join these two tables together and compute the mortality rates by group.
smoker_numbers %>%
  inner_join(smoker_numbers_dead,by = "smoker") %>% # Joins rows with the same value of "smoker"
  mutate(mort_rate = num_dead/num_people) # mutate() creates a new variable, which can be a function of the other variables in the dataframe.
```

See anything interesting?
  
What went wrong? Why are we observing that smokers have a *lower* mortality rate than
non-smokers? This contradicts the *context* surrounding this analysis, which in this case is the large body of formal and anecdotal evidence suggesting that smoking is harmful to health.

Did we make a mistake?
  
One thing we definitely did was ignore some present information. Specifically,
we also know how old the women were. How can we include this information in our
exploratory analysis? We can compute mortality rates by age:
  
```{r mort-rate-2,warning=FALSE,message=FALSE}
smoker_numbers_age <- femsmoke %>%
  group_by(smoker,age) %>% # Now we're grouping by smoker AND age. The rest of the code remains unchanged.
  summarize(num_people = sum(y))

smoker_numbers_age_dead <- femsmoke %>%
  filter(dead == "yes") %>%
  group_by(smoker,age) %>%
  summarize(num_dead = sum(y))

smoker_numbers_age %>%
  inner_join(smoker_numbers_age_dead,by = c("smoker","age")) %>% 
  mutate(mort_rate = num_dead/num_people)
```

Older people are more likely to die within the 20 year followup period. 
However, examining the raw counts of people in each group, we also see that 
*in these data, older people are less likely to smoke* than younger people. So
*in these data*, less smokers died, because less smokers were old, and more
old people died.

But was our first analysis wrong? No. Our first analysis was fine: we computed
the mortality rate in each group. The problem was in the reporting, or the way
we told the story. We didn't provide enough information when we said "the mortality
rate for smokers was lower than for non-smokers". We should have mentioned that
this is *averaging over all age groups*. Even when we include age in the analysis,
we ought to mention the fact that there are a whole lot of variables we *could*
have measured but didn't, and we are implicitly averaging over these too.

Before moving on, get some practice doing exploratory analysis with the following
exercises:
  
### Exercises

1. What is the *relative risk* of mortality---the ratio of the mortality rates---for
smoking 18-24 year olds vs non-smoking 18-24 year olds? Compute the answer manually by
reading the numbers off the above table. Then compute it using `R` by doing the
following:
  
  - Create two datasets using `filter()`: one containing smokers and one containing
non-smokers. `filter()` out only the 18-24 year olds. This gives you two datasets
each with only one row. For example, `smokers <- femsmoke %>% filter(smoker == "yes",age = "18-24")`.
- `inner_join()` the two datasets together, using `age` as the `by` variable:
  `smokers %>% inner_join(???,by = "age")`

1. *Advanced*: modify the above steps to create the following table of relative mortality rates. You should start from a cleaned up version of the mortality rate by age table:
  
  ```{r rel-mort-rate-2}
rates_by_age <- smoker_numbers_age %>%
  inner_join(smoker_numbers_age_dead,by = c("smoker","age")) %>% 
  mutate(mort_rate = num_dead/num_people) %>%
  ungroup() # The data was previously grouped, we don't want this anymore
```

Use `dplyr::select()` to remove and rename columns, see `?dplyr::select`.

```{r rel-mort-rate-1,echo = FALSE}
smokers <- rates_by_age %>%
  ungroup() %>%
  filter(smoker == "yes") %>%
  dplyr::select(age,smoker_mort_rate = mort_rate)

nonsmokers <- rates_by_age %>%
  filter(smoker == "no") %>%
  dplyr::select(age,nonsmoker_mort_rate = mort_rate)

smokers %>%
  inner_join(nonsmokers,by = "age") %>%
  mutate(relative_risk = smoker_mort_rate/nonsmoker_mort_rate)

```

## Prescriptive analysis

Descriptive anlaysis is about understanding the data that we have. Exploratory anlaysis is about finding patterns in the data you have.

**Prescriptive analysis** is about using the data you have to explicitly support making a decision or coming to a conclusion. Often the data has been collected for a reason. Scientific experiments, surveys, questionnaries given when applying for a mortage or insurance product; these aren't just "fishing expeditions". These data are collected in the hopes of coming to a specific conclusion. We might wish to state the efficacy of a clinical treatment, or decide which politician is going to win an election, or whether to grant somebody a loan or underwrite an insurance policy.

Often a prescriptive analysis is conducted *using exactly the same steps* as an exploratory analysis. Here, you must be very careful. Knowing what you want the data to say can, and very often does, influence choices made by the researcher about how to analyze the data. This is often referred to as "researcher degrees of freedom".

For example, perhaps the smoking data was collected with the explicit purpose of supporting anti-smoking initiatives. This renders the above analysis *prescriptive*, because we know what we want to see: we want smokers to have a higher mortality rate than non-smokers in these data. In this *context*, let's re-compute the above table of mortality rates:

```{r prescriptive-1}
smoker_numbers %>%
  inner_join(smoker_numbers_dead,by = "smoker") %>%
  mutate(mort_rate = num_dead/num_people)
```

...hm. It appears that the data is telling us the opposite of what we thought it would.

What do we do now? Well, we ignored some information, namely the subjects' ages. So let's use those:

```{r prescriptive-2}
smoker_numbers_age %>%
  inner_join(smoker_numbers_age_dead,by = c("smoker","age")) %>% 
  mutate(mort_rate = num_dead/num_people)
```

Oh! It's because older people are more likely to die, and less likely to smoke.

We performed the exact same steps in our prescriptive analysis as we did in our exploratory analysis. Can we make the same conclusions?

This is where we need to be careful. When we go into the data with a specific question in mind, **we have to report everything we see that relates to this question**. We ought to report that

- Smokers had a lower mortality rate than non-smokers, and
- there were more young smokers than old smokers in the data, and
- younger people had a lower mortality rate than older people, in our data.

Context matters! If we just reported that "controlling for age, smokers have a higher mortality rate than non-smokers", we would be communicating a potentially *spurious* conclusion. What's so important about age? We could have controlled for other variables and perhaps seen different relationships. These "researcher degrees of freedom" are a big force behind what is known as the "reproducibility crisis" in modern science.

### Exercises

1. For each of the following analyses, state whether you think it is *descriptive*, *exploratory* or *prescriptive*, and say why.

  a. I work for an urban planning think tank and I want to present to our clients a proposal that advocates the development of green spaces in urban centres as a public health initiative. I use Health Canada data on obesity and Open Data Toronto data on locations of greenspaces to investigate whether areas with more green spaces have less obesity.
  a. I am interested in which streets have more heavily enforced parking laws, so I download data on all parking tickets given in Toronto last year and make a map.

## Rental housing in Toronto

The RentSafeTO: Apartment Building Standards program is designed to help renters
in the city of Toronto make informed choices about where to live, and to enforce
a minimum standard of quality upon rental units within the city. With rents 
skyrocketing and home ownership not a reasonable option for most, having an
informed view of the rental market is imperative for Toronto residents. It also helps keep leaders
accountable, specifically if we focus on social and community housing buildings.

Comprehensive and fairly clean data from the program, along with specific
information, is available at https://open.toronto.ca/dataset/apartment-building-evaluation/.
Data for the following were downloaded on `2019/09/16`. 

To start your analysis, go now and download the data
and open it in a spreadsheet and have a look. Familiarize yourselves with the
variable descriptions and how the data were collected; the *documentation*. This somewhat tedious 
task is a first step of **any** data analysis, in academia, industry, government,
or wherever.

### Load the data

The data are stored in a `.csv` file, which stands for "comma-separated-values".
Storing data in a text file with a separator, usually a comma, is very common.
These are referred to as "flat files" in an industrial context, to distinguish
them from data stored in databases.

We may read the data into `R` using the `read_csv` function in the `readr` package.
The `readr` package is part of the `tidyverse` package that we used before, so if
you installed that package, you have it loaded.

```{r read-apartment-1}
# https://open.toronto.ca/dataset/apartment-building-evaluation/
# install.packages("readr")

# Read the data in. This means call the readr::read_csv() function, point it
# to where you saved the data on your computer, and then save the result to a
# variable. I am naming this variable 'apartmentdata'.
# Type ?readr::read_csv if you want to read about this function.
apartmentdata <- readr::read_csv(
  file = "data/apartment-data/toronto-apartment-building-evaluations.csv"
)
```

The message displayed is telling you that `readr::read_csv()` guessed at what
kind of data were in each column, i.e. numbers, letters, dates, etc. You should
make sure, as I have while writing, that these are what you expect. You can
get a concise view of this dataset using the `glimpse` function in the `dplyr`
package, which is automatically loaded when you load the `tidyverse`:
  
  ```{r apartment-data-2}
glimpse(apartmentdata)
```

That's bigger than the `smoking` data! 3,446 rental apartment buildings, each with
32 factors measured. The buliding's address and Ward number are in there, which are
helpful for characterizing neighbourhoods.

### Analysis I: what does the data look like?

As a first step, we want to get an idea of what our data "looks like". This typically means picking some interesting variables and summarizing their distributions somehow.

Which variables to pick will depend on the context. Often it will be clear which variables are important, and sometimes not.

Because you read the documentation and familiarized yourselves with the variables in the dataset, you know that there is a variable called `SCORE` which sums up the individual category scores for each building. In the context of determining building quality, this seems like an important variable to look at.

We'll summarize the distribution of `SCORE` using a five-number summary and mean, and a **histogram** with a **kernel density estimate**.

First, prepare the data for analysis:

```{r apartment-descriptive-1}
# First, select only the columns you want
# This isn't strictly necessary but trust me, it makes 
# debugging WAY easier.
# I'm also renaming the columns so the dataframe looks prettier.
# Again, trust me. This stuff matters.
apartmentclean <- apartmentdata %>% 
  filter(!is.na(SCORE)) %>% # Remove apartments with missing scores
  dplyr::select(ward = WARD,
                score = SCORE,
                property_type = PROPERTY_TYPE,
                year_built = YEAR_BUILT,
                address = SITE_ADDRESS
  )
glimpse(apartmentclean) # Much nicer!
```

To compute the five-number summary (plus mean), use the `summary()` function in `R`. I also want to know the *standard deviation* of `SCORE`:

```{r apartment-descriptive-2}
summary(apartmentclean$score)
sd(apartmentclean$score,na.rm = TRUE)
```

The worst building in the city has a total score of 37, and the best gets 99. The median score---half the buildings in the city have a lower score, and half a higher score than this---is 72, and this roughly equals the mean of 72.28. 25% of buildings score higher than 77, and 25% score lower than 68. So most buildings seem to fall within less than one standard deviation of the mean, which indicates that these data are fairly *concentrated* about their mean.

To provide some context, go look up your own building (if you live in a rental building) or that of a friend in the data. Where does your building fall in terms of quality within Toronto?

So far we have used tabular displays to summarize our data, for both the `smoking` and the apartment data. To communicate more information at once, we can use a **plot**. 

The most common type of plot for visualizing an empirical estimate of a probability distribution is a **histogram**. A histogram bins the data into ranges and, by default, counts the number of data points in each range. We will tell the plotting function to compute the *proportion* of data in each range, and then *normalize* the results so that the plot integrates to one (and hence estimates a probability density). This won't change the shape of the plot.

We can make a histogram in `R` as follows:

```{r apartment-descriptive-3}
# The ggplot2 package is loaded as part of the tidyverse
score_histogram <- apartmentclean %>%
  ggplot(aes(x = score)) + # Tell ggplot to use score on the x axis
  theme_light() + # Make the plot pretty
  geom_histogram( # Makes a histogram
    aes(y = ..density..),
    bins = 20,
    colour = "black",
    fill = "lightgrey"
  ) +
  labs(title = "Distribution of RentSafeTO Apartment Building Standards score",
       x = "Score",
       y = "Density") +
  scale_x_continuous(breaks = seq(30,100,by = 5))

score_histogram
```

It appears that most buildings are in the 65 to 85 range. (Interesting point: at time of writing I am in the process of moving from one building to another; current building has a 66, new building has an 86 :D).

Another common estimate of a distribution is a **kernel density estimate**, which can be viewed as a more "continuous" version of a histogram that actually draws a curve on the plot that is supposed to represent the (empirical) probability density of the data. We can add this to our plot very quickly:

```{r apartment-descriptive-4}
score_histogram +
  geom_density()
```

### Analysis II: Do different wards have different quality housing?

A Ward is an administrative district within the city that has a single city 
counsellor. If I'm thinking about moving to, or within, Toronto, I want to know:
**Do different wards have different quality housing?**.

In order to address this question we need to decide on the following:

- **Variable of interest**. How do we *quantify* our research question? We need
to pick a *measure of quality*. Picking different measures can lead to different
conclusions.
- **Filters**. Do we look at all apartment buildings? Should we look only at those
built after, or before, a certain date? Only those that meet a certain minimum,
or maximum, standard of quality according to our definition? Are there any other
kinds of decisions we might have to consider?
- **Methods**. What kind of statistical tools should we use to address our research
question? We need to pick descriptive statistics to report, and decide whether 
we want to include other auxillary variables in the analysis.
- **Conclusions**. How do we report our results? Tables, charts, maps? Should we
include subjective, editorial commentary, or let the data speak for
themselves?

This is already overwhelming! Let's make an attempt at it. I propose:
  
- Our **variable of interest** should be `SCORE`, which you know (because you
read the documentation...) is the "overall score of the buliding". Higher is 
better. The actual formula is included in the documentation of the data.
- We will **filter** the data to only include buildings where `PROPERTY_TYPE == 'PRIVATE'`,
which will restrict our analysis to not include social housing. The quality of social
housing is an important social justice issue (that you will investigate in the exercises)
but it's somewhat separate (?) from the question of where to look for rental housing.
- Our **methods** will include looking at a table of average scores for each ward.
We will also look at whether older or newer buildings receive better scores. 
- We will summarize our **conclusions** through a subjective assessment of the 
above table of average scores.

With these decisions made, we may proceed with our analysis using the `tidyverse`
as follows:

```{r apartment-analysis-1}
# Apply filter(s).
apartmentfiltered <- apartmentclean %>%
  filter(property_type == "PRIVATE")
# When filtering, always compare the filtered and unfiltered data to ensure
# the result is as expected:
glimpse(apartmentclean)
glimpse(apartmentfiltered)
nrow(apartmentclean) - nrow(apartmentfiltered) # Dropped 567 rows.

# Now create the table of averages:
apartmentfiltered %>%
  group_by(ward) %>%
  summarize(avg_score = mean(score))

```

Bah! What happened? Why are there these `NA` values? 
  
  `NA` is the value `R` uses to mean "missing". We have to hope that whether a 
rental apartment building's `score` is missing *is not related* to what that score *is*,
that is, we hope apartments with higher or lower scores aren't missing more often.

We will ignore missingness for now. To do this, use the `na.rm = TRUE` option in
`mean`:

```{r apartment-analysis-2}
apartmentsummary <- apartmentfiltered %>%
  group_by(ward) %>%
  summarize(avg_score = mean(score,na.rm = TRUE))
apartmentsummary
```

This isn't a super friendly way of comparing these 26 numbers. We need some kind
od **visualization**, or **plot**, so we can take one look and get an idea of 
what is going on.

In order to pick what kind of plot to make, we have to understand what types
of variables we are attempting to compare. Broadly speaking, variables can
either be **continuous** or **categorical**. 

A **continuous** variable consists of
numbers. You can always compare the *order* of two values of a continuous variable
(say whether one is bigger than the other), because its values are just numbers.

A **categorical** variable takes values from a finite (discrete) set. These values
can be anything. Categorical variables sometimes have values which can be compared,
like "big", "medium", "small"; but often the values are not inherently comparable.

In the case of the table of average scores by ward, we will consider `avg_score`
to be a **continuous** variable and `ward` to be a **categorical** variable. Wards
are labelled with numbers, yes, but they are not inherently comparable; they are
areas within the city. It wouldn't make sense to say that ward 2 were "less" than
ward 17. We could have labelled them "A", "B", "C",... and the meaning would
have been the same.

One common visualization used to compare values of a **continuous** variable
with values of a **categorical** variable is a **dot chart**. A dot chart contains
the values of the categorical variable on the x-axis and the values of the
continuous variable on the y-axis. You can take one look and see whether there is
any pattern in values of the continuous variable with respect to the categorical
variable.

We create a **dot chart** to visualize these summary statistics using the `ggplot2`
package. This is also included with the `tidyverse` (surprise!):

```{r barchart-apartment-1}
apartmentsummary %>%
  ggplot(aes(x = ward,y = avg_score)) + # Set up the variable mappings
  theme_light() + # Make it pretty
  geom_point(pch = 21,colour = "black",fill = "grey") + # Add the bars
  labs(title = "Average ABS score shows moderate variability across wards in Toronto",
       x = "Ward",
       y = "Average ABS Score") # Informative title
```

It looks like some wards are better than others. Or are they? Can we make any
definitive conclusions based on this?

### Analysis III: trends in quality over time

Let's go further and analyze some other interesting aspects of these data. I'm
interested in knowing: **Are newer buildings higher quality**? 

We have the `score`
and the `year_built`, and we'd like to investigate whether newer buildings (higher
`year_built`) have higher `scores`. We have another decision to make. We could
consider `year_built` to be a categorical variable, and make a bar chart. Or, 
we could consider it to be a continuous variable.

Because values of `year_built` are inherently comparable, and because *our research
question involves making such comparisons*, we will consider `year_built` to be
a continuous variable.

One type of plot used to compare continuous variables is a **scatterplot**. A 
scatterplot has continuous variables on the x- and y-axes, and draws a point (or
bubble) at each place in the two-dimensional plane where a datapoint occurs. We
can make this kind of plot in `ggplot2` as well. This time, we use the raw 
(well, cleaned and filtered) data:

```{r apartment-plot-2}
apartmentfiltered %>%
  filter(year_built > 1900) %>%
  ggplot(aes(x = year_built,y = score)) +
  theme_light() + 
  geom_point(pch = 21,colour = "black",fill = "grey") + # pch=21 makes the bubbles hollow, looks nice
  scale_x_continuous(breaks = seq(1900,2020,by=10)) + # Set the x-axis range
  labs(title = "Less rental buildings are being built recently, but they are of higher quality",
       x = "Year Built",
       y = "ABS Score")
```

Very interesting. You can clearly see the baby boom of the 1950's to 1970's, followed by a massive slowdown in construction during the economic slump in the 1980's, and a complete stop when rent control was introduced in 1991 (remember, these are *rental* buildings only). Then, we see a new wave of
rental building construction, and the new buildings seem to be of higher quality.

What are the highest and lowest quality rental buildings in Toronto?

```{r apartment-3}
# Get the 10 highest scoring buildings
apartmentfiltered %>%
  arrange(desc(score)) %>% # Sort the data, descending, by score
  slice(1:10) # Take the first ten- i.e. the top ten
```

Wow. I know where I want to live.

### Summary

We have seen how even something simple like trying to figure out whether
different areas of the city have different quality housing can require
a lot of decision making. And these decisions require expertise. By taking
a principled approach to learning **data analysis**, you are *empowering*
yourself to live a life that is better informed.

But notice that we didn't really answer any questions in this chapter. We
saw some rough patterns, but were they real? If we made different decisions, or if we *sampled different data*, would we have seen different patterns?

In order to understand what the problem is and how to approach it, we
need to take a more detailed look at the concept of **error**. This is the
subject of Chapter 2.

### Exercises

1. Take each of the analyses we have performed on the Toronto rental data and say whether you think it's **descriptive**, **exploratory**, or **prescriptive**, or a mix, and say why.

1. What is that "YY" ward that shows up in the dot plot? Investigate this unusual observation. 

1. Read the documentation online and choose three variables that you find the most interesting. Reproduce the analyses I, II and III using your variables. Is there more or less variability across wards than with `score`?

1. What is the ward with the highest average score? In what ward is/are the building(s) with the highest score(s)? Is this the same ward, or not? Would you expect the ward with the highest average to also have the highest-scoring buildings? Repeat this question with the lowest scoring buildings instead of the highest.

1. If you live in a rental apartment, find it in these data. If not, find a friend's place. How does your building compare to other buildings in your ward? Does it score higher or lower? The `filter()` function is your friend here, or you can use `apartmentfiltered %>% arrange(SITE_ADDRESS) %>% print(n = Inf)` and then find yours in the list manually.

1. Combine the analyses of sections 2.3.2 and 2.3.3. with that of 2.3.4. Specifically, make a table and a boxplot of the average score by year. This means replace `ward` by `year_built` in the analysis of sections 2.3.2. and 2.3.3. Do your conclusions change when comparing with 2.3.4? Why or why not? Would you expect this to always be the case?

1. *Advanced*: analyze the quality of **social housing** in Toronto. Perform a similar analysis to what we performed here for `PROPERTY_TYPE == 'PRIVATE'`, but instead for `PROPERTY_TYPE %in% c('SOCIAL HOUSING','TCHC')` (equivalent to `PROPERTY_TYPE != 'PRIVATE'`). Does the quality of social housing in Toronto vary greatly across different wards? Is it improving or degrading over time? Do you think we have enough information here to definitively answer these questions?
